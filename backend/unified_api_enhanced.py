#!/usr/bin/env python3
"""
Unified Codebase Analytics API

This file consolidates functionality from:
- comprehensive_analysis.py: Deep codebase analysis with Codegen SDK
- api.py: FastAPI web service with metrics calculation
- analyzer.py: Advanced issue management and transaction support

Features:
- Comprehensive codebase analysis
- REST API endpoints for web access
- Advanced issue detection and management
- Multiple output formats (JSON, HTML, console)
- CLI and web interfaces

Generated by enhanced_consolidation.py
"""

# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =
# IMPORTS
# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =

import argparse
import hashlib
import json
import logging
import math
import os
import re
import requests
import socket
import subprocess
import sys
import tempfile
import time
import traceback

from collections import Counter
from dataclasses import asdict, dataclass, field
from datetime import datetime
from datetime import datetime, timedelta
from enum import Enum
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
from pydantic import BaseModel
from typing import Any, Dict, List, Optional, Set, Tuple, Union
from typing import Dict, List, Set, Tuple, Any, Optional, Union
from typing import Dict, List, Tuple, Any, Optional
import modal
import networkx as nx
import uvicorn

from codegen import Codebase
from codegen.configs.models.codebase import CodebaseConfig
from codegen.configs.models.secrets import SecretsConfig
from codegen.git.repo_operator.repo_operator import RepoOperator
from codegen.git.schemas.repo_config import RepoConfig
from codegen.sdk.codebase.config import ProjectConfig
from codegen.sdk.core.class_definition import Class
from codegen.sdk.core.codebase import Codebase
from codegen.sdk.core.expressions.binary_expression import BinaryExpression
from codegen.sdk.core.expressions.comparison_expression import ComparisonExpression
from codegen.sdk.core.expressions.unary_expression import UnaryExpression
from codegen.sdk.core.external_module import ExternalModule
from codegen.sdk.core.file import SourceFile
from codegen.sdk.core.function import Function
from codegen.sdk.core.import_resolution import Import
from codegen.sdk.core.statements.for_loop_statement import ForLoopStatement
from codegen.sdk.core.statements.if_block_statement import IfBlockStatement
from codegen.sdk.core.statements.try_catch_statement import TryCatchStatement
from codegen.sdk.core.statements.while_statement import WhileStatement
from codegen.sdk.core.symbol import Symbol
from codegen.sdk.enums import EdgeType, SymbolType
from codegen.shared.enums.programming_language import ProgrammingLanguage

from typing import Dict, List, Optional, Any

# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =
# FASTAPI APP INITIALIZATION
# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =

# Modal image configuration
image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .pip_install(
        "codegen", "fastapi", "uvicorn", "gitpython", "requests", "pydantic",
        "networkx", "datetime"
    )
)

app = modal.App(name="unified-analytics-app", image=image)
fastapi_app = FastAPI(title="Unified Codebase Analytics API", version="1.0.0")

fastapi_app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =
# CONSOLIDATED CLASSES
# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =

# From backend/comprehensive_analysis.py
class IssueType:
    """Types of issues that can be detected."""
    UNUSED_FUNCTION = 'Unused function'
    UNUSED_CLASS = 'Unused class'
    UNUSED_IMPORT = 'Unused import'
    UNUSED_PARAMETER = 'Unused parameter'
    PARAMETER_MISMATCH = 'Parameter mismatch'
    MISSING_TYPE_ANNOTATION = 'Missing type annotation'
    CIRCULAR_DEPENDENCY = 'Circular dependency'
    IMPLEMENTATION_ERROR = 'Implementation error'
    EMPTY_FUNCTION = 'Empty function'
    UNREACHABLE_CODE = 'Unreachable code'

# From backend/analyzer.py
class IssueSeverity(str, Enum):
    """Severity levels for issues."""
    CRITICAL = 'critical'
    ERROR = 'error'
    WARNING = 'warning'
    INFO = 'info'

# From backend/analyzer.py
@dataclass
class Issue:
    """Represents an issue found during analysis."""
    message: str
    severity: IssueSeverity
    location: CodeLocation
    category: IssueCategory | None = None
    analysis_type: AnalysisType | None = None
    status: IssueStatus = IssueStatus.OPEN
    symbol: str | None = None
    code: str | None = None
    suggestion: str | None = None
    related_symbols: list[str] = field(default_factory=list)
    related_locations: list[CodeLocation] = field(default_factory=list)
    id: str | None = None
    hash: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Initialize derived fields."""
        if self.id is None:
            import hashlib
            hash_input = f'{self.location.file}:{self.location.line}:{self.message}'
            self.id = hashlib.md5(hash_input.encode()).hexdigest()[:12]

    @property
    def file(self) -> str:
        """Get the file path."""
        return self.location.file

    @property
    def line(self) -> int | None:
        """Get the line number."""
        return self.location.line

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        result = {'id': self.id, 'message': self.message, 'severity': self.severity.value, 'location': self.location.to_dict(), 'status': self.status.value}
        if self.category:
            result['category'] = self.category.value
        if self.analysis_type:
            result['analysis_type'] = self.analysis_type.value
        if self.symbol:
            result['symbol'] = self.symbol
        if self.code:
            result['code'] = self.code
        if self.suggestion:
            result['suggestion'] = self.suggestion
        if self.related_symbols:
            result['related_symbols'] = self.related_symbols
        if self.related_locations:
            result['related_locations'] = [loc.to_dict() for loc in self.related_locations]
        if self.metadata:
            result['metadata'] = self.metadata
        return result

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> 'Issue':
        """Create from dictionary representation."""
        if 'severity' in data and isinstance(data['severity'], str):
            data['severity'] = IssueSeverity(data['severity'])
        if 'category' in data and isinstance(data['category'], str):
            data['category'] = IssueCategory(data['category'])
        if 'analysis_type' in data and isinstance(data['analysis_type'], str):
            data['analysis_type'] = AnalysisType(data['analysis_type'])
        if 'status' in data and isinstance(data['status'], str):
            data['status'] = IssueStatus(data['status'])
        if 'location' in data and isinstance(data['location'], dict):
            data['location'] = CodeLocation.from_dict(data['location'])
        if 'related_locations' in data and isinstance(data['related_locations'], list):
            data['related_locations'] = [CodeLocation.from_dict(loc) if isinstance(loc, dict) else loc for loc in data['related_locations']]
        return cls(**{k: v for k, v in data.items() if k in cls.__annotations__})

# From backend/comprehensive_analysis.py
class ComprehensiveAnalyzer:
    """
    Comprehensive analyzer for codebases using the Codegen SDK.
    Implements deep analysis of code issues, dependencies, and metrics.
    """

    def __init__(self, repo_path_or_url: str):
        """
        Initialize the analyzer with a repository path or URL.
        
        Args:
            repo_path_or_url: Path to local repo or URL to GitHub repo
        """
        self.repo_path_or_url = repo_path_or_url
        self.issues: List[Issue] = []
        self.start_time = time.time()
        self.codebase = None

    def analyze(self) -> Dict[str, Any]:
        """
        Perform a comprehensive analysis of the codebase.
        
        Returns:
            Dictionary with analysis results
        """
        print(f'Starting comprehensive analysis of {self.repo_path_or_url}...')
        try:
            print(f'Initializing codebase from {self.repo_path_or_url}')
            if self.repo_path_or_url.startswith(('http://', 'https://')):
                parts = self.repo_path_or_url.rstrip('/').split('/')
                repo_name = f'{parts[-2]}/{parts[-1]}'
                try:
                    self.codebase = Codebase.from_repo(repo_full_name=repo_name)
                    print(f'Successfully initialized codebase from GitHub repository: {repo_name}')
                except Exception as e:
                    print(f'Error initializing codebase from GitHub: {e}')
                    self.issues.append(Issue(self.repo_path_or_url, 'Initialization Error', f'Failed to initialize codebase from GitHub: {e}', IssueSeverity.ERROR, suggestion='Check your network connection and GitHub access permissions.'))
                    return {'error': f'Failed to initialize codebase: {str(e)}', 'success': False}
            else:
                try:
                    self.codebase = Codebase(self.repo_path_or_url)
                    print(f'Successfully initialized codebase from local path: {self.repo_path_or_url}')
                except Exception as e:
                    print(f'Error initializing codebase from local path: {e}')
                    self.issues.append(Issue(self.repo_path_or_url, 'Initialization Error', f'Failed to initialize codebase from local path: {e}', IssueSeverity.ERROR, suggestion='Ensure the path exists and contains valid source code.'))
                    return {'error': f'Failed to initialize codebase: {str(e)}', 'success': False}
            if not hasattr(self.codebase, 'files') or not self.codebase.files:
                self.issues.append(Issue(self.repo_path_or_url, 'Empty Codebase', 'Codebase was initialized but contains no files', IssueSeverity.ERROR, suggestion='Check if the repository contains supported language files.'))
                print('Warning: Codebase contains no files')
            try:
                self._analyze_dead_code()
            except Exception as e:
                print(f'Error in dead code analysis: {e}')
                self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Dead code analysis failed: {e}', IssueSeverity.ERROR))
            try:
                self._analyze_parameter_issues()
            except Exception as e:
                print(f'Error in parameter analysis: {e}')
                self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Parameter analysis failed: {e}', IssueSeverity.ERROR))
            try:
                self._analyze_type_annotations()
            except Exception as e:
                print(f'Error in type annotation analysis: {e}')
                self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Type annotation analysis failed: {e}', IssueSeverity.ERROR))
            try:
                self._analyze_circular_dependencies()
            except Exception as e:
                print(f'Error in circular dependency analysis: {e}')
                self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Circular dependency analysis failed: {e}', IssueSeverity.ERROR))
            try:
                self._analyze_implementation_issues()
            except Exception as e:
                print(f'Error in implementation issue analysis: {e}')
                self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Implementation issue analysis failed: {e}', IssueSeverity.ERROR))
            return self._generate_report()
        except Exception as e:
            print(f'Error analyzing codebase: {e}')
            import traceback
            traceback.print_exc()
            return {'error': str(e), 'success': False}

    def _analyze_dead_code(self):
        """Find and log unused code (functions, classes, imports)."""
        for func in self.codebase.functions:
            if not func.usages:
                self.issues.append(Issue(func, IssueType.UNUSED_FUNCTION, f'Unused function: {func.name}', IssueSeverity.WARNING, suggestion="Consider removing this unused function or documenting why it's needed"))
        for cls in self.codebase.classes:
            if not cls.usages:
                self.issues.append(Issue(cls, IssueType.UNUSED_CLASS, f'Unused class: {cls.name}', IssueSeverity.WARNING, suggestion="Consider removing this unused class or documenting why it's needed"))
        for file in self.codebase.files:
            for imp in file.imports:
                if not imp.usages:
                    self.issues.append(Issue(imp, IssueType.UNUSED_IMPORT, f"Unused import: {(imp.source if hasattr(imp, 'source') else str(imp))}", IssueSeverity.INFO, suggestion='Remove this unused import'))

    def _analyze_parameter_issues(self):
        """Find and log parameter issues (unused, mismatches)."""
        for func in self.codebase.functions:
            for param in func.parameters:
                if param.name == 'self' and func.is_method:
                    continue
                param_dependencies = [dep.name for dep in func.dependencies if hasattr(dep, 'name')]
                if param.name not in param_dependencies:
                    self.issues.append(Issue(func, IssueType.UNUSED_PARAMETER, f"Function '{func.name}' has unused parameter: {param.name}", IssueSeverity.INFO, suggestion=f"Consider removing the unused parameter '{param.name}' if it's not needed"))
            for call in func.call_sites:
                if hasattr(call, 'args') and hasattr(func, 'parameters'):
                    expected_params = set((p.name for p in func.parameters if not p.is_optional and p.name != 'self'))
                    actual_params = set()
                    if hasattr(call, 'args'):
                        for arg in call.args:
                            if hasattr(arg, 'parameter_name') and arg.parameter_name:
                                actual_params.add(arg.parameter_name)
                    missing = expected_params - actual_params
                    if missing:
                        has_kwargs = any((p.name.startswith('**') for p in func.parameters))
                        if not has_kwargs:
                            self.issues.append(Issue(call, IssueType.PARAMETER_MISMATCH, f"Call to '{func.name}' is missing parameters: {', '.join(missing)}", IssueSeverity.ERROR, suggestion='Add the missing parameters to the function call'))

    def _analyze_type_annotations(self):
        """Find and log missing type annotations."""
        for func in self.codebase.functions:
            file_path = str(func.file.path) if hasattr(func, 'file') and hasattr(func.file, 'path') else ''
            if any((file_ext in file_path for file_ext in ['.pyi'])):
                continue
            if not func.return_type and (not func.name.startswith('__')):
                self.issues.append(Issue(func, IssueType.MISSING_TYPE_ANNOTATION, f"Function '{func.name}' is missing return type annotation", IssueSeverity.INFO, suggestion='Add a return type annotation to improve type safety'))
            params_without_type = [p.name for p in func.parameters if not p.type and p.name != 'self' and (not p.name.startswith('*'))]
            if params_without_type:
                self.issues.append(Issue(func, IssueType.MISSING_TYPE_ANNOTATION, f"Function '{func.name}' has parameters without type annotations: {', '.join(params_without_type)}", IssueSeverity.INFO, suggestion='Add type annotations to all parameters'))

    def _analyze_circular_dependencies(self):
        """Find and log circular dependencies."""
        circular_deps = {}
        for file in self.codebase.files:
            visited = set()
            path = []
            self._check_circular_deps(file, visited, path, circular_deps)
        for file_path, cycles in circular_deps.items():
            for cycle in cycles:
                cycle_str = ' -> '.join([f.path for f in cycle])
                self.issues.append(Issue(file_path, IssueType.CIRCULAR_DEPENDENCY, f'Circular dependency detected: {cycle_str}', IssueSeverity.ERROR, suggestion='Refactor the code to break the circular dependency'))

    def _check_circular_deps(self, file, visited, path, circular_deps):
        """Helper method to check for circular dependencies using DFS."""
        if file in path:
            cycle = path[path.index(file):] + [file]
            if file.path not in circular_deps:
                circular_deps[file.path] = []
            circular_deps[file.path].append(cycle)
            return
        if file in visited:
            return
        visited.add(file)
        path.append(file)
        for imp in file.imports:
            if hasattr(imp, 'resolved_module') and imp.resolved_module:
                self._check_circular_deps(imp.resolved_module, visited, path.copy(), circular_deps)
        path.pop()

    def _analyze_implementation_issues(self):
        """Find and log implementation issues (empty functions, etc.)."""
        for func in self.codebase.functions:
            if func.name.startswith('__') and func.name.endswith('__'):
                continue
            if not func.body or not func.body.strip():
                is_override = False
                if hasattr(func, 'parent') and isinstance(func.parent, Class):
                    for parent_class in func.parent.parents:
                        if any((m.name == func.name for m in parent_class.methods)):
                            is_override = True
                            break
                if not is_override:
                    self.issues.append(Issue(func, IssueType.EMPTY_FUNCTION, f"Function '{func.name}' has an empty body", IssueSeverity.WARNING, suggestion="Implement the function or remove it if it's not needed"))

    def _generate_report(self) -> Dict[str, Any]:
        """
        Generate a comprehensive report of the analysis results.
        
        Returns:
            Dictionary with analysis results
        """
        analysis_duration = time.time() - self.start_time
        stats = {'total_files': len(list(self.codebase.files)), 'total_functions': len(list(self.codebase.functions)), 'total_classes': len(list(self.codebase.classes)), 'total_imports': len(list(self.codebase.imports)), 'total_issues': len(self.issues), 'analysis_duration': analysis_duration}
        issues_by_severity = {}
        for issue in self.issues:
            if issue.severity not in issues_by_severity:
                issues_by_severity[issue.severity] = []
            issues_by_severity[issue.severity].append(issue)
        issues_by_type = {}
        for issue in self.issues:
            if issue.type not in issues_by_type:
                issues_by_type[issue.type] = []
            issues_by_type[issue.type].append(issue)
        report = {'success': True, 'repo': self.repo_path_or_url, 'timestamp': datetime.now().isoformat(), 'duration': analysis_duration, 'stats': stats, 'issues': {'all': [str(issue) for issue in self.issues], 'by_severity': {sev: [str(issue) for issue in issues] for sev, issues in issues_by_severity.items()}, 'by_type': {type_: [str(issue) for issue in issues] for type_, issues in issues_by_type.items()}}, 'counts': {'unused_functions': len(issues_by_type.get(IssueType.UNUSED_FUNCTION, [])), 'unused_classes': len(issues_by_type.get(IssueType.UNUSED_CLASS, [])), 'unused_imports': len(issues_by_type.get(IssueType.UNUSED_IMPORT, [])), 'parameter_issues': len(issues_by_type.get(IssueType.UNUSED_PARAMETER, [])) + len(issues_by_type.get(IssueType.PARAMETER_MISMATCH, [])), 'type_annotation_issues': len(issues_by_type.get(IssueType.MISSING_TYPE_ANNOTATION, [])), 'circular_dependencies': len(issues_by_type.get(IssueType.CIRCULAR_DEPENDENCY, [])), 'implementation_issues': len(issues_by_type.get(IssueType.IMPLEMENTATION_ERROR, [])) + len(issues_by_type.get(IssueType.EMPTY_FUNCTION, []))}}
        self._print_report(report)
        self._save_report(report)
        return report

    def _print_report(self, report: Dict[str, Any]):
        """Print the analysis report to the console."""
        print('\n' + '=' * 80)
        print(f'ðŸ” COMPREHENSIVE CODEBASE ANALYSIS: {self.repo_path_or_url}')
        print('=' * 80)
        print(f"Analysis Time: {report['timestamp']}")
        print(f"Analysis Duration: {report['duration']:.2f} seconds")
        print('\n' + '-' * 40)
        print('CODEBASE STATISTICS:')
        print('-' * 40)
        print(f"Total Files: {report['stats']['total_files']}")
        print(f"Total Functions: {report['stats']['total_functions']}")
        print(f"Total Classes: {report['stats']['total_classes']}")
        print(f"Total Imports: {report['stats']['total_imports']}")
        print(f"Total Issues: {report['stats']['total_issues']}")
        print('\n' + '-' * 40)
        print('ISSUES BY TYPE:')
        print('-' * 40)
        for issue_type, count in report['counts'].items():
            print(f"{issue_type.replace('_', ' ').title()}: {count}")
        print('\n' + '-' * 40)
        print('ISSUES BY SEVERITY:')
        print('-' * 40)
        for severity, issues in report['issues']['by_severity'].items():
            print(f'{severity.upper()}: {len(issues)}')
        print('\n' + '-' * 40)
        print('TOP ISSUES:')
        print('-' * 40)
        for i, issue in enumerate(report['issues']['all'][:10], 1):
            print(f'{i}. {issue}')
        if len(report['issues']['all']) > 10:
            print(f"... and {len(report['issues']['all']) - 10} more issues")

    def _save_report(self, report: Dict[str, Any]):
        """Save the analysis report to a file."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'comprehensive_analysis_{timestamp}.txt'
        detailed_filename = f'detailed_analysis_{timestamp}.txt'
        print(f'\nSaving full analysis to {filename}...')
        with open(filename, 'w') as f:
            f.write(f'COMPREHENSIVE CODEBASE ANALYSIS: {self.repo_path_or_url}\n')
            f.write(f"Analysis Time: {report['timestamp']}\n")
            f.write(f"Analysis Duration: {report['duration']:.2f} seconds\n\n")
            f.write('CODEBASE STATISTICS:\n')
            f.write(f"Total Files: {report['stats']['total_files']}\n")
            f.write(f"Total Functions: {report['stats']['total_functions']}\n")
            f.write(f"Total Classes: {report['stats']['total_classes']}\n")
            f.write(f"Total Imports: {report['stats']['total_imports']}\n")
            f.write(f"Total Issues: {report['stats']['total_issues']}\n\n")
            f.write('ISSUES BY TYPE:\n')
            for issue_type, count in report['counts'].items():
                f.write(f"{issue_type.replace('_', ' ').title()}: {count}\n")
            f.write('\n')
            f.write('ISSUES BY SEVERITY:\n')
            for severity, issues in report['issues']['by_severity'].items():
                f.write(f'{severity.upper()}: {len(issues)}\n')
            f.write('\n')
            f.write('ALL ISSUES:\n')
            for i, issue in enumerate(report['issues']['all'], 1):
                f.write(f'{i}. {issue}\n')
            f.write('\n')
            f.write('=' * 80 + '\n')
            f.write('NOTE: Detailed summaries of codebase elements are available in a separate file\n')
            f.write(f'See: {detailed_filename}\n')
            f.write('=' * 80 + '\n')
        try:
            print(f'Saving detailed analysis to {detailed_filename}...')
            self._save_detailed_summaries(detailed_filename)
            print(f'Detailed summaries saved to {detailed_filename}')
        except Exception as e:
            print(f'Error saving detailed summaries: {e}')
            import traceback
            traceback.print_exc()
        print(f'Analysis results saved to {filename}')

    def _save_detailed_summaries(self, filename: str):
        """Save detailed summaries of the codebase, files, classes, and functions."""
        with open(filename, 'w') as f:
            f.write('=' * 80 + '\n')
            f.write(' ' * 20 + 'COMPREHENSIVE CODEBASE ANALYSIS DETAILS\n')
            f.write(' ' * 20 + f'Repository: {self.repo_path_or_url}\n')
            f.write(' ' * 20 + f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write('=' * 80 + '\n\n')
            try:
                f.write('=' * 80 + '\n')
                f.write(' ' * 30 + 'CODEBASE SUMMARY\n')
                f.write('=' * 80 + '\n')
                f.write(get_codebase_summary(self.codebase))
                f.write('\n\n')
            except Exception as e:
                f.write(f'Error generating codebase summary: {str(e)}\n\n')
            try:
                f.write('=' * 80 + '\n')
                f.write(' ' * 30 + 'FILE SUMMARIES\n')
                f.write('=' * 80 + '\n')
                files = list(self.codebase.files)
                for i, file in enumerate(files):
                    f.write(f"File {i + 1}/{len(files)}: {getattr(file, 'path', 'Unknown path')}\n")
                    f.write('-' * 60 + '\n')
                    try:
                        f.write(get_file_summary(file))
                    except Exception as e:
                        f.write(f'Error generating file summary: {str(e)}\n')
                    f.write('\n\n')
            except Exception as e:
                f.write(f'Error processing file summaries: {str(e)}\n\n')
            try:
                f.write('=' * 80 + '\n')
                f.write(' ' * 30 + 'CLASS SUMMARIES\n')
                f.write('=' * 80 + '\n')
                classes = list(self.codebase.classes)
                if classes:
                    for i, cls in enumerate(classes):
                        f.write(f"Class {i + 1}/{len(classes)}: {getattr(cls, 'name', 'Unknown class')}\n")
                        f.write('-' * 60 + '\n')
                        try:
                            f.write(get_class_summary(cls))
                        except Exception as e:
                            f.write(f'Error generating class summary: {str(e)}\n')
                        f.write('\n\n')
                else:
                    f.write('No classes found in the codebase.\n\n')
            except Exception as e:
                f.write(f'Error processing class summaries: {str(e)}\n\n')
            try:
                f.write('=' * 80 + '\n')
                f.write(' ' * 30 + 'FUNCTION SUMMARIES\n')
                f.write('=' * 80 + '\n')
                functions = list(self.codebase.functions)
                if functions:
                    max_funcs = min(30, len(functions))
                    f.write(f'Showing {max_funcs} of {len(functions)} functions\n\n')
                    for i, func in enumerate(functions[:max_funcs]):
                        f.write(f"Function {i + 1}/{max_funcs}: {getattr(func, 'name', 'Unknown function')}\n")
                        f.write('-' * 60 + '\n')
                        try:
                            f.write(get_function_summary(func))
                        except Exception as e:
                            f.write(f'Error generating function summary: {str(e)}\n')
                        f.write('\n\n')
                    if len(functions) > max_funcs:
                        f.write(f'... and {len(functions) - max_funcs} more functions not shown\n\n')
                else:
                    f.write('No functions found in the codebase.\n\n')
            except Exception as e:
                f.write(f'Error processing function summaries: {str(e)}\n\n')
            try:
                f.write('=' * 80 + '\n')
                f.write(' ' * 30 + 'KEY SYMBOL USAGE SUMMARIES\n')
                f.write('=' * 80 + '\n')
                symbols = list(self.codebase.symbols)
                if symbols:
                    try:
                        symbols_with_usage = [(s, len(getattr(s, 'usages', []))) for s in symbols]
                        sorted_symbols = [s for s, _ in sorted(symbols_with_usage, key=lambda x: x[1], reverse=True)]
                        top_symbols = sorted_symbols[:10]
                    except:
                        top_symbols = symbols[:10]
                    for i, symbol in enumerate(top_symbols):
                        f.write(f"Symbol {i + 1}/{len(top_symbols)}: {getattr(symbol, 'name', 'Unknown symbol')}\n")
                        f.write('-' * 60 + '\n')
                        try:
                            f.write(get_symbol_summary(symbol))
                        except Exception as e:
                            f.write(f'Error generating symbol summary: {str(e)}\n')
                        f.write('\n\n')
                else:
                    f.write('No symbols found for detailed analysis.\n\n')
            except Exception as e:
                f.write(f'Error processing symbol summaries: {str(e)}\n\n')
            f.write('=' * 80 + '\n')
            f.write(' ' * 25 + 'END OF DETAILED CODEBASE ANALYSIS\n')
            f.write('=' * 80 + '\n')

# From backend/api.py
class CodebaseStats(BaseModel):
    test_functions_count: int
    test_classes_count: int
    tests_per_file: float
    total_classes: int
    total_functions: int
    total_imports: int
    deepest_inheritance_class: Optional[Dict]
    recursive_functions: List[str]
    most_called_function: Dict
    function_with_most_calls: Dict
    unused_functions: List[Dict]
    dead_code: List[Dict]

# From backend/api.py
class FileTestStats(BaseModel):
    filepath: str
    test_class_count: int
    file_length: int
    function_count: int

# From backend/api.py
class FunctionContext(BaseModel):
    implementation: Dict
    dependencies: List[Dict]
    usages: List[Dict]

# From backend/api.py
class TestAnalysis(BaseModel):
    total_test_functions: int
    total_test_classes: int
    tests_per_file: float
    top_test_files: List[Dict[str, Any]]

# From backend/api.py
class FunctionAnalysis(BaseModel):
    total_functions: int
    most_called_function: Dict[str, Any]
    function_with_most_calls: Dict[str, Any]
    recursive_functions: List[str]
    unused_functions: List[Dict[str, str]]
    dead_code: List[Dict[str, str]]

# From backend/api.py
class ClassAnalysis(BaseModel):
    total_classes: int
    deepest_inheritance: Optional[Dict[str, Any]]
    total_imports: int

# From backend/api.py
class FileIssue(BaseModel):
    critical: List[Dict[str, str]]
    major: List[Dict[str, str]]
    minor: List[Dict[str, str]]

# From backend/api.py
class ExtendedAnalysis(BaseModel):
    test_analysis: TestAnalysis
    function_analysis: FunctionAnalysis
    class_analysis: ClassAnalysis
    file_issues: Dict[str, FileIssue]
    repo_structure: Dict[str, Any]

# From backend/api.py
class RepoRequest(BaseModel):
    repo_url: str

# From backend/api.py
class Symbol(BaseModel):
    id: str
    name: str
    type: str
    filepath: str
    start_line: int
    end_line: int
    issues: Optional[List[Dict[str, str]]] = None

# From backend/api.py
class FileNode(BaseModel):
    name: str
    type: str
    path: str
    issues: Optional[Dict[str, int]] = None
    symbols: Optional[List[Symbol]] = None
    children: Optional[Dict[str, 'FileNode']] = None

# From backend/api.py
class AnalysisResponse(BaseModel):
    repo_url: str
    description: str
    num_files: int
    num_functions: int
    num_classes: int
    line_metrics: Dict[str, Dict[str, float]]
    cyclomatic_complexity: Dict[str, float]
    depth_of_inheritance: Dict[str, float]
    halstead_metrics: Dict[str, int]
    maintainability_index: Dict[str, int]
    monthly_commits: Dict[str, int]
    repo_structure: FileNode

# From backend/analyzer.py
class AnalysisType(str, Enum):
    """Types of analysis that can be performed."""
    CODEBASE = 'codebase'
    PR = 'pr'
    COMPARISON = 'comparison'
    CODE_QUALITY = 'code_quality'
    DEPENDENCY = 'dependency'
    SECURITY = 'security'
    PERFORMANCE = 'performance'
    TYPE_CHECKING = 'type_checking'
    COMPREHENSIVE = 'comprehensive'

# From backend/analyzer.py
class IssueCategory(str, Enum):
    """Categories of issues that can be detected."""
    DEAD_CODE = 'dead_code'
    COMPLEXITY = 'complexity'
    STYLE_ISSUE = 'style_issue'
    DOCUMENTATION = 'documentation'
    TYPE_ERROR = 'type_error'
    PARAMETER_MISMATCH = 'parameter_mismatch'
    RETURN_TYPE_ERROR = 'return_type_error'
    IMPLEMENTATION_ERROR = 'implementation_error'
    MISSING_IMPLEMENTATION = 'missing_implementation'
    IMPORT_ERROR = 'import_error'
    DEPENDENCY_CYCLE = 'dependency_cycle'
    MODULE_COUPLING = 'module_coupling'
    API_CHANGE = 'api_change'
    API_USAGE_ERROR = 'api_usage_error'
    SECURITY_VULNERABILITY = 'security_vulnerability'
    PERFORMANCE_ISSUE = 'performance_issue'

# From backend/analyzer.py
class IssueStatus(str, Enum):
    """Status of an issue."""
    OPEN = 'open'
    FIXED = 'fixed'
    WONTFIX = 'wontfix'
    INVALID = 'invalid'
    DUPLICATE = 'duplicate'

# From backend/analyzer.py
class ChangeType(str, Enum):
    """Type of change for a diff."""
    Added = 'added'
    Removed = 'removed'
    Modified = 'modified'
    Renamed = 'renamed'

# From backend/analyzer.py
class TransactionPriority(int, Enum):
    """Priority levels for transactions."""
    HIGH = 0
    MEDIUM = 5
    LOW = 10

# From backend/analyzer.py
@dataclass
class CodeLocation:
    """Location of an issue in code."""
    file: str
    line: int | None = None
    column: int | None = None
    end_line: int | None = None
    end_column: int | None = None

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        return {k: v for k, v in asdict(self).items() if v is not None}

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> 'CodeLocation':
        """Create from dictionary representation."""
        return cls(**{k: v for k, v in data.items() if k in cls.__annotations__})

    def __str__(self) -> str:
        """Convert to string representation."""
        if self.line is not None:
            if self.column is not None:
                return f'{self.file}:{self.line}:{self.column}'
            return f'{self.file}:{self.line}'
        return self.file

# From backend/analyzer.py
class IssueCollection:
    """Collection of issues with filtering and grouping capabilities."""

    def __init__(self, issues: list[Issue] | None=None):
        """
        Initialize the issue collection.

        Args:
            issues: Initial list of issues
        """
        self.issues = issues or []
        self._filters = []

    def add_issue(self, issue: Issue):
        """
        Add an issue to the collection.

        Args:
            issue: Issue to add
        """
        self.issues.append(issue)

    def add_issues(self, issues: list[Issue]):
        """
        Add multiple issues to the collection.

        Args:
            issues: Issues to add
        """
        self.issues.extend(issues)

    def add_filter(self, filter_func, description: str=''):
        """
        Add a filter function.

        Args:
            filter_func: Function that returns True if issue should be included
            description: Description of the filter
        """
        self._filters.append((filter_func, description))

    def get_issues(self, severity: IssueSeverity | None=None, category: IssueCategory | None=None, status: IssueStatus | None=None, file_path: str | None=None, symbol: str | None=None) -> list[Issue]:
        """
        Get issues matching the specified criteria.

        Args:
            severity: Severity to filter by
            category: Category to filter by
            status: Status to filter by
            file_path: File path to filter by
            symbol: Symbol name to filter by

        Returns:
            List of matching issues
        """
        filtered_issues = self.issues
        for filter_func, _ in self._filters:
            filtered_issues = [i for i in filtered_issues if filter_func(i)]
        if severity:
            filtered_issues = [i for i in filtered_issues if i.severity == severity]
        if category:
            filtered_issues = [i for i in filtered_issues if i.category == category]
        if status:
            filtered_issues = [i for i in filtered_issues if i.status == status]
        if file_path:
            filtered_issues = [i for i in filtered_issues if i.location.file == file_path]
        if symbol:
            filtered_issues = [i for i in filtered_issues if i.symbol == symbol or (i.related_symbols and symbol in i.related_symbols)]
        return filtered_issues

    def group_by_severity(self) -> dict[IssueSeverity, list[Issue]]:
        """
        Group issues by severity.

        Returns:
            Dictionary mapping severities to lists of issues
        """
        result = {severity: [] for severity in IssueSeverity}
        for issue in self.issues:
            result[issue.severity].append(issue)
        return result

    def group_by_category(self) -> dict[IssueCategory, list[Issue]]:
        """
        Group issues by category.

        Returns:
            Dictionary mapping categories to lists of issues
        """
        result = {category: [] for category in IssueCategory}
        for issue in self.issues:
            if issue.category:
                result[issue.category].append(issue)
        return result

    def group_by_file(self) -> dict[str, list[Issue]]:
        """
        Group issues by file.

        Returns:
            Dictionary mapping file paths to lists of issues
        """
        result = {}
        for issue in self.issues:
            if issue.location.file not in result:
                result[issue.location.file] = []
            result[issue.location.file].append(issue)
        return result

    def statistics(self) -> dict[str, Any]:
        """
        Get statistics about the issues.

        Returns:
            Dictionary with issue statistics
        """
        by_severity = self.group_by_severity()
        by_category = self.group_by_category()
        by_status = {status: [] for status in IssueStatus}
        for issue in self.issues:
            by_status[issue.status].append(issue)
        return {'total': len(self.issues), 'by_severity': {severity.value: len(issues) for severity, issues in by_severity.items()}, 'by_category': {category.value: len(issues) for category, issues in by_category.items() if len(issues) > 0}, 'by_status': {status.value: len(issues) for status, issues in by_status.items()}, 'file_count': len(self.group_by_file())}

    def to_dict(self) -> dict[str, Any]:
        """
        Convert to dictionary representation.

        Returns:
            Dictionary representation of the issue collection
        """
        return {'issues': [issue.to_dict() for issue in self.issues], 'statistics': self.statistics(), 'filters': [desc for _, desc in self._filters if desc]}

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> 'IssueCollection':
        """
        Create from dictionary representation.

        Args:
            data: Dictionary representation

        Returns:
            Issue collection
        """
        collection = cls()
        if 'issues' in data and isinstance(data['issues'], list):
            collection.add_issues([Issue.from_dict(issue) if isinstance(issue, dict) else issue for issue in data['issues']])
        return collection

    def save_to_file(self, file_path: str, format: str='json'):
        """
        Save to file.

        Args:
            file_path: Path to save to
            format: Format to save in
        """
        if format == 'json':
            with open(file_path, 'w') as f:
                json.dump(self.to_dict(), f, indent=2)
        else:
            raise ValueError(f'Unsupported format: {format}')

    @classmethod
    def load_from_file(cls, file_path: str) -> 'IssueCollection':
        """
        Load from file.

        Args:
            file_path: Path to load from

        Returns:
            Issue collection
        """
        with open(file_path) as f:
            data = json.load(f)
        return cls.from_dict(data)

# From backend/analyzer.py
@dataclass
class AnalysisSummary:
    """Summary statistics for an analysis."""
    total_files: int = 0
    total_functions: int = 0
    total_classes: int = 0
    total_issues: int = 0
    analysis_time: str = field(default_factory=lambda: datetime.now().isoformat())
    analysis_duration_ms: int | None = None

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        return {k: v for k, v in asdict(self).items() if v is not None}

# From backend/analyzer.py
@dataclass
class CodeQualityResult:
    """Results of code quality analysis."""
    dead_code: dict[str, Any] = field(default_factory=dict)
    complexity: dict[str, Any] = field(default_factory=dict)
    parameter_issues: dict[str, Any] = field(default_factory=dict)
    style_issues: dict[str, Any] = field(default_factory=dict)
    implementation_issues: dict[str, Any] = field(default_factory=dict)
    maintainability: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        return dict(asdict(self).items())

# From backend/analyzer.py
@dataclass
class DependencyResult:
    """Results of dependency analysis."""
    import_dependencies: dict[str, Any] = field(default_factory=dict)
    circular_dependencies: dict[str, Any] = field(default_factory=dict)
    module_coupling: dict[str, Any] = field(default_factory=dict)
    external_dependencies: dict[str, Any] = field(default_factory=dict)
    call_graph: dict[str, Any] = field(default_factory=dict)
    class_hierarchy: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        return dict(asdict(self).items())

# From backend/analyzer.py
@dataclass
class AnalysisResult:
    """Comprehensive analysis result."""
    analysis_types: list[AnalysisType]
    summary: AnalysisSummary = field(default_factory=AnalysisSummary)
    issues: IssueCollection = field(default_factory=IssueCollection)
    code_quality: CodeQualityResult | None = None
    dependencies: DependencyResult | None = None
    metadata: dict[str, Any] = field(default_factory=dict)
    repo_name: str | None = None
    repo_path: str | None = None
    language: str | None = None

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        result = {'analysis_types': [at.value for at in self.analysis_types], 'summary': self.summary.to_dict(), 'issues': self.issues.to_dict(), 'metadata': self.metadata}
        if self.repo_name:
            result['repo_name'] = self.repo_name
        if self.repo_path:
            result['repo_path'] = self.repo_path
        if self.language:
            result['language'] = self.language
        if self.code_quality:
            result['code_quality'] = self.code_quality.to_dict()
        if self.dependencies:
            result['dependencies'] = self.dependencies.to_dict()
        return result

    def save_to_file(self, file_path: str, indent: int=2):
        """
        Save analysis result to a file.

        Args:
            file_path: Path to save to
            indent: JSON indentation level
        """
        with open(file_path, 'w') as f:
            json.dump(self.to_dict(), f, indent=indent)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> 'AnalysisResult':
        """
        Create analysis result from dictionary.

        Args:
            data: Dictionary representation

        Returns:
            Analysis result object
        """
        analysis_types = [AnalysisType(at) if isinstance(at, str) else at for at in data.get('analysis_types', [])]
        summary = AnalysisSummary(**data.get('summary', {})) if 'summary' in data else AnalysisSummary()
        issues = IssueCollection.from_dict(data.get('issues', {})) if 'issues' in data else IssueCollection()
        result = cls(analysis_types=analysis_types, summary=summary, issues=issues, repo_name=data.get('repo_name'), repo_path=data.get('repo_path'), language=data.get('language'), metadata=data.get('metadata', {}))
        if 'code_quality' in data:
            result.code_quality = CodeQualityResult(**data['code_quality'])
        if 'dependencies' in data:
            result.dependencies = DependencyResult(**data['dependencies'])
        return result

    @classmethod
    def load_from_file(cls, file_path: str) -> 'AnalysisResult':
        """
        Load analysis result from file.

        Args:
            file_path: Path to load from

        Returns:
            Analysis result object
        """
        with open(file_path) as f:
            data = json.load(f)
        return cls.from_dict(data)

# From backend/analyzer.py
class CodebaseAnalyzer:
    """
    Comprehensive code analyzer for detecting issues and analyzing codebase structure.
    
    This class provides a unified interface for analyzing codebases, integrating
    functionality from various analyzer modules to detect code issues, analyze
    dependencies, and provide insights into code quality and structure.
    """

    def __init__(self, repo_url: str | None=None, repo_path: str | None=None, base_branch: str='main', language: str | None=None, file_ignore_list: list[str] | None=None, config: dict[str, Any] | None=None):
        """
        Initialize the codebase analyzer.

        Args:
            repo_url: URL of the repository to analyze
            repo_path: Local path to the repository to analyze
            base_branch: Base branch for comparison
            language: Programming language of the codebase
            file_ignore_list: List of file patterns to ignore
            config: Additional configuration options
        """
        self.repo_url = repo_url
        self.repo_path = repo_path
        self.base_branch = base_branch
        self.language = language
        self.file_ignore_list = file_ignore_list or []
        self.config = config or {}
        self.codebase = None
        self.issues = IssueCollection()
        self.analysis_result = None
        if repo_url:
            self._init_from_url(repo_url, language)
        elif repo_path:
            self._init_from_path(repo_path, language)

    def _init_from_url(self, repo_url: str, language: str | None=None):
        """
        Initialize codebase from a repository URL.

        Args:
            repo_url: URL of the repository
            language: Programming language of the codebase
        """
        try:
            if repo_url.endswith('.git'):
                repo_url = repo_url[:-4]
            parts = repo_url.rstrip('/').split('/')
            repo_name = parts[-1]
            owner = parts[-2]
            repo_full_name = f'{owner}/{repo_name}'
            tmp_dir = tempfile.mkdtemp(prefix='analyzer_')
            config = CodebaseConfig(debug=False, allow_external=True, py_resolve_syspath=True)
            secrets = SecretsConfig()
            prog_lang = None
            if language:
                prog_lang = ProgrammingLanguage(language.upper())
            logger.info(f'Initializing codebase from {repo_url}')
            self.codebase = Codebase.from_repo(repo_full_name=repo_full_name, tmp_dir=tmp_dir, language='python', config=config, secrets=secrets)
            logger.info(f'Successfully initialized codebase from {repo_url}')
        except Exception as e:
            logger.exception(f'Error initializing codebase from URL: {e}')
            raise

    def _init_from_path(self, repo_path: str, language: str | None=None):
        """
        Initialize codebase from a local repository path.

        Args:
            repo_path: Path to the repository
            language: Programming language of the codebase
        """
        try:
            config = CodebaseConfig(debug=False, allow_external=True, py_resolve_syspath=True)
            secrets = SecretsConfig()
            logger.info(f'Initializing codebase from {repo_path}')
            prog_lang = None
            if language:
                prog_lang = ProgrammingLanguage(language.upper())
            repo_config = RepoConfig.from_repo_path(repo_path)
            repo_config.respect_gitignore = False
            repo_operator = RepoOperator(repo_config=repo_config, bot_commit=False)
            project_config = ProjectConfig(repo_operator=repo_operator, programming_language=prog_lang if prog_lang else None)
            self.codebase = Codebase(projects=[project_config], config=config, secrets=secrets)
            logger.info(f'Successfully initialized codebase from {repo_path}')
        except Exception as e:
            logger.exception(f'Error initializing codebase from path: {e}')
            raise

    def analyze(self, analysis_types: list[AnalysisType] | None=None, output_format: str='json', output_file: str | None=None) -> AnalysisResult:
        """
        Perform comprehensive analysis on the codebase.

        Args:
            analysis_types: Types of analysis to perform. Defaults to code quality analysis.
            output_format: Format of the output (json, html, console)
            output_file: Path to save results to

        Returns:
            AnalysisResult containing the findings
        """
        if not self.codebase:
            raise ValueError('Codebase not initialized')
        if not analysis_types:
            analysis_types = [AnalysisType.CODE_QUALITY]
        start_time = datetime.now()
        self.analysis_result = AnalysisResult(analysis_types=analysis_types, repo_name=self.repo_url.split('/')[-1] if self.repo_url else None, repo_path=self.repo_path, language=self.language)
        logger.info(f'Starting analysis with types: {[at.value for at in analysis_types]}')
        if AnalysisType.COMPREHENSIVE in analysis_types:
            logger.info('Running comprehensive analysis (includes all analysis types)')
            self._analyze_code_quality()
            self._analyze_dependencies()
            self._analyze_performance()
            self._analyze_parameter_issues()
            self._analyze_type_annotations()
            self._analyze_circular_dependencies()
        else:
            if AnalysisType.CODE_QUALITY in analysis_types:
                self._analyze_code_quality()
            if AnalysisType.DEPENDENCY in analysis_types:
                self._analyze_dependencies()
            if AnalysisType.PERFORMANCE in analysis_types:
                self._analyze_performance()
        analysis_duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        self.analysis_result.summary.analysis_duration_ms = analysis_duration_ms
        self.analysis_result.summary.total_files = len(list(self.codebase.files))
        self.analysis_result.summary.total_functions = len(list(self.codebase.functions))
        self.analysis_result.summary.total_classes = len(list(self.codebase.classes))
        self.analysis_result.summary.total_issues = len(self.issues.issues)
        self.analysis_result.issues = self.issues
        if output_file and output_format == 'json':
            self.analysis_result.save_to_file(output_file)
            logger.info(f'Results saved to {output_file}')
        elif output_format == 'html':
            self._generate_html_report(output_file)
        elif output_format == 'console':
            self._print_console_report()
        return self.analysis_result

    def _analyze_code_quality(self):
        """Analyze code quality issues in the codebase."""
        logger.info('Starting code quality analysis')
        result = CodeQualityResult()
        result.dead_code = self._find_dead_code()
        result.parameter_issues = self._check_function_parameters()
        result.implementation_issues = self._check_implementations()
        self.analysis_result.code_quality = result
        logger.info(f'Code quality analysis complete. Found {len(self.issues.issues)} issues.')

    def _analyze_dependencies(self):
        """Analyze dependencies between components in the codebase."""
        logger.info('Starting dependency analysis')
        result = DependencyResult()
        result.import_dependencies = self._analyze_import_dependencies()
        result.circular_dependencies = self._find_circular_dependencies()
        self.analysis_result.dependencies = result
        logger.info('Dependency analysis complete')

    def _analyze_performance(self):
        """Analyze performance characteristics of the codebase."""
        logger.info('Starting performance analysis')
        functions = list(self.codebase.functions)
        for func in functions:
            complexity = calculate_cyclomatic_complexity(func)
            if complexity > 15:
                file_path = func.file.file_path if hasattr(func, 'file') and hasattr(func.file, 'file_path') else 'unknown'
                func_name = func.name if hasattr(func, 'name') else str(func)
                self.issues.add_issue(create_issue(message=f'High complexity function may be a performance bottleneck: {func_name} (complexity: {complexity})', severity=IssueSeverity.WARNING, file=file_path, line=func.line if hasattr(func, 'line') else None, category=IssueCategory.PERFORMANCE_ISSUE, symbol=func_name, suggestion='Consider refactoring this function to reduce complexity and improve performance'))
            if hasattr(func, 'function_calls'):
                for call in func.function_calls:
                    if hasattr(call, 'name') and call.name == func.name:
                        file_path = func.file.file_path if hasattr(func, 'file') and hasattr(func.file, 'file_path') else 'unknown'
                        func_name = func.name if hasattr(func, 'name') else str(func)
                        self.issues.add_issue(create_issue(message=f'Recursive function may cause performance issues: {func_name}', severity=IssueSeverity.INFO, file=file_path, line=func.line if hasattr(func, 'line') else None, category=IssueCategory.PERFORMANCE_ISSUE, symbol=func_name, suggestion="Ensure recursive function has a proper base case and won't cause stack overflow"))
        logger.info('Performance analysis complete')

    def _find_dead_code(self) -> dict[str, Any]:
        """
        Find unused code (dead code) in the codebase.

        Returns:
            Dictionary containing dead code analysis results
        """
        logger.info('Analyzing dead code')
        dead_code = {'unused_functions': [], 'unused_classes': [], 'unused_variables': [], 'unused_imports': []}
        for function in self.codebase.functions:
            if hasattr(function, 'decorators') and function.decorators:
                continue
            has_call_sites = hasattr(function, 'call_sites') and len(function.call_sites) > 0
            has_usages = hasattr(function, 'usages') and len(function.usages) > 0
            if not has_call_sites and (not has_usages):
                if hasattr(function, 'is_magic') and function.is_magic or (hasattr(function, 'name') and function.name in ['main', '__main__']):
                    continue
                file_path = function.file.file_path if hasattr(function, 'file') and hasattr(function.file, 'file_path') else 'unknown'
                func_name = function.name if hasattr(function, 'name') else str(function)
                dead_code['unused_functions'].append({'name': func_name, 'file': file_path, 'line': function.line if hasattr(function, 'line') else None})
                self.issues.add_issue(create_issue(message=f'Unused function: {func_name}', severity=IssueSeverity.WARNING, file=file_path, line=function.line if hasattr(function, 'line') else None, category=IssueCategory.DEAD_CODE, symbol=func_name, suggestion="Consider removing this unused function or documenting why it's needed"))
        for cls in self.codebase.classes:
            has_usages = hasattr(cls, 'usages') and len(cls.usages) > 0
            if not has_usages:
                file_path = cls.file.file_path if hasattr(cls, 'file') and hasattr(cls.file, 'file_path') else 'unknown'
                cls_name = cls.name if hasattr(cls, 'name') else str(cls)
                dead_code['unused_classes'].append({'name': cls_name, 'file': file_path, 'line': cls.line if hasattr(cls, 'line') else None})
                self.issues.add_issue(create_issue(message=f'Unused class: {cls_name}', severity=IssueSeverity.WARNING, file=file_path, line=cls.line if hasattr(cls, 'line') else None, category=IssueCategory.DEAD_CODE, symbol=cls_name, suggestion="Consider removing this unused class or documenting why it's needed"))
        for file in self.codebase.files:
            if hasattr(file, 'is_binary') and file.is_binary:
                continue
            if not hasattr(file, 'imports'):
                continue
            file_path = file.file_path if hasattr(file, 'file_path') else str(file)
            for imp in file.imports:
                if not hasattr(imp, 'usages'):
                    continue
                if len(imp.usages) == 0:
                    import_source = imp.source if hasattr(imp, 'source') else str(imp)
                    dead_code['unused_imports'].append({'import': import_source, 'file': file_path, 'line': imp.line if hasattr(imp, 'line') else None})
                    location = CodeLocation(file=file_path, line=imp.line if hasattr(imp, 'line') else None)
                    self.issues.add_issue(Issue(message=f'Unused import: {import_source}', severity=IssueSeverity.INFO, location=location, category=IssueCategory.DEAD_CODE, suggestion='Remove this unused import'))
        dead_code['summary'] = {'unused_functions_count': len(dead_code['unused_functions']), 'unused_classes_count': len(dead_code['unused_classes']), 'unused_variables_count': len(dead_code['unused_variables']), 'unused_imports_count': len(dead_code['unused_imports']), 'total_dead_code_count': len(dead_code['unused_functions']) + len(dead_code['unused_classes']) + len(dead_code['unused_variables']) + len(dead_code['unused_imports'])}
        return dead_code

    def _check_function_parameters(self) -> dict[str, Any]:
        """
        Check for function parameter issues.

        Returns:
            Dictionary containing parameter analysis results
        """
        logger.info('Analyzing function parameters')
        parameter_issues = {'missing_types': [], 'unused_parameters': [], 'incorrect_usage': []}
        for function in self.codebase.functions:
            if not hasattr(function, 'parameters'):
                continue
            file_path = function.file.file_path if hasattr(function, 'file') and hasattr(function.file, 'file_path') else 'unknown'
            func_name = function.name if hasattr(function, 'name') else str(function)
            missing_types = []
            for param in function.parameters:
                if not hasattr(param, 'name'):
                    continue
                if not hasattr(param, 'type') or not param.type:
                    missing_types.append(param.name)
            if missing_types:
                parameter_issues['missing_types'].append({'function': func_name, 'file': file_path, 'line': function.line if hasattr(function, 'line') else None, 'parameters': missing_types})
                self.issues.add_issue(create_issue(message=f"Function '{func_name}' has parameters without type annotations: {', '.join(missing_types)}", severity=IssueSeverity.WARNING, file=file_path, line=function.line if hasattr(function, 'line') else None, category=IssueCategory.TYPE_ERROR, symbol=func_name, suggestion='Add type annotations to all parameters'))
            if hasattr(function, 'call_sites'):
                for call_site in function.call_sites:
                    if not hasattr(call_site, 'args'):
                        continue
                    required_count = 0
                    if hasattr(function, 'parameters'):
                        required_count = sum((1 for p in function.parameters if not hasattr(p, 'has_default') or not p.has_default))
                    call_file = call_site.file.file_path if hasattr(call_site, 'file') and hasattr(call_site.file, 'file_path') else 'unknown'
                    call_line = call_site.line if hasattr(call_site, 'line') else None
                    arg_count = len(call_site.args)
                    if arg_count < required_count:
                        parameter_issues['incorrect_usage'].append({'function': func_name, 'caller_file': call_file, 'caller_line': call_line, 'required_count': required_count, 'provided_count': arg_count})
                        self.issues.add_issue(create_issue(message=f"Call to '{func_name}' has too few arguments ({arg_count} provided, {required_count} required)", severity=IssueSeverity.ERROR, file=call_file, line=call_line, category=IssueCategory.PARAMETER_MISMATCH, symbol=func_name, suggestion=f"Provide all required arguments to '{func_name}'"))
        parameter_issues['summary'] = {'missing_types_count': len(parameter_issues['missing_types']), 'unused_parameters_count': len(parameter_issues['unused_parameters']), 'incorrect_usage_count': len(parameter_issues['incorrect_usage']), 'total_issues': len(parameter_issues['missing_types']) + len(parameter_issues['unused_parameters']) + len(parameter_issues['incorrect_usage'])}
        return parameter_issues

    def _check_implementations(self) -> dict[str, Any]:
        """
        Check for implementation issues.

        Returns:
            Dictionary containing implementation analysis results
        """
        logger.info('Analyzing implementations')
        implementation_issues = {'empty_functions': [], 'abstract_methods_without_implementation': [], 'summary': {'empty_functions_count': 0, 'abstract_methods_without_implementation_count': 0}}
        for function in self.codebase.functions:
            if hasattr(function, 'source'):
                source = function.source
                is_empty = False
                if not source or source.strip() == '':
                    is_empty = True
                else:
                    body_lines = source.split('\n')[1:] if '\n' in source else []
                    non_empty_lines = [line for line in body_lines if line.strip() and (not line.strip().startswith('#')) and (not (line.strip().startswith('"""') or line.strip().startswith("'''"))) and (line.strip() != 'pass')]
                    if not non_empty_lines:
                        is_empty = True
                if is_empty:
                    file_path = function.file.file_path if hasattr(function, 'file') and hasattr(function.file, 'file_path') else 'unknown'
                    func_name = function.name if hasattr(function, 'name') else str(function)
                    is_abstract = hasattr(function, 'is_abstract') and function.is_abstract or (hasattr(function, 'parent') and hasattr(function.parent, 'is_interface') and function.parent.is_interface)
                    if not is_abstract:
                        implementation_issues['empty_functions'].append({'name': func_name, 'file': file_path, 'line': function.line if hasattr(function, 'line') else None})
                        self.issues.add_issue(create_issue(message=f"Function '{func_name}' is empty", severity=IssueSeverity.WARNING, file=file_path, line=function.line if hasattr(function, 'line') else None, category=IssueCategory.MISSING_IMPLEMENTATION, symbol=func_name, suggestion='Implement this function or remove it if not needed'))
        implementation_issues['summary']['empty_functions_count'] = len(implementation_issues['empty_functions'])
        implementation_issues['summary']['abstract_methods_without_implementation_count'] = len(implementation_issues['abstract_methods_without_implementation'])
        return implementation_issues

    def _analyze_import_dependencies(self) -> dict[str, Any]:
        """
        Analyze import dependencies between files.

        Returns:
            Dictionary containing import dependency analysis results
        """
        logger.info('Analyzing import dependencies')
        dependency_graph = get_dependency_graph(self.codebase)
        dependency_counts = {file_path: len(deps) for file_path, deps in dependency_graph.items()}
        high_dependency_files = [{'file': file_path, 'dependency_count': count} for file_path, count in dependency_counts.items() if count > 10]
        high_dependency_files.sort(key=lambda x: x['dependency_count'], reverse=True)
        for item in high_dependency_files[:10]:
            self.issues.add_issue(create_issue(message=f"File has high number of dependencies: {item['dependency_count']}", severity=IssueSeverity.WARNING, file=item['file'], category=IssueCategory.MODULE_COUPLING, suggestion='Consider refactoring to reduce dependencies'))
        return {'dependency_graph': dependency_graph, 'dependency_counts': dependency_counts, 'high_dependency_files': high_dependency_files, 'summary': {'total_files': len(dependency_graph), 'files_with_high_dependencies': len(high_dependency_files), 'max_dependencies': max(dependency_counts.values()) if dependency_counts else 0, 'avg_dependencies': sum(dependency_counts.values()) / len(dependency_counts) if dependency_counts else 0}}

    def _find_circular_dependencies(self) -> dict[str, Any]:
        """
        Find circular dependencies in the codebase.

        Returns:
            Dictionary containing circular dependency analysis results
        """
        logger.info('Analyzing circular dependencies')
        dependency_graph = get_dependency_graph(self.codebase)
        circular_deps = []
        visited = set()

        def detect_cycles(node, path):
            if node in path:
                cycle_start = path.index(node)
                cycle = path[cycle_start:] + [node]
                cycle_key = '->'.join(sorted(cycle))
                if cycle_key not in visited:
                    visited.add(cycle_key)
                    circular_deps.append(cycle)
                return
            new_path = path + [node]
            for dep in dependency_graph.get(node, []):
                detect_cycles(dep, new_path)
        for node in dependency_graph:
            detect_cycles(node, [])
        for cycle in circular_deps:
            cycle_str = ' -> '.join(cycle)
            self.issues.add_issue(create_issue(message=f'Circular dependency detected: {cycle_str}', severity=IssueSeverity.ERROR, file=cycle[0], category=IssueCategory.DEPENDENCY_CYCLE, suggestion='Break the circular dependency by refactoring one of the modules'))
        return {'circular_dependencies': circular_deps, 'summary': {'circular_dependency_count': len(circular_deps)}}

    def save_results(self, output_file: str):
        """
        Save analysis results to a file.

        Args:
            output_file: Path to the output file
        """
        if not self.analysis_result:
            raise ValueError('No analysis results to save')
        self.analysis_result.save_to_file(output_file)
        logger.info(f'Results saved to {output_file}')

    def _generate_html_report(self, output_file: str | None=None):
        """
        Generate an HTML report of the analysis results.
        
        Args:
            output_file: Path to save the report to
        """
        if not self.analysis_result:
            raise ValueError('No analysis results to save')
        if not output_file:
            output_file = 'codebase_analysis_report.html'
        repo_name = self.repo_url.split('/')[-1] if self.repo_url else self.repo_path or 'Unknown'
        analysis_time = self.analysis_result.summary.analysis_time
        html = f"""\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <title>Codebase Analysis Report</title>\n            <style>\n                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n                h1, h2, h3 {{ color: #333; }}\n                .section {{ margin-bottom: 30px; }}\n                .metric {{ margin-bottom: 20px; }}\n                .metric-title {{ font-weight: bold; }}\n                pre {{ background-color: #f5f5f5; padding: 10px; border-radius: 5px; overflow-x: auto; }}\n                table {{ border-collapse: collapse; width: 100%; }}\n                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n                th {{ background-color: #f2f2f2; }}\n                tr:nth-child(even) {{ background-color: #f9f9f9; }}\n            </style>\n        </head>\n        <body>\n            <h1>Codebase Analysis Report</h1>\n            <div class="section">\n                <h2>Metadata</h2>\n                <p><strong>Repository:</strong> {repo_name}</p>\n                <p><strong>Analysis Time:</strong> {analysis_time}</p>\n                <p><strong>Language:</strong> {self.language or 'Auto-detected'}</p>\n            </div>\n        """
        html += f'\n        <div class="section">\n            <h2>Summary</h2>\n            <table>\n                <tr>\n                    <th>Metric</th>\n                    <th>Value</th>\n                </tr>\n                <tr>\n                    <td>Total Files</td>\n                    <td>{self.analysis_result.summary.total_files}</td>\n                </tr>\n                <tr>\n                    <td>Total Classes</td>\n                    <td>{self.analysis_result.summary.total_classes}</td>\n                </tr>\n                <tr>\n                    <td>Total Functions</td>\n                    <td>{self.analysis_result.summary.total_functions}</td>\n                </tr>\n                <tr>\n                    <td>Total Issues</td>\n                    <td>{self.analysis_result.summary.total_issues}</td>\n                </tr>\n                <tr>\n                    <td>Analysis Duration</td>\n                    <td>{self.analysis_result.summary.analysis_duration_ms / 1000:.2f} seconds</td>\n                </tr>\n            </table>\n        </div>\n        '
        if self.issues and self.issues.issues:
            issues_by_severity = self.issues.group_by_severity()
            html += '\n            <div class="section">\n                <h2>Issues</h2>\n                <h3>By Severity</h3>\n                <table>\n                    <tr>\n                        <th>Severity</th>\n                        <th>Count</th>\n                    </tr>\n            '
            for severity in IssueSeverity:
                count = len(issues_by_severity[severity])
                if count > 0:
                    html += f'\n                    <tr>\n                        <td>{severity.value}</td>\n                        <td>{count}</td>\n                    </tr>\n                    '
            html += '\n                </table>\n                \n                <h3>Top Issues</h3>\n                <table>\n                    <tr>\n                        <th>Severity</th>\n                        <th>Category</th>\n                        <th>Message</th>\n                        <th>Location</th>\n                    </tr>\n            '
            sorted_issues = sorted(self.issues.issues, key=lambda x: {IssueSeverity.CRITICAL: 0, IssueSeverity.ERROR: 1, IssueSeverity.WARNING: 2, IssueSeverity.INFO: 3}.get(x.severity, 4))
            for issue in sorted_issues[:20]:
                location = f'{issue.location.file}'
                if issue.location.line:
                    location += f':{issue.location.line}'
                category = issue.category.value if issue.category else ''
                html += f'\n                <tr>\n                    <td>{issue.severity.value}</td>\n                    <td>{category}</td>\n                    <td>{issue.message}</td>\n                    <td>{location}</td>\n                </tr>\n                '
            html += '\n                </table>\n            </div>\n            '
        if self.analysis_result.code_quality:
            html += '\n            <div class="section">\n                <h2>Code Quality</h2>\n            '
            if 'dead_code' in self.analysis_result.code_quality.__dict__:
                dead_code = self.analysis_result.code_quality.dead_code
                if 'summary' in dead_code:
                    html += '\n                    <h3>Dead Code</h3>\n                    <table>\n                        <tr>\n                            <th>Metric</th>\n                            <th>Count</th>\n                        </tr>\n                    '
                    for key, value in dead_code['summary'].items():
                        html += f"\n                        <tr>\n                            <td>{key.replace('_', ' ').title()}</td>\n                            <td>{value}</td>\n                        </tr>\n                        "
                    html += '\n                    </table>\n                    '
            html += '\n            </div>\n            '
        if self.analysis_result.dependencies:
            html += '\n            <div class="section">\n                <h2>Dependencies</h2>\n            '
            if 'circular_dependencies' in self.analysis_result.dependencies.__dict__:
                circular_deps = self.analysis_result.dependencies.circular_dependencies
                if 'circular_dependencies' in circular_deps:
                    html += '\n                    <h3>Circular Dependencies</h3>\n                    <table>\n                        <tr>\n                            <th>#</th>\n                            <th>Dependency Cycle</th>\n                        </tr>\n                    '
                    for i, cycle in enumerate(circular_deps['circular_dependencies'][:10], 1):
                        html += f"\n                        <tr>\n                            <td>{i}</td>\n                            <td>{' -> '.join(cycle)}</td>\n                        </tr>\n                        "
                    html += '\n                    </table>\n                    '
            html += '\n            </div>\n            '
        html += '\n        </body>\n        </html>\n        '
        with open(output_file, 'w') as f:
            f.write(html)
        logger.info(f'HTML report saved to {output_file}')

    def _print_console_report(self):
        """Print a summary report to the console."""
        if not self.analysis_result:
            raise ValueError('No analysis results to print')
        repo_name = self.repo_url.split('/')[-1] if self.repo_url else self.repo_path or 'Unknown'
        print(f"\n{'=' * 80}")
        print(f'CODEBASE ANALYSIS REPORT: {repo_name}')
        print(f"{'=' * 80}")
        print(f'Analysis Time: {self.analysis_result.summary.analysis_time}')
        print(f"Language: {self.language or 'Auto-detected'}")
        print(f'Analysis Duration: {self.analysis_result.summary.analysis_duration_ms / 1000:.2f} seconds')
        print(f"\n{'-' * 40}")
        print('SUMMARY:')
        print(f"{'-' * 40}")
        print(f'Total Files: {self.analysis_result.summary.total_files}')
        print(f'Total Classes: {self.analysis_result.summary.total_classes}')
        print(f'Total Functions: {self.analysis_result.summary.total_functions}')
        print(f'Total Issues: {self.analysis_result.summary.total_issues}')
        if self.issues and self.issues.issues:
            issues_by_severity = self.issues.group_by_severity()
            print(f"\n{'-' * 40}")
            print('ISSUES BY SEVERITY:')
            print(f"{'-' * 40}")
            for severity in IssueSeverity:
                count = len(issues_by_severity[severity])
                if count > 0:
                    print(f'{severity.value}: {count}')
            print(f"\n{'-' * 40}")
            print('TOP ISSUES:')
            print(f"{'-' * 40}")
            sorted_issues = sorted(self.issues.issues, key=lambda x: {IssueSeverity.CRITICAL: 0, IssueSeverity.ERROR: 1, IssueSeverity.WARNING: 2, IssueSeverity.INFO: 3}.get(x.severity, 4))
            for i, issue in enumerate(sorted_issues[:10], 1):
                severity_icon = {IssueSeverity.CRITICAL: 'âŒ', IssueSeverity.ERROR: 'â›”', IssueSeverity.WARNING: 'âš ï¸', IssueSeverity.INFO: 'â„¹ï¸'}.get(issue.severity, '')
                location = f'{issue.location.file}'
                if issue.location.line:
                    location += f':{issue.location.line}'
                category = f'[{issue.category.value}]' if issue.category else ''
                print(f'{i}. {severity_icon} {category} {issue.message}')
                print(f'   Location: {location}')
                if issue.suggestion:
                    print(f'   Suggestion: {issue.suggestion}')
                print()
        if self.analysis_result.code_quality:
            print(f"\n{'-' * 40}")
            print('CODE QUALITY SUMMARY:')
            print(f"{'-' * 40}")
            if 'dead_code' in self.analysis_result.code_quality.__dict__:
                dead_code = self.analysis_result.code_quality.dead_code
                if 'summary' in dead_code:
                    for key, value in dead_code['summary'].items():
                        print(f"{key.replace('_', ' ').title()}: {value}")
        if self.analysis_result.dependencies:
            print(f"\n{'-' * 40}")
            print('DEPENDENCIES SUMMARY:')
            print(f"{'-' * 40}")
            if 'circular_dependencies' in self.analysis_result.dependencies.__dict__:
                circular_deps = self.analysis_result.dependencies.circular_dependencies
                if 'circular_dependencies' in circular_deps:
                    cycles = circular_deps['circular_dependencies']
                    print(f'Circular Dependencies: {len(cycles)}')
                    if cycles:
                        print('\nExample circular dependencies:')
                        for i, cycle in enumerate(cycles[:3], 1):
                            print(f"{i}. {' -> '.join(cycle)}")

# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =
# CORE FUNCTIONS
# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =

# From backend/comprehensive_analysis.py
def main():
    """Run the comprehensive analyzer from the command line."""
    parser = argparse.ArgumentParser(description='Analyze a codebase comprehensively using the Codegen SDK')
    parser.add_argument('--repo', default='./', help='Repository URL or local path to analyze')
    args = parser.parse_args()
    analyzer = ComprehensiveAnalyzer(args.repo)
    analyzer.analyze()
    return 0

# From backend/comprehensive_analysis.py
def analyze(self) -> Dict[str, Any]:
    """
        Perform a comprehensive analysis of the codebase.
        
        Returns:
            Dictionary with analysis results
        """
    print(f'Starting comprehensive analysis of {self.repo_path_or_url}...')
    try:
        print(f'Initializing codebase from {self.repo_path_or_url}')
        if self.repo_path_or_url.startswith(('http://', 'https://')):
            parts = self.repo_path_or_url.rstrip('/').split('/')
            repo_name = f'{parts[-2]}/{parts[-1]}'
            try:
                self.codebase = Codebase.from_repo(repo_full_name=repo_name)
                print(f'Successfully initialized codebase from GitHub repository: {repo_name}')
            except Exception as e:
                print(f'Error initializing codebase from GitHub: {e}')
                self.issues.append(Issue(self.repo_path_or_url, 'Initialization Error', f'Failed to initialize codebase from GitHub: {e}', IssueSeverity.ERROR, suggestion='Check your network connection and GitHub access permissions.'))
                return {'error': f'Failed to initialize codebase: {str(e)}', 'success': False}
        else:
            try:
                self.codebase = Codebase(self.repo_path_or_url)
                print(f'Successfully initialized codebase from local path: {self.repo_path_or_url}')
            except Exception as e:
                print(f'Error initializing codebase from local path: {e}')
                self.issues.append(Issue(self.repo_path_or_url, 'Initialization Error', f'Failed to initialize codebase from local path: {e}', IssueSeverity.ERROR, suggestion='Ensure the path exists and contains valid source code.'))
                return {'error': f'Failed to initialize codebase: {str(e)}', 'success': False}
        if not hasattr(self.codebase, 'files') or not self.codebase.files:
            self.issues.append(Issue(self.repo_path_or_url, 'Empty Codebase', 'Codebase was initialized but contains no files', IssueSeverity.ERROR, suggestion='Check if the repository contains supported language files.'))
            print('Warning: Codebase contains no files')
        try:
            self._analyze_dead_code()
        except Exception as e:
            print(f'Error in dead code analysis: {e}')
            self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Dead code analysis failed: {e}', IssueSeverity.ERROR))
        try:
            self._analyze_parameter_issues()
        except Exception as e:
            print(f'Error in parameter analysis: {e}')
            self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Parameter analysis failed: {e}', IssueSeverity.ERROR))
        try:
            self._analyze_type_annotations()
        except Exception as e:
            print(f'Error in type annotation analysis: {e}')
            self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Type annotation analysis failed: {e}', IssueSeverity.ERROR))
        try:
            self._analyze_circular_dependencies()
        except Exception as e:
            print(f'Error in circular dependency analysis: {e}')
            self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Circular dependency analysis failed: {e}', IssueSeverity.ERROR))
        try:
            self._analyze_implementation_issues()
        except Exception as e:
            print(f'Error in implementation issue analysis: {e}')
            self.issues.append(Issue(self.repo_path_or_url, 'Analysis Error', f'Implementation issue analysis failed: {e}', IssueSeverity.ERROR))
        return self._generate_report()
    except Exception as e:
        print(f'Error analyzing codebase: {e}')
        import traceback
        traceback.print_exc()
        return {'error': str(e), 'success': False}

# From backend/api.py
def get_monthly_commits(repo_path: str) -> Dict[str, int]:
    """
    Get the number of commits per month for the last 12 months.

    Args:
        repo_path: Path to the git repository

    Returns:
        Dictionary with month-year as key and number of commits as value
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365)
    date_format = '%Y-%m-%d'
    since_date = start_date.strftime(date_format)
    until_date = end_date.strftime(date_format)
    repo_path = 'https://github.com/' + repo_path
    try:
        original_dir = os.getcwd()
        with tempfile.TemporaryDirectory() as temp_dir:
            subprocess.run(['git', 'clone', repo_path, temp_dir], check=True)
            os.chdir(temp_dir)
            cmd = ['git', 'log', f'--since={since_date}', f'--until={until_date}', '--format=%aI']
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            commit_dates = result.stdout.strip().split('\n')
            monthly_counts = {}
            current_date = start_date
            while current_date <= end_date:
                month_key = current_date.strftime('%Y-%m')
                monthly_counts[month_key] = 0
                current_date = (current_date.replace(day=1) + timedelta(days=32)).replace(day=1)
            for date_str in commit_dates:
                if date_str:
                    commit_date = datetime.fromisoformat(date_str.strip())
                    month_key = commit_date.strftime('%Y-%m')
                    if month_key in monthly_counts:
                        monthly_counts[month_key] += 1
            os.chdir(original_dir)
            return dict(sorted(monthly_counts.items()))
    except subprocess.CalledProcessError as e:
        print(f'Error executing git command: {e}')
        return {}
    except Exception as e:
        print(f'Error processing git commits: {e}')
        return {}
    finally:
        try:
            os.chdir(original_dir)
        except:
            pass

# From backend/api.py
def calculate_cyclomatic_complexity(function):

    def analyze_statement(statement):
        complexity = 0
        if isinstance(statement, IfBlockStatement):
            complexity += 1
            if hasattr(statement, 'elif_statements'):
                complexity += len(statement.elif_statements)
        elif isinstance(statement, (ForLoopStatement, WhileStatement)):
            complexity += 1
        elif isinstance(statement, TryCatchStatement):
            complexity += len(getattr(statement, 'except_blocks', []))
        if hasattr(statement, 'condition') and isinstance(statement.condition, str):
            complexity += statement.condition.count(' and ') + statement.condition.count(' or ')
        if hasattr(statement, 'nested_code_blocks'):
            for block in statement.nested_code_blocks:
                complexity += analyze_block(block)
        return complexity

    def analyze_block(block):
        if not block or not hasattr(block, 'statements'):
            return 0
        return sum((analyze_statement(stmt) for stmt in block.statements))
    return 1 + analyze_block(function.code_block) if hasattr(function, 'code_block') else 1

# From backend/analyzer.py
def create_issue(message: str, severity: str | IssueSeverity, file: str, line: int | None=None, category: str | IssueCategory | None=None, symbol: str | None=None, suggestion: str | None=None) -> Issue:
    """
    Create an issue with simplified parameters.

    Args:
        message: Issue message
        severity: Issue severity
        file: File path
        line: Line number
        category: Issue category
        symbol: Symbol name
        suggestion: Suggested fix

    Returns:
        Issue object
    """
    if isinstance(severity, str):
        severity = IssueSeverity(severity)
    if isinstance(category, str) and category:
        category = IssueCategory(category)
    location = CodeLocation(file=file, line=line)
    return Issue(message=message, severity=severity, location=location, category=category, symbol=symbol, suggestion=suggestion)

# From backend/analyzer.py
def get_codebase_summary(codebase: Codebase) -> str:
    """
    Generate a comprehensive summary of a codebase.
    
    Args:
        codebase: The Codebase object to summarize
        
    Returns:
        A formatted string containing a summary of the codebase's nodes and edges
    """
    node_summary = f'Contains {len(codebase.ctx.get_nodes())} nodes\n- {len(list(codebase.files))} files\n- {len(list(codebase.imports))} imports\n- {len(list(codebase.external_modules))} external_modules\n- {len(list(codebase.symbols))} symbols\n\t- {len(list(codebase.classes))} classes\n\t- {len(list(codebase.functions))} functions\n\t- {len(list(codebase.global_vars))} global_vars\n\t- {len(list(codebase.interfaces))} interfaces\n'
    edge_summary = f'Contains {len(codebase.ctx.edges)} edges\n- {len([x for x in codebase.ctx.edges if x[2].type == EdgeType.SYMBOL_USAGE])} symbol -> used symbol\n- {len([x for x in codebase.ctx.edges if x[2].type == EdgeType.IMPORT_SYMBOL_RESOLUTION])} import -> used symbol\n- {len([x for x in codebase.ctx.edges if x[2].type == EdgeType.EXPORT])} export -> exported symbol\n    '
    return f'{node_summary}\n{edge_summary}'

# From backend/analyzer.py
def analyze_codebase(repo_path: str | None=None, repo_url: str | None=None, output_file: str | None=None, analysis_types: list[str] | None=None, language: str | None=None, output_format: str='json') -> AnalysisResult:
    """
    Analyze a codebase and optionally save results to a file.
    
    Args:
        repo_path: Path to the repository to analyze
        repo_url: URL of the repository to analyze
        output_file: Optional path to save results to
        analysis_types: Optional list of analysis types to perform
        language: Optional programming language of the codebase
        output_format: Format for output (json, html, console)
        
    Returns:
        AnalysisResult containing the findings
    """
    analysis_type_enums = []
    if analysis_types:
        for at in analysis_types:
            try:
                analysis_type_enums.append(AnalysisType(at))
            except ValueError:
                logger.warning(f'Unknown analysis type: {at}')
    if repo_path:
        analyzer = CodebaseAnalyzer(repo_path=repo_path, language=language)
    elif repo_url:
        analyzer = CodebaseAnalyzer(repo_url=repo_url, language=language)
    else:
        raise ValueError('Either repo_path or repo_url must be provided')
    result = analyzer.analyze(analysis_type_enums, output_format=output_format, output_file=output_file)
    return result

# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =
# API ENDPOINTS
# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =

@fastapi_app.get("/api/codebase/stats")
async def get_codebase_stats(codebase_id: str) -> CodebaseStats:
    """Get comprehensive statistics about the codebase."""
    try:
        # Filter test functions and classes
        test_functions = [x for x in codebase.functions if x.name.startswith('test_')]
        test_classes = [x for x in codebase.classes if x.name.startswith('Test')]
        # Calculate tests per file
        tests_per_file = len(test_functions) / len(codebase.files) if codebase.files else 0
        # Find class with deepest inheritance
        deepest_class = None
        if codebase.classes:
            deepest = max(codebase.classes, key=lambda x: len(x.superclasses))
            deepest_class = {
                'name': deepest.name,
                'depth': len(deepest.superclasses),
                'chain': [s.name for s in deepest.superclasses]
            }
        # Find recursive functions
        recursive = [f.name for f in codebase.functions 
                    if any(call.name == f.name for call in f.function_calls)][:5]
        # Find most called function
        most_called = max(codebase.functions, key=lambda f: len(f.call_sites))
        most_called_info = {
            'name': most_called.name,
            'call_count': len(most_called.call_sites),
            'callers': [{'function': call.parent_function.name, 
                        'line': call.start_point[0]} 
                       for call in most_called.call_sites]
        }
        # Find function with most calls
        most_calls = max(codebase.functions, key=lambda f: len(f.function_calls))
        most_calls_info = {
            'name': most_calls.name,
            'calls_count': len(most_calls.function_calls),
            'called_functions': [call.name for call in most_calls.function_calls]
        }
        # Find unused functions
        unused = [{'name': f.name, 'filepath': f.filepath} 
                 for f in codebase.functions if len(f.call_sites) == 0]
        # Find dead code
        dead_code = find_dead_code(codebase)
        dead_code_info = [{'name': f.name, 'filepath': f.filepath} for f in dead_code]
        return CodebaseStats(
            test_functions_count=len(test_functions),
            test_classes_count=len(test_classes),
            tests_per_file=tests_per_file,
            total_classes=len(codebase.classes),
            total_functions=len(codebase.functions),
            total_imports=len(codebase.imports),
            deepest_inheritance_class=deepest_class,
            recursive_functions=recursive,
            most_called_function=most_called_info,
            function_with_most_calls=most_calls_info,
            unused_functions=unused,
            dead_code=dead_code_info
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@fastapi_app.get("/api/codebase/test-files")
async def get_test_file_stats(codebase_id: str) -> List[FileTestStats]:
    """Get statistics about test files in the codebase."""
    try:
        test_classes = [x for x in codebase.classes if x.name.startswith('Test')]
        file_test_counts = Counter([x.file for x in test_classes])
        stats = []
        for file, num_tests in file_test_counts.most_common()[:5]:
            stats.append(FileTestStats(
                filepath=file.filepath,
                test_class_count=num_tests,
                file_length=len(file.source),
                function_count=len(file.functions)
            ))
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@fastapi_app.get("/api/function/{function_id}/context")
async def get_function_context(function_id: str) -> FunctionContext:
    """Get detailed context for a specific function."""
    try:
        function = get_function_by_id(function_id)  # You'll need to implement this
        context = {
            "implementation": {
                "source": function.source,
                "filepath": function.filepath
            },
            "dependencies": [],
            "usages": []
        }
        # Add dependencies
        for dep in function.dependencies:
            if isinstance(dep, Import):
                dep = hop_through_imports(dep)  # You'll need to implement this
            context["dependencies"].append({
                "source": dep.source,
                "filepath": dep.filepath
            })
        # Add usages
        for usage in function.usages:
            context["usages"].append({
                "source": usage.usage_symbol.source,
                "filepath": usage.usage_symbol.filepath
            })
        return FunctionContext(**context)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@fastapi_app.get("/api/function/{function_id}/call-chain")
async def get_function_call_chain(function_id: str) -> List[str]:
    """Get the maximum call chain for a function."""
    try:
        function = get_function_by_id(function_id)
        chain = get_max_call_chain(function)
        return [f.name for f in chain]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@fastapi_app.post("/analyze_repo")
async def analyze_repo(request: RepoRequest) -> AnalysisResponse:
    """Single entry point for repository analysis."""
    repo_url = request.repo_url
    codebase = Codebase.from_repo(repo_url)
    # Original analysis
    num_files = len(codebase.files(extensions="*"))
    num_functions = len(codebase.functions)
    num_classes = len(codebase.classes)
    total_loc = total_lloc = total_sloc = total_comments = 0
    total_complexity = 0
    total_volume = 0
    total_mi = 0
    total_doi = 0
    monthly_commits = get_monthly_commits(repo_url)
    # Analyze files and collect symbols
    file_issues = {}
    file_symbols = {}
    for file in codebase.files:
        # Line metrics
        loc, lloc, sloc, comments = count_lines(file.source)
        total_loc += loc
        total_lloc += lloc
        total_sloc += sloc
        total_comments += comments
        # Analyze issues
        issues = analyze_file_issues(file)
        if any(len(v) > 0 for v in issues.values()):
            file_issues[file.filepath] = issues
        # Collect symbols
        symbols = []
        # Add functions as symbols
        for func in file.functions:
            issues = []
            # Check for issues
            if not any(func.name in str(usage) for usage in func.usages):
                issues.append({
                    'type': 'minor',
                    'message': f'Unused function'
                })
            if hasattr(func, 'code_block'):
                code = func.code_block.source
                if 'None' in code and not any(s in code for s in ['is None', '== None', '!= None']):
                    issues.append({
                        'type': 'critical',
                        'message': f'Potential unsafe null reference'
                    })
                if 'TODO' in code or 'FIXME' in code:
                    issues.append({
                        'type': 'major',
                        'message': f'Incomplete implementation'
                    })
            symbols.append(Symbol(
                id=str(hash(func.name + file.filepath)),
                name=func.name,
                type='function',
                filepath=file.filepath,
                start_line=func.start_point[0] if hasattr(func, 'start_point') else 0,
                end_line=func.end_point[0] if hasattr(func, 'end_point') else 0,
                issues=issues if issues else None
            ))
        # Add classes as symbols
        for cls in file.classes:
            symbols.append(Symbol(
                id=str(hash(cls.name + file.filepath)),
                name=cls.name,
                type='class',
                filepath=file.filepath,
                start_line=cls.start_point[0] if hasattr(cls, 'start_point') else 0,
                end_line=cls.end_point[0] if hasattr(cls, 'end_point') else 0
            ))
        if symbols:
            file_symbols[file.filepath] = symbols
    # Build repository structure with symbols
    repo_structure = build_repo_structure(codebase.files, file_issues, file_symbols)
    # Calculate metrics
    callables = codebase.functions + [m for c in codebase.classes for m in c.methods]
    num_callables = 0
    for func in callables:
        if not hasattr(func, "code_block"):
            continue
        complexity = calculate_cyclomatic_complexity(func)
        operators, operands = get_operators_and_operands(func)
        volume, N1, N2, n1, n2 = calculate_halstead_volume(operators, operands)
        loc = len(func.code_block.source.splitlines())
        mi_score = calculate_maintainability_index(volume, complexity, loc)
        total_complexity += complexity
        total_volume += volume
        total_mi += mi_score
        num_callables += 1
    for cls in codebase.classes:
        doi = calculate_doi(cls)
        total_doi += doi
    desc = get_github_repo_description(repo_url)
    return AnalysisResponse(
        repo_url=repo_url,
        description=desc,
        num_files=num_files,
        num_functions=num_functions,
        num_classes=num_classes,
        line_metrics={
            "total": {
                "loc": total_loc,
                "lloc": total_lloc,
                "sloc": total_sloc,
                "comments": total_comments,
                "comment_density": (total_comments / total_loc * 100)
                if total_loc > 0
                else 0,
            },
        },
        cyclomatic_complexity={
            "average": total_complexity / num_callables if num_callables > 0 else 0,
        },
        depth_of_inheritance={
            "average": total_doi / len(codebase.classes) if codebase.classes else 0,
        },
        halstead_metrics={
            "total_volume": int(total_volume),
            "average_volume": int(total_volume / num_callables)
            if num_callables > 0
            else 0,
        },
        maintainability_index={
            "average": int(total_mi / num_callables) if num_callables > 0 else 0,
        },
        monthly_commits=monthly_commits,
        repo_structure=repo_structure
    )
@fastapi_app.get("/function/{function_id}/call-chain")
async def get_function_call_chain(function_id: str) -> List[str]:
    """Get the maximum call chain for a function."""
    try:
        function = get_function_by_id(function_id)
        chain = get_max_call_chain(function)
        return [f.name for f in chain]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@fastapi_app.get("/function/{function_id}/context")
async def get_function_context(function_id: str) -> FunctionContext:
    """Get detailed context for a specific function."""
    try:
        function = get_function_by_id(function_id)
        context = {
            "implementation": {
                "source": function.source,
                "filepath": function.filepath
            },
            "dependencies": [],
            "usages": []
        }
        # Add dependencies
        for dep in function.dependencies:
            if isinstance(dep, Import):
                dep = hop_through_imports(dep)
            context["dependencies"].append({
                "source": dep.source,
                "filepath": dep.filepath
            })
        # Add usages
        for usage in function.usages:
            context["usages"].append({
                "source": usage.usage_symbol.source,
                "filepath": usage.usage_symbol.filepath
            })
        return FunctionContext(**context)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@fastapi_app.get("/symbol/{symbol_id}/context")
async def get_symbol_context(symbol_id: str) -> Dict[str, Any]:
    """Get detailed context for any symbol."""
    try:
        symbol = get_symbol_by_id(symbol_id)  # You'll need to implement this
        return get_detailed_symbol_context(symbol)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
# Helper function to get a function by ID (you'll need to implement this)
def get_function_by_id(function_id: str):
    # Implementation depends on how you store/retrieve functions
    pass


@app.function(image=image)
@modal.asgi_app()
def fastapi_modal_app():
    return fastapi_app


# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =
# MODAL DEPLOYMENT
# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =

@app.function(image=image)
@modal.asgi_app()
def fastapi_modal_app():
    return fastapi_app

# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =
# MAIN EXECUTION
# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =# =

if __name__ == "__main__":
    import argparse
    import socket
    
    parser = argparse.ArgumentParser(
        description="Unified Codebase Analytics API"
    )
    parser.add_argument(
        "--mode", 
        choices=["api", "cli", "analyze"],
        default="api",
        help="Run mode: api (web server), cli (command line), analyze (direct analysis)"
    )
    parser.add_argument(
        "--repo",
        help="Repository URL or path for analysis"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="Port for API server"
    )
    args = parser.parse_args()
    
    if args.mode == "api":
        def find_available_port(start_port=8000, max_port=8100):
            """Find an available port starting from start_port"""
            for port in range(start_port, max_port):
                try:
                    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                        s.bind(("0.0.0.0", port))
                        return port
                except OSError:
                    continue
            raise RuntimeError(f"No available ports found between {start_port} and {max_port}")
        
        port = find_available_port(args.port)
        print(f"ðŸš€ Starting Unified Codebase Analytics API on http://localhost:{port}")
        print(f"ðŸ“š API documentation available at http://localhost:{port}/docs")
        print(f"ðŸ” Interactive API explorer at http://localhost:{port}/redoc")
        
        uvicorn.run(fastapi_app, host="0.0.0.0", port=port)
        
    elif args.mode == "cli":
        if not args.repo:
            print("âŒ --repo is required for CLI mode")
            sys.exit(1)
        
        print(f"ðŸ” Analyzing repository: {args.repo}")
        # Add CLI analysis logic here
        try:
            result = analyze_codebase(repo_url=args.repo)
            print("âœ… Analysis complete!")
            print(f"ðŸ“Š Results: {result}")
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            sys.exit(1)
        
    elif args.mode == "analyze":
        if not args.repo:
            print("âŒ --repo is required for analyze mode")
            sys.exit(1)
        
        print(f"ðŸ”¬ Direct analysis of: {args.repo}")
        # Add direct analysis logic here
        try:
            # Use the comprehensive analyzer
            analyzer = ComprehensiveAnalyzer(args.repo)
            result = analyzer.analyze()
            print("âœ… Comprehensive analysis complete!")
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            sys.exit(1)