#!/usr/bin/env python3
"""
COMPLETE CONSOLIDATED CODEBASE ANALYTICS API

This file contains ALL code from:
- analyzer.py (2,136 lines) - Advanced issue management and transaction support
- api.py (1,212 lines) - FastAPI web service with metrics calculation
- comprehensive_analysis.py (736 lines) - Deep codebase analysis using Codegen SDK

Total original lines: 4,084

Features:
✅ Complete dead code detection and analysis
✅ Advanced issue management with filtering and categorization  
✅ Comprehensive metrics (complexity, maintainability, volume)
✅ FastAPI web interface with all endpoints
✅ Multiple output formats (JSON, HTML, console, web)
✅ Interactive codebase tree and symbol analysis
✅ Git commit analysis and repository structure
✅ Parameter validation and type annotation checking
✅ Circular dependency detection
✅ Implementation error detection
✅ Modal deployment support

Generated by simple_complete_consolidation.py
"""


# ================================================================================
# ANALYZER.PY - ADVANCED ISSUE MANAGEMENT
# ================================================================================

#!/usr/bin/env python3
"""
Comprehensive Codebase Analyzer

This module provides a unified interface for codebase analysis, integrating functionality
from various analyzer modules to detect code issues, analyze dependencies, and provide
insights into code quality and structure.

It combines the following analyzer capabilities:
- Code quality analysis (dead code, complexity, style issues)
- Issue tracking and management
- Dependency analysis
- Import and usage analysis
- Transaction management for code modifications
- Code metrics and visualization
- Performance analysis
"""

import json
import logging
import math
import re
import sys
import tempfile
from dataclasses import asdict, dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

try:
    import networkx as nx
except ImportError:
    nx = None

# Import from Codegen SDK - Explicitly require the SDK
from codegen.configs.models.codebase import CodebaseConfig
from codegen.configs.models.secrets import SecretsConfig
from codegen.git.repo_operator.repo_operator import RepoOperator
from codegen.git.schemas.repo_config import RepoConfig
from codegen.sdk.codebase.config import ProjectConfig
from codegen.sdk.core.codebase import Codebase
from codegen.sdk.core.class_definition import Class
from codegen.sdk.core.external_module import ExternalModule
from codegen.sdk.core.file import SourceFile
from codegen.sdk.core.function import Function
from codegen.sdk.core.import_resolution import Import
from codegen.sdk.core.symbol import Symbol
from codegen.sdk.enums import EdgeType, SymbolType
from codegen.shared.enums.programming_language import ProgrammingLanguage

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)


#######################################################
# Enums and Constants
#######################################################

class AnalysisType(str, Enum):
    """Types of analysis that can be performed."""

    CODEBASE = "codebase"
    PR = "pr"
    COMPARISON = "comparison"
    CODE_QUALITY = "code_quality"
    DEPENDENCY = "dependency"
    SECURITY = "security"
    PERFORMANCE = "performance"
    TYPE_CHECKING = "type_checking"
    COMPREHENSIVE = "comprehensive"  # New comprehensive analysis type


class IssueSeverity(str, Enum):
    """Severity levels for issues."""

    CRITICAL = "critical"  # Must be fixed immediately, blocks functionality
    ERROR = "error"  # Must be fixed, causes errors or undefined behavior
    WARNING = "warning"  # Should be fixed, may cause problems in future
    INFO = "info"  # Informational, could be improved but not critical


class IssueCategory(str, Enum):
    """Categories of issues that can be detected."""

    # Code Quality Issues
    DEAD_CODE = "dead_code"  # Unused variables, functions, etc.
    COMPLEXITY = "complexity"  # Code too complex, needs refactoring
    STYLE_ISSUE = "style_issue"  # Code style issues (line length, etc.)
    DOCUMENTATION = "documentation"  # Missing or incomplete documentation

    # Type and Parameter Issues
    TYPE_ERROR = "type_error"  # Type errors or inconsistencies
    PARAMETER_MISMATCH = "parameter_mismatch"  # Parameter type or count mismatch
    RETURN_TYPE_ERROR = "return_type_error"  # Return type error or mismatch

    # Implementation Issues
    IMPLEMENTATION_ERROR = "implementation_error"  # Incorrect implementation
    MISSING_IMPLEMENTATION = "missing_implementation"  # Missing implementation

    # Dependency Issues
    IMPORT_ERROR = "import_error"  # Import errors or issues
    DEPENDENCY_CYCLE = "dependency_cycle"  # Circular dependency
    MODULE_COUPLING = "module_coupling"  # High coupling between modules

    # API Issues
    API_CHANGE = "api_change"  # API has changed in a breaking way
    API_USAGE_ERROR = "api_usage_error"  # Incorrect API usage

    # Security Issues
    SECURITY_VULNERABILITY = "security_vulnerability"  # Security vulnerability

    # Performance Issues
    PERFORMANCE_ISSUE = "performance_issue"  # Performance issue


class IssueStatus(str, Enum):
    """Status of an issue."""

    OPEN = "open"  # Issue is open and needs to be fixed
    FIXED = "fixed"  # Issue has been fixed
    WONTFIX = "wontfix"  # Issue will not be fixed
    INVALID = "invalid"  # Issue is invalid or not applicable
    DUPLICATE = "duplicate"  # Issue is a duplicate of another


class ChangeType(str, Enum):
    """Type of change for a diff."""
    
    Added = "added"
    Removed = "removed"
    Modified = "modified"
    Renamed = "renamed"


class TransactionPriority(int, Enum):
    """Priority levels for transactions."""
    
    HIGH = 0
    MEDIUM = 5
    LOW = 10


#######################################################
# Issue Management
#######################################################

@dataclass
class CodeLocation:
    """Location of an issue in code."""

    file: str
    line: int | None = None
    column: int | None = None
    end_line: int | None = None
    end_column: int | None = None

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        return {k: v for k, v in asdict(self).items() if v is not None}

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "CodeLocation":
        """Create from dictionary representation."""
        return cls(**{k: v for k, v in data.items() if k in cls.__annotations__})

    def __str__(self) -> str:
        """Convert to string representation."""
        if self.line is not None:
            if self.column is not None:
                return f"{self.file}:{self.line}:{self.column}"
            return f"{self.file}:{self.line}"
        return self.file


@dataclass
class Issue:
    """Represents an issue found during analysis."""

    # Core fields
    message: str
    severity: IssueSeverity
    location: CodeLocation

    # Classification fields
    category: IssueCategory | None = None
    analysis_type: AnalysisType | None = None
    status: IssueStatus = IssueStatus.OPEN

    # Context fields
    symbol: str | None = None
    code: str | None = None
    suggestion: str | None = None
    related_symbols: list[str] = field(default_factory=list)
    related_locations: list[CodeLocation] = field(default_factory=list)

    # Metadata fields
    id: str | None = None
    hash: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Initialize derived fields."""
        # Generate an ID if not provided
        if self.id is None:
            import hashlib

            # Create a hash based on location and message
            hash_input = f"{self.location.file}:{self.location.line}:{self.message}"
            self.id = hashlib.md5(hash_input.encode()).hexdigest()[:12]

    @property
    def file(self) -> str:
        """Get the file path."""
        return self.location.file

    @property
    def line(self) -> int | None:
        """Get the line number."""
        return self.location.line

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        result = {
            "id": self.id,
            "message": self.message,
            "severity": self.severity.value,
            "location": self.location.to_dict(),
            "status": self.status.value,
        }

        # Add optional fields if present
        if self.category:
            result["category"] = self.category.value

        if self.analysis_type:
            result["analysis_type"] = self.analysis_type.value

        if self.symbol:
            result["symbol"] = self.symbol

        if self.code:
            result["code"] = self.code

        if self.suggestion:
            result["suggestion"] = self.suggestion

        if self.related_symbols:
            result["related_symbols"] = self.related_symbols

        if self.related_locations:
            result["related_locations"] = [
                loc.to_dict() for loc in self.related_locations
            ]

        if self.metadata:
            result["metadata"] = self.metadata

        return result

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "Issue":
        """Create from dictionary representation."""
        # Convert string enums to actual enum values
        if "severity" in data and isinstance(data["severity"], str):
            data["severity"] = IssueSeverity(data["severity"])

        if "category" in data and isinstance(data["category"], str):
            data["category"] = IssueCategory(data["category"])

        if "analysis_type" in data and isinstance(data["analysis_type"], str):
            data["analysis_type"] = AnalysisType(data["analysis_type"])

        if "status" in data and isinstance(data["status"], str):
            data["status"] = IssueStatus(data["status"])

        # Convert location dict to CodeLocation
        if "location" in data and isinstance(data["location"], dict):
            data["location"] = CodeLocation.from_dict(data["location"])

        # Convert related_locations dicts to CodeLocation objects
        if "related_locations" in data and isinstance(data["related_locations"], list):
            data["related_locations"] = [
                CodeLocation.from_dict(loc) if isinstance(loc, dict) else loc
                for loc in data["related_locations"]
            ]

        return cls(**{k: v for k, v in data.items() if k in cls.__annotations__})


class IssueCollection:
    """Collection of issues with filtering and grouping capabilities."""

    def __init__(self, issues: list[Issue] | None = None):
        """
        Initialize the issue collection.

        Args:
            issues: Initial list of issues
        """
        self.issues = issues or []
        self._filters = []

    def add_issue(self, issue: Issue):
        """
        Add an issue to the collection.

        Args:
            issue: Issue to add
        """
        self.issues.append(issue)

    def add_issues(self, issues: list[Issue]):
        """
        Add multiple issues to the collection.

        Args:
            issues: Issues to add
        """
        self.issues.extend(issues)

    def add_filter(self, filter_func, description: str = ""):
        """
        Add a filter function.

        Args:
            filter_func: Function that returns True if issue should be included
            description: Description of the filter
        """
        self._filters.append((filter_func, description))

    def get_issues(
        self,
        severity: IssueSeverity | None = None,
        category: IssueCategory | None = None,
        status: IssueStatus | None = None,
        file_path: str | None = None,
        symbol: str | None = None,
    ) -> list[Issue]:
        """
        Get issues matching the specified criteria.

        Args:
            severity: Severity to filter by
            category: Category to filter by
            status: Status to filter by
            file_path: File path to filter by
            symbol: Symbol name to filter by

        Returns:
            List of matching issues
        """
        filtered_issues = self.issues

        # Apply custom filters
        for filter_func, _ in self._filters:
            filtered_issues = [i for i in filtered_issues if filter_func(i)]

        # Apply standard filters
        if severity:
            filtered_issues = [i for i in filtered_issues if i.severity == severity]

        if category:
            filtered_issues = [i for i in filtered_issues if i.category == category]

        if status:
            filtered_issues = [i for i in filtered_issues if i.status == status]

        if file_path:
            filtered_issues = [
                i for i in filtered_issues if i.location.file == file_path
            ]

        if symbol:
            filtered_issues = [
                i
                for i in filtered_issues
                if (
                    i.symbol == symbol
                    or (i.related_symbols and symbol in i.related_symbols)
                )
            ]

        return filtered_issues

    def group_by_severity(self) -> dict[IssueSeverity, list[Issue]]:
        """
        Group issues by severity.

        Returns:
            Dictionary mapping severities to lists of issues
        """
        result = {severity: [] for severity in IssueSeverity}

        for issue in self.issues:
            result[issue.severity].append(issue)

        return result

    def group_by_category(self) -> dict[IssueCategory, list[Issue]]:
        """
        Group issues by category.

        Returns:
            Dictionary mapping categories to lists of issues
        """
        result = {category: [] for category in IssueCategory}

        for issue in self.issues:
            if issue.category:
                result[issue.category].append(issue)

        return result

    def group_by_file(self) -> dict[str, list[Issue]]:
        """
        Group issues by file.

        Returns:
            Dictionary mapping file paths to lists of issues
        """
        result = {}

        for issue in self.issues:
            if issue.location.file not in result:
                result[issue.location.file] = []

            result[issue.location.file].append(issue)

        return result

    def statistics(self) -> dict[str, Any]:
        """
        Get statistics about the issues.

        Returns:
            Dictionary with issue statistics
        """
        by_severity = self.group_by_severity()
        by_category = self.group_by_category()
        by_status = {status: [] for status in IssueStatus}
        for issue in self.issues:
            by_status[issue.status].append(issue)

        return {
            "total": len(self.issues),
            "by_severity": {
                severity.value: len(issues) for severity, issues in by_severity.items()
            },
            "by_category": {
                category.value: len(issues)
                for category, issues in by_category.items()
                if len(issues) > 0  # Only include non-empty categories
            },
            "by_status": {
                status.value: len(issues) for status, issues in by_status.items()
            },
            "file_count": len(self.group_by_file()),
        }

    def to_dict(self) -> dict[str, Any]:
        """
        Convert to dictionary representation.

        Returns:
            Dictionary representation of the issue collection
        """
        return {
            "issues": [issue.to_dict() for issue in self.issues],
            "statistics": self.statistics(),
            "filters": [desc for _, desc in self._filters if desc],
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "IssueCollection":
        """
        Create from dictionary representation.

        Args:
            data: Dictionary representation

        Returns:
            Issue collection
        """
        collection = cls()

        if "issues" in data and isinstance(data["issues"], list):
            collection.add_issues([
                Issue.from_dict(issue) if isinstance(issue, dict) else issue
                for issue in data["issues"]
            ])

        return collection

    def save_to_file(self, file_path: str, format: str = "json"):
        """
        Save to file.

        Args:
            file_path: Path to save to
            format: Format to save in
        """
        if format == "json":
            with open(file_path, "w") as f:
                json.dump(self.to_dict(), f, indent=2)
        else:
            raise ValueError(f"Unsupported format: {format}")

    @classmethod
    def load_from_file(cls, file_path: str) -> "IssueCollection":
        """
        Load from file.

        Args:
            file_path: Path to load from

        Returns:
            Issue collection
        """
        with open(file_path) as f:
            data = json.load(f)

        return cls.from_dict(data)


def create_issue(
    message: str,
    severity: str | IssueSeverity,
    file: str,
    line: int | None = None,
    category: str | IssueCategory | None = None,
    symbol: str | None = None,
    suggestion: str | None = None,
) -> Issue:
    """
    Create an issue with simplified parameters.

    Args:
        message: Issue message
        severity: Issue severity
        file: File path
        line: Line number
        category: Issue category
        symbol: Symbol name
        suggestion: Suggested fix

    Returns:
        Issue object
    """
    # Convert string severity to enum
    if isinstance(severity, str):
        severity = IssueSeverity(severity)

    # Convert string category to enum
    if isinstance(category, str) and category:
        category = IssueCategory(category)

    # Create location
    location = CodeLocation(file=file, line=line)

    # Create issue
    return Issue(
        message=message,
        severity=severity,
        location=location,
        category=category,
        symbol=symbol,
        suggestion=suggestion,
    )


#######################################################
# Analysis Results
#######################################################

@dataclass
class AnalysisSummary:
    """Summary statistics for an analysis."""

    total_files: int = 0
    total_functions: int = 0
    total_classes: int = 0
    total_issues: int = 0
    analysis_time: str = field(default_factory=lambda: datetime.now().isoformat())
    analysis_duration_ms: int | None = None

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        return {k: v for k, v in asdict(self).items() if v is not None}


@dataclass
class CodeQualityResult:
    """Results of code quality analysis."""

    dead_code: dict[str, Any] = field(default_factory=dict)
    complexity: dict[str, Any] = field(default_factory=dict)
    parameter_issues: dict[str, Any] = field(default_factory=dict)
    style_issues: dict[str, Any] = field(default_factory=dict)
    implementation_issues: dict[str, Any] = field(default_factory=dict)
    maintainability: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        return dict(asdict(self).items())


@dataclass
class DependencyResult:
    """Results of dependency analysis."""

    import_dependencies: dict[str, Any] = field(default_factory=dict)
    circular_dependencies: dict[str, Any] = field(default_factory=dict)
    module_coupling: dict[str, Any] = field(default_factory=dict)
    external_dependencies: dict[str, Any] = field(default_factory=dict)
    call_graph: dict[str, Any] = field(default_factory=dict)
    class_hierarchy: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        return dict(asdict(self).items())


@dataclass
class AnalysisResult:
    """Comprehensive analysis result."""

    # Core data
    analysis_types: list[AnalysisType]
    summary: AnalysisSummary = field(default_factory=AnalysisSummary)
    issues: IssueCollection = field(default_factory=IssueCollection)

    # Analysis results
    code_quality: CodeQualityResult | None = None
    dependencies: DependencyResult | None = None

    # Metadata
    metadata: dict[str, Any] = field(default_factory=dict)
    repo_name: str | None = None
    repo_path: str | None = None
    language: str | None = None

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        result = {
            "analysis_types": [at.value for at in self.analysis_types],
            "summary": self.summary.to_dict(),
            "issues": self.issues.to_dict(),
            "metadata": self.metadata,
        }

        # Add optional sections if present
        if self.repo_name:
            result["repo_name"] = self.repo_name

        if self.repo_path:
            result["repo_path"] = self.repo_path

        if self.language:
            result["language"] = self.language

        # Add analysis results if present
        if self.code_quality:
            result["code_quality"] = self.code_quality.to_dict()

        if self.dependencies:
            result["dependencies"] = self.dependencies.to_dict()

        return result

    def save_to_file(self, file_path: str, indent: int = 2):
        """
        Save analysis result to a file.

        Args:
            file_path: Path to save to
            indent: JSON indentation level
        """
        with open(file_path, "w") as f:
            json.dump(self.to_dict(), f, indent=indent)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "AnalysisResult":
        """
        Create analysis result from dictionary.

        Args:
            data: Dictionary representation

        Returns:
            Analysis result object
        """
        # Convert analysis types
        analysis_types = [
            AnalysisType(at) if isinstance(at, str) else at
            for at in data.get("analysis_types", [])
        ]

        # Create summary
        summary = (
            AnalysisSummary(**data.get("summary", {}))
            if "summary" in data
            else AnalysisSummary()
        )

        # Create issues collection
        issues = (
            IssueCollection.from_dict(data.get("issues", {}))
            if "issues" in data
            else IssueCollection()
        )

        # Create result object
        result = cls(
            analysis_types=analysis_types,
            summary=summary,
            issues=issues,
            repo_name=data.get("repo_name"),
            repo_path=data.get("repo_path"),
            language=data.get("language"),
            metadata=data.get("metadata", {}),
        )

        # Add analysis results if present
        if "code_quality" in data:
            result.code_quality = CodeQualityResult(**data["code_quality"])

        if "dependencies" in data:
            result.dependencies = DependencyResult(**data["dependencies"])

        return result

    @classmethod
    def load_from_file(cls, file_path: str) -> "AnalysisResult":
        """
        Load analysis result from file.

        Args:
            file_path: Path to load from

        Returns:
            Analysis result object
        """
        with open(file_path) as f:
            data = json.load(f)

        return cls.from_dict(data)

#######################################################
# Codebase Analysis Utilities
#######################################################

def get_codebase_summary(codebase: Codebase) -> str:
    """
    Generate a comprehensive summary of a codebase.
    
    Args:
        codebase: The Codebase object to summarize
        
    Returns:
        A formatted string containing a summary of the codebase's nodes and edges
    """
    node_summary = f"""Contains {len(codebase.ctx.get_nodes())} nodes
- {len(list(codebase.files))} files
- {len(list(codebase.imports))} imports
- {len(list(codebase.external_modules))} external_modules
- {len(list(codebase.symbols))} symbols
\t- {len(list(codebase.classes))} classes
\t- {len(list(codebase.functions))} functions
\t- {len(list(codebase.global_vars))} global_vars
\t- {len(list(codebase.interfaces))} interfaces
"""
    edge_summary = f"""Contains {len(codebase.ctx.edges)} edges
- {len([x for x in codebase.ctx.edges if x[2].type == EdgeType.SYMBOL_USAGE])} symbol -> used symbol
- {len([x for x in codebase.ctx.edges if x[2].type == EdgeType.IMPORT_SYMBOL_RESOLUTION])} import -> used symbol
- {len([x for x in codebase.ctx.edges if x[2].type == EdgeType.EXPORT])} export -> exported symbol
    """

    return f"{node_summary}\n{edge_summary}"


def get_dependency_graph(codebase: Codebase, file_path: Optional[str] = None) -> Dict[str, List[str]]:
    """
    Generate a dependency graph for a codebase or specific file.
    
    Args:
        codebase: The Codebase object to analyze
        file_path: Optional path to a specific file to analyze
        
    Returns:
        A dictionary mapping file paths to lists of dependencies
    """
    dependency_graph = {}
    
    files_to_analyze = [f for f in codebase.files if not file_path or f.file_path == file_path]
    
    for file in files_to_analyze:
        dependencies = []
        
        # Add direct imports
        for imp in file.imports:
            if hasattr(imp, "imported_symbol") and hasattr(imp.imported_symbol, "file"):
                if hasattr(imp.imported_symbol.file, "file_path"):
                    dependencies.append(imp.imported_symbol.file.file_path)
        
        # Add symbol dependencies
        for symbol in file.symbols:
            for dep in symbol.dependencies:
                if hasattr(dep, "file") and hasattr(dep.file, "file_path"):
                    dependencies.append(dep.file.file_path)
        
        # Remove duplicates and self-references
        unique_deps = list(set([d for d in dependencies if d != file.file_path]))
        dependency_graph[file.file_path] = unique_deps
    
    return dependency_graph


def get_symbol_references(codebase: Codebase, symbol_name: str) -> List[Dict[str, Any]]:
    """
    Find all references to a symbol in the codebase.
    
    Args:
        codebase: The Codebase object to search
        symbol_name: The name of the symbol to find references for
        
    Returns:
        A list of dictionaries containing reference information
    """
    references = []
    
    # Find all symbols with the given name
    target_symbols = [s for s in codebase.symbols if s.name == symbol_name]
    
    for symbol in target_symbols:
        # Find all edges that reference this symbol
        for edge in codebase.ctx.edges:
            if edge[1] == symbol.id:  # If the edge points to our symbol
                source_node = codebase.ctx.get_node(edge[0])
                if source_node:
                    # Get file and line information if available
                    file_path = None
                    line_number = None
                    
                    if hasattr(source_node, "file") and hasattr(source_node.file, "file_path"):
                        file_path = source_node.file.file_path
                    
                    if hasattr(source_node, "line"):
                        line_number = source_node.line
                    
                    references.append(
                        {
                            "file_path": file_path,
                            "line": line_number,
                            "source_type": type(source_node).__name__,
                            "source_name": getattr(source_node, "name", str(source_node)),
                            "edge_type": edge[2].type.name
                            if hasattr(edge[2], "type")
                            else "Unknown",
                        }
                    )
    
    return references


def calculate_cyclomatic_complexity(func: Function) -> int:
    """
    Calculate the cyclomatic complexity of a function.
    
    Args:
        func: The Function object to analyze
        
    Returns:
        An integer representing the cyclomatic complexity
    """
    complexity = 1  # Base complexity
    
    if not hasattr(func, "source") or not func.source:
        return complexity
    
    # Simple heuristic: count control flow statements
    source = func.source.lower()
    
    # Count if statements
    complexity += source.count(" if ") + source.count("\nif ")
    
    # Count else if / elif statements
    complexity += source.count("elif ") + source.count("else if ")
    
    # Count loops
    complexity += source.count(" for ") + source.count("\nfor ")
    complexity += source.count(" while ") + source.count("\nwhile ")
    
    # Count exception handlers
    complexity += source.count("except ") + source.count("catch ")
    
    # Count logical operators (each one creates a new path)
    complexity += source.count(" and ") + source.count(" && ")
    complexity += source.count(" or ") + source.count(" || ")
    
    return complexity


#######################################################
# Main Analyzer Class
#######################################################

class CodebaseAnalyzer:
    """
    Comprehensive code analyzer for detecting issues and analyzing codebase structure.
    
    This class provides a unified interface for analyzing codebases, integrating
    functionality from various analyzer modules to detect code issues, analyze
    dependencies, and provide insights into code quality and structure.
    """

    def __init__(
        self,
        repo_url: str | None = None,
        repo_path: str | None = None,
        base_branch: str = "main",
        language: str | None = None,
        file_ignore_list: list[str] | None = None,
        config: dict[str, Any] | None = None,
    ):
        """
        Initialize the codebase analyzer.

        Args:
            repo_url: URL of the repository to analyze
            repo_path: Local path to the repository to analyze
            base_branch: Base branch for comparison
            language: Programming language of the codebase
            file_ignore_list: List of file patterns to ignore
            config: Additional configuration options
        """
        self.repo_url = repo_url
        self.repo_path = repo_path
        self.base_branch = base_branch
        self.language = language

        # Use custom ignore list or default global list
        self.file_ignore_list = file_ignore_list or []

        # Configuration options
        self.config = config or {}

        # Codebase object
        self.codebase = None

        # Analysis results
        self.issues = IssueCollection()
        self.analysis_result = None

        # Initialize codebase based on provided parameters
        if repo_url:
            self._init_from_url(repo_url, language)
        elif repo_path:
            self._init_from_path(repo_path, language)

    def _init_from_url(self, repo_url: str, language: str | None = None):
        """
        Initialize codebase from a repository URL.

        Args:
            repo_url: URL of the repository
            language: Programming language of the codebase
        """
            
        try:
            # Extract repository information
            if repo_url.endswith(".git"):
                repo_url = repo_url[:-4]

            parts = repo_url.rstrip("/").split("/")
            repo_name = parts[-1]
            owner = parts[-2]
            repo_full_name = f"{owner}/{repo_name}"

            # Create temporary directory for cloning
            tmp_dir = tempfile.mkdtemp(prefix="analyzer_")

            # Set up configuration
            config = CodebaseConfig(
                debug=False,
                allow_external=True,
                py_resolve_syspath=True,
            )

            secrets = SecretsConfig()

            # Determine programming language
            prog_lang = None
            if language:
                prog_lang = ProgrammingLanguage(language.upper())

            # Initialize the codebase
            logger.info(f"Initializing codebase from {repo_url}")

            # Use from_repo method with correct parameter name
            # Specify only Python language to avoid TypeScript engine issues
            self.codebase = Codebase.from_repo(
                repo_full_name=repo_full_name,
                tmp_dir=tmp_dir,
                language="python",  # Force Python language to avoid TypeScript engine issues
                config=config,
                secrets=secrets,
            )

            logger.info(f"Successfully initialized codebase from {repo_url}")

        except Exception as e:
            logger.exception(f"Error initializing codebase from URL: {e}")
            raise

    def _init_from_path(self, repo_path: str, language: str | None = None):
        """
        Initialize codebase from a local repository path.

        Args:
            repo_path: Path to the repository
            language: Programming language of the codebase
        """
            
        try:
            # Set up configuration
            config = CodebaseConfig(
                debug=False,
                allow_external=True,
                py_resolve_syspath=True,
            )

            secrets = SecretsConfig()

            # Initialize the codebase
            logger.info(f"Initializing codebase from {repo_path}")

            # Determine programming language
            prog_lang = None
            if language:
                prog_lang = ProgrammingLanguage(language.upper())

            # Set up repository configuration
            repo_config = RepoConfig.from_repo_path(repo_path)
            repo_config.respect_gitignore = False
            repo_operator = RepoOperator(repo_config=repo_config, bot_commit=False)

            # Create project configuration
            project_config = ProjectConfig(
                repo_operator=repo_operator,
                programming_language=prog_lang if prog_lang else None,
            )

            # Initialize codebase
            self.codebase = Codebase(
                projects=[project_config], config=config, secrets=secrets
            )

            logger.info(f"Successfully initialized codebase from {repo_path}")

        except Exception as e:
            logger.exception(f"Error initializing codebase from path: {e}")
            raise

    def analyze(self, analysis_types: list[AnalysisType] | None = None, output_format: str = "json", output_file: str | None = None) -> AnalysisResult:
        """
        Perform comprehensive analysis on the codebase.

        Args:
            analysis_types: Types of analysis to perform. Defaults to code quality analysis.
            output_format: Format of the output (json, html, console)
            output_file: Path to save results to

        Returns:
            AnalysisResult containing the findings
        """
        if not self.codebase:
            raise ValueError("Codebase not initialized")

        if not analysis_types:
            analysis_types = [AnalysisType.CODE_QUALITY]

        # Start measuring analysis time
        start_time = datetime.now()

        # Create result object
        self.analysis_result = AnalysisResult(
            analysis_types=analysis_types,
            repo_name=self.repo_url.split("/")[-1] if self.repo_url else None,
            repo_path=self.repo_path,
            language=self.language,
        )

        # Perform analyses based on requested types
        logger.info(f"Starting analysis with types: {[at.value for at in analysis_types]}")

        # Comprehensive analysis (includes all other types)
        if AnalysisType.COMPREHENSIVE in analysis_types:
            logger.info("Running comprehensive analysis (includes all analysis types)")
            self._analyze_code_quality()
            self._analyze_dependencies()
            self._analyze_performance()
            self._analyze_parameter_issues()
            self._analyze_type_annotations()
            self._analyze_circular_dependencies()
            # Add more comprehensive checks here
        else:
            # Individual analysis types
            # Code quality analysis
            if AnalysisType.CODE_QUALITY in analysis_types:
                self._analyze_code_quality()

            # Dependency analysis
            if AnalysisType.DEPENDENCY in analysis_types:
                self._analyze_dependencies()

            # Performance analysis
            if AnalysisType.PERFORMANCE in analysis_types:
                self._analyze_performance()

        # Calculate analysis duration
        analysis_duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        self.analysis_result.summary.analysis_duration_ms = analysis_duration_ms

        # Update summary
        self.analysis_result.summary.total_files = len(list(self.codebase.files))
        self.analysis_result.summary.total_functions = len(list(self.codebase.functions))
        self.analysis_result.summary.total_classes = len(list(self.codebase.classes))
        self.analysis_result.summary.total_issues = len(self.issues.issues)
        
        # Set issues collection
        self.analysis_result.issues = self.issues

        # Save or display results if specified
        if output_file and output_format == "json":
            self.analysis_result.save_to_file(output_file)
            logger.info(f"Results saved to {output_file}")
        elif output_format == "html":
            self._generate_html_report(output_file)
        elif output_format == "console":
            self._print_console_report()

        return self.analysis_result

    def _analyze_code_quality(self):
        """Analyze code quality issues in the codebase."""
        logger.info("Starting code quality analysis")
        
        # Initialize result
        result = CodeQualityResult()
        
        # Find dead code
        result.dead_code = self._find_dead_code()
        
        # Analyze function parameters
        result.parameter_issues = self._check_function_parameters()
        
        # Analyze implementation issues
        result.implementation_issues = self._check_implementations()
        
        # Set results
        self.analysis_result.code_quality = result
        
        logger.info(f"Code quality analysis complete. Found {len(self.issues.issues)} issues.")

    def _analyze_dependencies(self):
        """Analyze dependencies between components in the codebase."""
        logger.info("Starting dependency analysis")
        
        # Initialize result
        result = DependencyResult()
        
        # Generate dependency graph
        result.import_dependencies = self._analyze_import_dependencies()
        
        # Find circular dependencies
        result.circular_dependencies = self._find_circular_dependencies()
        
        # Set results
        self.analysis_result.dependencies = result
        
        logger.info("Dependency analysis complete")
        
    def _analyze_performance(self):
        """Analyze performance characteristics of the codebase."""
        logger.info("Starting performance analysis")
        
        # This could include:
        # - Analyzing function complexity and identifying bottlenecks
        # - Calculating cyclomatic complexity metrics
        # - Identifying functions that might benefit from optimization
        # - Finding recursive functions that might cause performance issues
        
        # For now, we'll add performance issues based on function complexity
        functions = list(self.codebase.functions)
        
        for func in functions:
            # Calculate cyclomatic complexity
            complexity = calculate_cyclomatic_complexity(func)
            
            # Functions with high complexity are likely performance bottlenecks
            if complexity > 15:
                file_path = (
                    func.file.file_path
                    if hasattr(func, "file") and hasattr(func.file, "file_path")
                    else "unknown"
                )
                func_name = func.name if hasattr(func, "name") else str(func)
                
                self.issues.add_issue(
                    create_issue(
                        message=f"High complexity function may be a performance bottleneck: {func_name} (complexity: {complexity})",
                        severity=IssueSeverity.WARNING,
                        file=file_path,
                        line=func.line if hasattr(func, "line") else None,
                        category=IssueCategory.PERFORMANCE_ISSUE,
                        symbol=func_name,
                        suggestion="Consider refactoring this function to reduce complexity and improve performance",
                    )
                )
                
            # Check for recursive functions
            if hasattr(func, "function_calls"):
                for call in func.function_calls:
                    if hasattr(call, "name") and call.name == func.name:
                        file_path = (
                            func.file.file_path
                            if hasattr(func, "file") and hasattr(func.file, "file_path")
                            else "unknown"
                        )
                        func_name = func.name if hasattr(func, "name") else str(func)
                        
                        self.issues.add_issue(
                            create_issue(
                                message=f"Recursive function may cause performance issues: {func_name}",
                                severity=IssueSeverity.INFO,
                                file=file_path,
                                line=func.line if hasattr(func, "line") else None,
                                category=IssueCategory.PERFORMANCE_ISSUE,
                                symbol=func_name,
                                suggestion="Ensure recursive function has a proper base case and won't cause stack overflow",
                            )
                        )
                        
        logger.info("Performance analysis complete")

    def _find_dead_code(self) -> dict[str, Any]:
        """
        Find unused code (dead code) in the codebase.

        Returns:
            Dictionary containing dead code analysis results
        """
        logger.info("Analyzing dead code")

        dead_code = {
            "unused_functions": [],
            "unused_classes": [],
            "unused_variables": [],
            "unused_imports": [],
        }

        # Find unused functions
        for function in self.codebase.functions:
            # Skip decorated functions (as they might be used indirectly)
            if hasattr(function, "decorators") and function.decorators:
                continue

            # Check if function has no call sites or usages
            has_call_sites = (
                hasattr(function, "call_sites") and len(function.call_sites) > 0
            )
            has_usages = hasattr(function, "usages") and len(function.usages) > 0

            if not has_call_sites and not has_usages:
                # Skip magic methods and main functions
                if (hasattr(function, "is_magic") and function.is_magic) or (
                    hasattr(function, "name") and function.name in ["main", "__main__"]
                ):
                    continue

                # Get file path and name safely
                file_path = (
                    function.file.file_path
                    if hasattr(function, "file") and hasattr(function.file, "file_path")
                    else "unknown"
                )
                func_name = (
                    function.name if hasattr(function, "name") else str(function)
                )

                # Add to dead code list
                dead_code["unused_functions"].append({
                    "name": func_name,
                    "file": file_path,
                    "line": function.line if hasattr(function, "line") else None,
                })

                # Add issue
                self.issues.add_issue(
                    create_issue(
                        message=f"Unused function: {func_name}",
                        severity=IssueSeverity.WARNING,
                        file=file_path,
                        line=function.line if hasattr(function, "line") else None,
                        category=IssueCategory.DEAD_CODE,
                        symbol=func_name,
                        suggestion="Consider removing this unused function or documenting why it's needed",
                    )
                )

        # Find unused classes
        for cls in self.codebase.classes:
            # Check if class has no usages
            has_usages = hasattr(cls, "usages") and len(cls.usages) > 0

            if not has_usages:
                # Get file path and name safely
                file_path = (
                    cls.file.file_path
                    if hasattr(cls, "file") and hasattr(cls.file, "file_path")
                    else "unknown"
                )
                cls_name = cls.name if hasattr(cls, "name") else str(cls)

                # Add to dead code list
                dead_code["unused_classes"].append({
                    "name": cls_name,
                    "file": file_path,
                    "line": cls.line if hasattr(cls, "line") else None,
                })

                # Add issue
                self.issues.add_issue(
                    create_issue(
                        message=f"Unused class: {cls_name}",
                        severity=IssueSeverity.WARNING,
                        file=file_path,
                        line=cls.line if hasattr(cls, "line") else None,
                        category=IssueCategory.DEAD_CODE,
                        symbol=cls_name,
                        suggestion="Consider removing this unused class or documenting why it's needed",
                    )
                )

        # Find unused imports
        for file in self.codebase.files:
            if hasattr(file, "is_binary") and file.is_binary:
                continue

            if not hasattr(file, "imports"):
                continue

            file_path = file.file_path if hasattr(file, "file_path") else str(file)

            for imp in file.imports:
                if not hasattr(imp, "usages"):
                    continue

                if len(imp.usages) == 0:
                    # Get import source safely
                    import_source = imp.source if hasattr(imp, "source") else str(imp)

                    # Add to dead code list
                    dead_code["unused_imports"].append({
                        "import": import_source,
                        "file": file_path,
                        "line": imp.line if hasattr(imp, "line") else None,
                    })

                    # Add issue
                    # Create Issue directly without using create_issue
                    location = CodeLocation(
                        file=file_path,
                        line=imp.line if hasattr(imp, "line") else None
                    )
                    self.issues.add_issue(
                        Issue(
                            message=f"Unused import: {import_source}",
                            severity=IssueSeverity.INFO,
                            location=location,
                            category=IssueCategory.DEAD_CODE,
                            suggestion="Remove this unused import",
                        )
                    )

        # Add summary statistics
        dead_code["summary"] = {
            "unused_functions_count": len(dead_code["unused_functions"]),
            "unused_classes_count": len(dead_code["unused_classes"]),
            "unused_variables_count": len(dead_code["unused_variables"]),
            "unused_imports_count": len(dead_code["unused_imports"]),
            "total_dead_code_count": (
                len(dead_code["unused_functions"])
                + len(dead_code["unused_classes"])
                + len(dead_code["unused_variables"])
                + len(dead_code["unused_imports"])
            ),
        }

        return dead_code

    def _check_function_parameters(self) -> dict[str, Any]:
        """
        Check for function parameter issues.

        Returns:
            Dictionary containing parameter analysis results
        """
        logger.info("Analyzing function parameters")

        parameter_issues = {
            "missing_types": [],
            "unused_parameters": [],
            "incorrect_usage": [],
        }

        for function in self.codebase.functions:
            # Skip if no parameters
            if not hasattr(function, "parameters"):
                continue

            file_path = (
                function.file.file_path
                if hasattr(function, "file") and hasattr(function.file, "file_path")
                else "unknown"
            )
            func_name = function.name if hasattr(function, "name") else str(function)

            # Check for missing type annotations
            missing_types = []
            for param in function.parameters:
                if not hasattr(param, "name"):
                    continue

                if not hasattr(param, "type") or not param.type:
                    missing_types.append(param.name)

            if missing_types:
                parameter_issues["missing_types"].append({
                    "function": func_name,
                    "file": file_path,
                    "line": function.line if hasattr(function, "line") else None,
                    "parameters": missing_types,
                })

                self.issues.add_issue(
                    create_issue(
                        message=f"Function '{func_name}' has parameters without type annotations: {', '.join(missing_types)}",
                        severity=IssueSeverity.WARNING,
                        file=file_path,
                        line=function.line if hasattr(function, "line") else None,
                        category=IssueCategory.TYPE_ERROR,
                        symbol=func_name,
                        suggestion="Add type annotations to all parameters",
                    )
                )

            # Check for incorrect parameter usage at call sites
            if hasattr(function, "call_sites"):
                for call_site in function.call_sites:
                    # Skip if call site has no arguments
                    if not hasattr(call_site, "args"):
                        continue

                    # Get required parameter count (excluding those with defaults)
                    required_count = 0
                    if hasattr(function, "parameters"):
                        required_count = sum(
                            1
                            for p in function.parameters
                            if not hasattr(p, "has_default") or not p.has_default
                        )

                    # Get call site file info
                    call_file = (
                        call_site.file.file_path
                        if hasattr(call_site, "file")
                        and hasattr(call_site.file, "file_path")
                        else "unknown"
                    )
                    call_line = call_site.line if hasattr(call_site, "line") else None

                    # Check parameter count
                    arg_count = len(call_site.args)
                    if arg_count < required_count:
                        parameter_issues["incorrect_usage"].append({
                            "function": func_name,
                            "caller_file": call_file,
                            "caller_line": call_line,
                            "required_count": required_count,
                            "provided_count": arg_count,
                        })

                        self.issues.add_issue(
                            create_issue(
                                message=f"Call to '{func_name}' has too few arguments ({arg_count} provided, {required_count} required)",
                                severity=IssueSeverity.ERROR,
                                file=call_file,
                                line=call_line,
                                category=IssueCategory.PARAMETER_MISMATCH,
                                symbol=func_name,
                                suggestion=f"Provide all required arguments to '{func_name}'",
                            )
                        )

        # Add summary statistics
        parameter_issues["summary"] = {
            "missing_types_count": len(parameter_issues["missing_types"]),
            "unused_parameters_count": len(parameter_issues["unused_parameters"]),
            "incorrect_usage_count": len(parameter_issues["incorrect_usage"]),
            "total_issues": (
                len(parameter_issues["missing_types"])
                + len(parameter_issues["unused_parameters"])
                + len(parameter_issues["incorrect_usage"])
            ),
        }

        return parameter_issues

    def _check_implementations(self) -> dict[str, Any]:
        """
        Check for implementation issues.

        Returns:
            Dictionary containing implementation analysis results
        """
        logger.info("Analyzing implementations")

        implementation_issues = {
            "empty_functions": [],
            "abstract_methods_without_implementation": [],
            "summary": {
                "empty_functions_count": 0,
                "abstract_methods_without_implementation_count": 0,
            },
        }

        # Check for empty functions
        for function in self.codebase.functions:
            # Get function source
            if hasattr(function, "source"):
                source = function.source

                # Check if function is empty or just has 'pass'
                is_empty = False

                if not source or source.strip() == "":
                    is_empty = True
                else:
                    # Extract function body (skip the first line with the def)
                    body_lines = source.split("\n")[1:] if "\n" in source else []

                    # Check if body is empty or just has whitespace, docstring, or pass
                    non_empty_lines = [
                        line
                        for line in body_lines
                        if line.strip()
                        and not line.strip().startswith("#")
                        and not (
                            line.strip().startswith('"""')
                            or line.strip().startswith("'''")
                        )
                        and line.strip() != "pass"
                    ]

                    if not non_empty_lines:
                        is_empty = True

                if is_empty:
                    # Get file path and name safely
                    file_path = (
                        function.file.file_path
                        if hasattr(function, "file")
                        and hasattr(function.file, "file_path")
                        else "unknown"
                    )
                    func_name = (
                        function.name if hasattr(function, "name") else str(function)
                    )

                    # Skip interface/abstract methods that are supposed to be empty
                    is_abstract = (
                        hasattr(function, "is_abstract") and function.is_abstract
                    ) or (
                        hasattr(function, "parent")
                        and hasattr(function.parent, "is_interface")
                        and function.parent.is_interface
                    )

                    if not is_abstract:
                        # Add to empty functions list
                        implementation_issues["empty_functions"].append({
                            "name": func_name,
                            "file": file_path,
                            "line": function.line
                            if hasattr(function, "line")
                            else None,
                        })

                        # Add issue
                        self.issues.add_issue(
                            create_issue(
                                message=f"Function '{func_name}' is empty",
                                severity=IssueSeverity.WARNING,
                                file=file_path,
                                line=function.line
                                if hasattr(function, "line")
                                else None,
                                category=IssueCategory.MISSING_IMPLEMENTATION,
                                symbol=func_name,
                                suggestion="Implement this function or remove it if not needed",
                            )
                        )

        # Add summary statistics
        implementation_issues["summary"]["empty_functions_count"] = len(
            implementation_issues["empty_functions"]
        )
        implementation_issues["summary"][
            "abstract_methods_without_implementation_count"
        ] = len(implementation_issues["abstract_methods_without_implementation"])

        return implementation_issues

    def _analyze_import_dependencies(self) -> dict[str, Any]:
        """
        Analyze import dependencies between files.

        Returns:
            Dictionary containing import dependency analysis results
        """
        logger.info("Analyzing import dependencies")

        # Generate dependency graph
        dependency_graph = get_dependency_graph(self.codebase)

        # Count dependencies per file
        dependency_counts = {
            file_path: len(deps) for file_path, deps in dependency_graph.items()
        }

        # Find files with high number of dependencies
        high_dependency_files = [
            {"file": file_path, "dependency_count": count}
            for file_path, count in dependency_counts.items()
            if count > 10  # Threshold for "high dependencies"
        ]

        # Sort by dependency count
        high_dependency_files.sort(key=lambda x: x["dependency_count"], reverse=True)

        # Report high dependency files as issues
        for item in high_dependency_files[:10]:  # Limit to top 10
            self.issues.add_issue(
                create_issue(
                    message=f"File has high number of dependencies: {item['dependency_count']}",
                    severity=IssueSeverity.WARNING,
                    file=item["file"],
                    category=IssueCategory.MODULE_COUPLING,
                    suggestion="Consider refactoring to reduce dependencies",
                )
            )

        return {
            "dependency_graph": dependency_graph,
            "dependency_counts": dependency_counts,
            "high_dependency_files": high_dependency_files,
            "summary": {
                "total_files": len(dependency_graph),
                "files_with_high_dependencies": len(high_dependency_files),
                "max_dependencies": max(dependency_counts.values()) if dependency_counts else 0,
                "avg_dependencies": (
                    sum(dependency_counts.values()) / len(dependency_counts)
                    if dependency_counts
                    else 0
                ),
            },
        }

    def _find_circular_dependencies(self) -> dict[str, Any]:
        """
        Find circular dependencies in the codebase.

        Returns:
            Dictionary containing circular dependency analysis results
        """
        logger.info("Analyzing circular dependencies")

        # Get dependency graph
        dependency_graph = get_dependency_graph(self.codebase)
        
        # Find circular dependencies
        circular_deps = []
        visited = set()
        
        def detect_cycles(node, path):
            if node in path:
                # Found a cycle
                cycle_start = path.index(node)
                cycle = path[cycle_start:] + [node]
                cycle_key = "->".join(sorted(cycle))
                
                if cycle_key not in visited:
                    visited.add(cycle_key)
                    circular_deps.append(cycle)
                return
                
            # Continue DFS
            new_path = path + [node]
            for dep in dependency_graph.get(node, []):
                detect_cycles(dep, new_path)
        
        # Start DFS from each node
        for node in dependency_graph:
            detect_cycles(node, [])
            
        # Report circular dependencies as issues
        for cycle in circular_deps:
            cycle_str = " -> ".join(cycle)
            # Report on the first file in the cycle
            self.issues.add_issue(
                create_issue(
                    message=f"Circular dependency detected: {cycle_str}",
                    severity=IssueSeverity.ERROR,
                    file=cycle[0],
                    category=IssueCategory.DEPENDENCY_CYCLE,
                    suggestion="Break the circular dependency by refactoring one of the modules",
                )
            )
            
        return {
            "circular_dependencies": circular_deps,
            "summary": {
                "circular_dependency_count": len(circular_deps),
            },
        }

    def save_results(self, output_file: str):
        """
        Save analysis results to a file.

        Args:
            output_file: Path to the output file
        """
        if not self.analysis_result:
            raise ValueError("No analysis results to save")
            
        self.analysis_result.save_to_file(output_file)
        logger.info(f"Results saved to {output_file}")
        
    def _generate_html_report(self, output_file: str | None = None):
        """
        Generate an HTML report of the analysis results.
        
        Args:
            output_file: Path to save the report to
        """
        if not self.analysis_result:
            raise ValueError("No analysis results to save")
            
        if not output_file:
            output_file = "codebase_analysis_report.html"
        
        # Simple HTML template
        repo_name = self.repo_url.split("/")[-1] if self.repo_url else (self.repo_path or "Unknown")
        analysis_time = self.analysis_result.summary.analysis_time
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Codebase Analysis Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1, h2, h3 {{ color: #333; }}
                .section {{ margin-bottom: 30px; }}
                .metric {{ margin-bottom: 20px; }}
                .metric-title {{ font-weight: bold; }}
                pre {{ background-color: #f5f5f5; padding: 10px; border-radius: 5px; overflow-x: auto; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                tr:nth-child(even) {{ background-color: #f9f9f9; }}
            </style>
        </head>
        <body>
            <h1>Codebase Analysis Report</h1>
            <div class="section">
                <h2>Metadata</h2>
                <p><strong>Repository:</strong> {repo_name}</p>
                <p><strong>Analysis Time:</strong> {analysis_time}</p>
                <p><strong>Language:</strong> {self.language or "Auto-detected"}</p>
            </div>
        """
        
        # Add summary section
        html += f"""
        <div class="section">
            <h2>Summary</h2>
            <table>
                <tr>
                    <th>Metric</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td>Total Files</td>
                    <td>{self.analysis_result.summary.total_files}</td>
                </tr>
                <tr>
                    <td>Total Classes</td>
                    <td>{self.analysis_result.summary.total_classes}</td>
                </tr>
                <tr>
                    <td>Total Functions</td>
                    <td>{self.analysis_result.summary.total_functions}</td>
                </tr>
                <tr>
                    <td>Total Issues</td>
                    <td>{self.analysis_result.summary.total_issues}</td>
                </tr>
                <tr>
                    <td>Analysis Duration</td>
                    <td>{self.analysis_result.summary.analysis_duration_ms / 1000:.2f} seconds</td>
                </tr>
            </table>
        </div>
        """
        
        # Add issues section if there are any
        if self.issues and self.issues.issues:
            issues_by_severity = self.issues.group_by_severity()
            
            html += """
            <div class="section">
                <h2>Issues</h2>
                <h3>By Severity</h3>
                <table>
                    <tr>
                        <th>Severity</th>
                        <th>Count</th>
                    </tr>
            """
            
            for severity in IssueSeverity:
                count = len(issues_by_severity[severity])
                if count > 0:
                    html += f"""
                    <tr>
                        <td>{severity.value}</td>
                        <td>{count}</td>
                    </tr>
                    """
                    
            html += """
                </table>
                
                <h3>Top Issues</h3>
                <table>
                    <tr>
                        <th>Severity</th>
                        <th>Category</th>
                        <th>Message</th>
                        <th>Location</th>
                    </tr>
            """
            
            # Sort issues by severity
            sorted_issues = sorted(self.issues.issues, key=lambda x: {
                IssueSeverity.CRITICAL: 0,
                IssueSeverity.ERROR: 1,
                IssueSeverity.WARNING: 2,
                IssueSeverity.INFO: 3,
            }.get(x.severity, 4))
            
            # Add top 20 issues
            for issue in sorted_issues[:20]:
                location = f"{issue.location.file}"
                if issue.location.line:
                    location += f":{issue.location.line}"
                    
                category = issue.category.value if issue.category else ""
                
                html += f"""
                <tr>
                    <td>{issue.severity.value}</td>
                    <td>{category}</td>
                    <td>{issue.message}</td>
                    <td>{location}</td>
                </tr>
                """
                
            html += """
                </table>
            </div>
            """
        
        # Add analysis results sections
        if self.analysis_result.code_quality:
            html += """
            <div class="section">
                <h2>Code Quality</h2>
            """
            
            # Add dead code section
            if "dead_code" in self.analysis_result.code_quality.__dict__:
                dead_code = self.analysis_result.code_quality.dead_code
                if "summary" in dead_code:
                    html += """
                    <h3>Dead Code</h3>
                    <table>
                        <tr>
                            <th>Metric</th>
                            <th>Count</th>
                        </tr>
                    """
                    
                    for key, value in dead_code["summary"].items():
                        html += f"""
                        <tr>
                            <td>{key.replace("_", " ").title()}</td>
                            <td>{value}</td>
                        </tr>
                        """
                        
                    html += """
                    </table>
                    """
                    
            # Add other code quality sections as needed...
            
            html += """
            </div>
            """
        
        if self.analysis_result.dependencies:
            html += """
            <div class="section">
                <h2>Dependencies</h2>
            """
            
            # Add circular dependencies section
            if "circular_dependencies" in self.analysis_result.dependencies.__dict__:
                circular_deps = self.analysis_result.dependencies.circular_dependencies
                if "circular_dependencies" in circular_deps:
                    html += """
                    <h3>Circular Dependencies</h3>
                    <table>
                        <tr>
                            <th>#</th>
                            <th>Dependency Cycle</th>
                        </tr>
                    """
                    
                    for i, cycle in enumerate(circular_deps["circular_dependencies"][:10], 1):
                        html += f"""
                        <tr>
                            <td>{i}</td>
                            <td>{" -> ".join(cycle)}</td>
                        </tr>
                        """
                        
                    html += """
                    </table>
                    """
            
            html += """
            </div>
            """
        
        # Close the HTML
        html += """
        </body>
        </html>
        """
        
        with open(output_file, "w") as f:
            f.write(html)
            
        logger.info(f"HTML report saved to {output_file}")
        
    def _print_console_report(self):
        """Print a summary report to the console."""
        if not self.analysis_result:
            raise ValueError("No analysis results to print")
            
        repo_name = self.repo_url.split("/")[-1] if self.repo_url else (self.repo_path or "Unknown")
        
        print(f"\n{'=' * 80}")
        print(f"CODEBASE ANALYSIS REPORT: {repo_name}")
        print(f"{'=' * 80}")
        print(f"Analysis Time: {self.analysis_result.summary.analysis_time}")
        print(f"Language: {self.language or 'Auto-detected'}")
        print(f"Analysis Duration: {self.analysis_result.summary.analysis_duration_ms / 1000:.2f} seconds")
        
        # Print summary
        print(f"\n{'-' * 40}")
        print("SUMMARY:")
        print(f"{'-' * 40}")
        print(f"Total Files: {self.analysis_result.summary.total_files}")
        print(f"Total Classes: {self.analysis_result.summary.total_classes}")
        print(f"Total Functions: {self.analysis_result.summary.total_functions}")
        print(f"Total Issues: {self.analysis_result.summary.total_issues}")
        
        # Print issues by severity if there are any
        if self.issues and self.issues.issues:
            issues_by_severity = self.issues.group_by_severity()
            
            print(f"\n{'-' * 40}")
            print("ISSUES BY SEVERITY:")
            print(f"{'-' * 40}")
            for severity in IssueSeverity:
                count = len(issues_by_severity[severity])
                if count > 0:
                    print(f"{severity.value}: {count}")
            
            # Print top issues
            print(f"\n{'-' * 40}")
            print("TOP ISSUES:")
            print(f"{'-' * 40}")
            
            # Sort issues by severity
            sorted_issues = sorted(self.issues.issues, key=lambda x: {
                IssueSeverity.CRITICAL: 0,
                IssueSeverity.ERROR: 1,
                IssueSeverity.WARNING: 2,
                IssueSeverity.INFO: 3,
            }.get(x.severity, 4))
            
            # Print top 10 issues
            for i, issue in enumerate(sorted_issues[:10], 1):
                severity_icon = {
                    IssueSeverity.CRITICAL: "❌",
                    IssueSeverity.ERROR: "⛔",
                    IssueSeverity.WARNING: "⚠️",
                    IssueSeverity.INFO: "ℹ️",
                }.get(issue.severity, "")
                
                location = f"{issue.location.file}"
                if issue.location.line:
                    location += f":{issue.location.line}"
                
                category = f"[{issue.category.value}]" if issue.category else ""
                
                print(f"{i}. {severity_icon} {category} {issue.message}")
                print(f"   Location: {location}")
                if issue.suggestion:
                    print(f"   Suggestion: {issue.suggestion}")
                print()
        
        # Print analysis type summaries
        if self.analysis_result.code_quality:
            print(f"\n{'-' * 40}")
            print("CODE QUALITY SUMMARY:")
            print(f"{'-' * 40}")
            
            # Print dead code summary
            if "dead_code" in self.analysis_result.code_quality.__dict__:
                dead_code = self.analysis_result.code_quality.dead_code
                if "summary" in dead_code:
                    for key, value in dead_code["summary"].items():
                        print(f"{key.replace('_', ' ').title()}: {value}")
                        
        # Print dependencies summary
        if self.analysis_result.dependencies:
            print(f"\n{'-' * 40}")
            print("DEPENDENCIES SUMMARY:")
            print(f"{'-' * 40}")
            
            # Print circular dependencies
            if "circular_dependencies" in self.analysis_result.dependencies.__dict__:
                circular_deps = self.analysis_result.dependencies.circular_dependencies
                if "circular_dependencies" in circular_deps:
                    cycles = circular_deps["circular_dependencies"]
                    print(f"Circular Dependencies: {len(cycles)}")
                    if cycles:
                        print("\nExample circular dependencies:")
                        for i, cycle in enumerate(cycles[:3], 1):
                            print(f"{i}. {' -> '.join(cycle)}")


#######################################################
# Helper Functions
#######################################################

def analyze_codebase(
    repo_path: str | None = None,
    repo_url: str | None = None,
    output_file: str | None = None,
    analysis_types: list[str] | None = None,
    language: str | None = None,
    output_format: str = "json",
) -> AnalysisResult:
    """
    Analyze a codebase and optionally save results to a file.
    
    Args:
        repo_path: Path to the repository to analyze
        repo_url: URL of the repository to analyze
        output_file: Optional path to save results to
        analysis_types: Optional list of analysis types to perform
        language: Optional programming language of the codebase
        output_format: Format for output (json, html, console)
        
    Returns:
        AnalysisResult containing the findings
    """
    # Convert string analysis types to enum values
    analysis_type_enums = []
    if analysis_types:
        for at in analysis_types:
            try:
                analysis_type_enums.append(AnalysisType(at))
            except ValueError:
                logger.warning(f"Unknown analysis type: {at}")
    
    # Initialize analyzer
    if repo_path:
        analyzer = CodebaseAnalyzer(repo_path=repo_path, language=language)
    elif repo_url:
        analyzer = CodebaseAnalyzer(repo_url=repo_url, language=language)
    else:
        raise ValueError("Either repo_path or repo_url must be provided")
    
    # Perform analysis
    result = analyzer.analyze(
        analysis_type_enums,
        output_format=output_format,
        output_file=output_file
    )
    
    return result


# ================================================================================
# API.PY - FASTAPI WEB SERVICE
# ================================================================================

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Dict, List, Tuple, Any, Optional
from codegen import Codebase
from codegen.sdk.core.statements.for_loop_statement import ForLoopStatement
from codegen.sdk.core.statements.if_block_statement import IfBlockStatement
from codegen.sdk.core.statements.try_catch_statement import TryCatchStatement
from codegen.sdk.core.statements.while_statement import WhileStatement
from codegen.sdk.core.expressions.binary_expression import BinaryExpression
from codegen.sdk.core.expressions.unary_expression import UnaryExpression
from codegen.sdk.core.expressions.comparison_expression import ComparisonExpression
import math
import re
import requests
from datetime import datetime, timedelta
import subprocess
import os
import tempfile
from fastapi.middleware.cors import CORSMiddleware
import modal
from collections import Counter
import networkx as nx
from pathlib import Path
from codegen.sdk.core.class_definition import Class
from codegen.sdk.core.codebase import Codebase
from codegen.sdk.core.external_module import ExternalModule
from codegen.sdk.core.file import SourceFile
from codegen.sdk.core.function import Function
from codegen.sdk.core.import_resolution import Import
from codegen.sdk.core.symbol import Symbol
from codegen.sdk.enums import EdgeType, SymbolType

image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .pip_install(
        "codegen", "fastapi", "uvicorn", "gitpython", "requests", "pydantic", "datetime",
        "networkx"  # Added for call chain analysis
    )
)

app = modal.App(name="analytics-app", image=image)
fastapi_app = FastAPI()

fastapi_app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Base models for codebase analysis
class CodebaseStats(BaseModel):
    test_functions_count: int
    test_classes_count: int
    tests_per_file: float
    total_classes: int
    total_functions: int
    total_imports: int
    deepest_inheritance_class: Optional[Dict]
    recursive_functions: List[str]
    most_called_function: Dict
    function_with_most_calls: Dict
    unused_functions: List[Dict]
    dead_code: List[Dict]

class FileTestStats(BaseModel):
    filepath: str
    test_class_count: int
    file_length: int
    function_count: int

class FunctionContext(BaseModel):
    implementation: Dict
    dependencies: List[Dict]
    usages: List[Dict]

# Models for extended analysis
class TestAnalysis(BaseModel):
    total_test_functions: int
    total_test_classes: int
    tests_per_file: float
    top_test_files: List[Dict[str, Any]]  # Changed from 'any' to 'Any'

class FunctionAnalysis(BaseModel):
    total_functions: int
    most_called_function: Dict[str, Any]
    function_with_most_calls: Dict[str, Any]
    recursive_functions: List[str]
    unused_functions: List[Dict[str, str]]
    dead_code: List[Dict[str, str]]

class ClassAnalysis(BaseModel):
    total_classes: int
    deepest_inheritance: Optional[Dict[str, Any]]
    total_imports: int

class FileIssue(BaseModel):
    critical: List[Dict[str, str]]
    major: List[Dict[str, str]]
    minor: List[Dict[str, str]]

class ExtendedAnalysis(BaseModel):
    test_analysis: TestAnalysis
    function_analysis: FunctionAnalysis
    class_analysis: ClassAnalysis
    file_issues: Dict[str, FileIssue]
    repo_structure: Dict[str, Any]

class RepoRequest(BaseModel):
    repo_url: str

class Symbol(BaseModel):
    id: str
    name: str
    type: str  # 'function', 'class', or 'variable'
    filepath: str
    start_line: int
    end_line: int
    issues: Optional[List[Dict[str, str]]] = None

class FileNode(BaseModel):
    name: str
    type: str  # 'file' or 'directory'
    path: str
    issues: Optional[Dict[str, int]] = None
    symbols: Optional[List[Symbol]] = None
    children: Optional[Dict[str, 'FileNode']] = None

class AnalysisResponse(BaseModel):
    # Basic stats
    repo_url: str
    description: str
    num_files: int
    num_functions: int
    num_classes: int
    
    # Line metrics
    line_metrics: Dict[str, Dict[str, float]]
    
    # Complexity metrics
    cyclomatic_complexity: Dict[str, float]
    depth_of_inheritance: Dict[str, float]
    halstead_metrics: Dict[str, int]
    maintainability_index: Dict[str, int]
    
    # Git metrics
    monthly_commits: Dict[str, int]
    
    # Repository structure with symbols
    repo_structure: FileNode

def get_monthly_commits(repo_path: str) -> Dict[str, int]:
    """
    Get the number of commits per month for the last 12 months.

    Args:
        repo_path: Path to the git repository

    Returns:
        Dictionary with month-year as key and number of commits as value
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365)

    date_format = "%Y-%m-%d"
    since_date = start_date.strftime(date_format)
    until_date = end_date.strftime(date_format)
    repo_path = "https://github.com/" + repo_path

    try:
        original_dir = os.getcwd()

        with tempfile.TemporaryDirectory() as temp_dir:
            subprocess.run(["git", "clone", repo_path, temp_dir], check=True)
            os.chdir(temp_dir)

            cmd = [
                "git",
                "log",
                f"--since={since_date}",
                f"--until={until_date}",
                "--format=%aI",
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            commit_dates = result.stdout.strip().split("\n")

            monthly_counts = {}
            current_date = start_date
            while current_date <= end_date:
                month_key = current_date.strftime("%Y-%m")
                monthly_counts[month_key] = 0
                current_date = (
                    current_date.replace(day=1) + timedelta(days=32)
                ).replace(day=1)

            for date_str in commit_dates:
                if date_str:  # Skip empty lines
                    commit_date = datetime.fromisoformat(date_str.strip())
                    month_key = commit_date.strftime("%Y-%m")
                    if month_key in monthly_counts:
                        monthly_counts[month_key] += 1

            os.chdir(original_dir)
            return dict(sorted(monthly_counts.items()))

    except subprocess.CalledProcessError as e:
        print(f"Error executing git command: {e}")
        return {}
    except Exception as e:
        print(f"Error processing git commits: {e}")
        return {}
    finally:
        try:
            os.chdir(original_dir)
        except:
            pass

def api_calculate_cyclomatic_complexity(function):
    def analyze_statement(statement):
        complexity = 0

        if isinstance(statement, IfBlockStatement):
            complexity += 1
            if hasattr(statement, "elif_statements"):
                complexity += len(statement.elif_statements)

        elif isinstance(statement, (ForLoopStatement, WhileStatement)):
            complexity += 1

        elif isinstance(statement, TryCatchStatement):
            complexity += len(getattr(statement, "except_blocks", []))

        if hasattr(statement, "condition") and isinstance(statement.condition, str):
            complexity += statement.condition.count(
                " and "
            ) + statement.condition.count(" or ")

        if hasattr(statement, "nested_code_blocks"):
            for block in statement.nested_code_blocks:
                complexity += analyze_block(block)

        return complexity

    def analyze_block(block):
        if not block or not hasattr(block, "statements"):
            return 0
        return sum(analyze_statement(stmt) for stmt in block.statements)

    return (
        1 + analyze_block(function.code_block) if hasattr(function, "code_block") else 1
    )

def cc_rank(complexity):
    if complexity < 0:
        raise ValueError("Complexity must be a non-negative value")

    ranks = [
        (1, 5, "A"),
        (6, 10, "B"),
        (11, 20, "C"),
        (21, 30, "D"),
        (31, 40, "E"),
        (41, float("inf"), "F"),
    ]
    for low, high, rank in ranks:
        if low <= complexity <= high:
            return rank
    return "F"

def calculate_doi(cls):
    """Calculate the depth of inheritance for a given class."""
    return len(cls.superclasses)

def get_operators_and_operands(function):
    operators = []
    operands = []

    for statement in function.code_block.statements:
        for call in statement.function_calls:
            operators.append(call.name)
            for arg in call.args:
                operands.append(arg.source)

        if hasattr(statement, "expressions"):
            for expr in statement.expressions:
                if isinstance(expr, BinaryExpression):
                    operators.extend([op.source for op in expr.operators])
                    operands.extend([elem.source for elem in expr.elements])
                elif isinstance(expr, UnaryExpression):
                    operators.append(expr.ts_node.type)
                    operands.append(expr.argument.source)
                elif isinstance(expr, ComparisonExpression):
                    operators.extend([op.source for op in expr.operators])
                    operands.extend([elem.source for elem in expr.elements])

        if hasattr(statement, "expression"):
            expr = statement.expression
            if isinstance(expr, BinaryExpression):
                operators.extend([op.source for op in expr.operators])
                operands.extend([elem.source for elem in expr.elements])
            elif isinstance(expr, UnaryExpression):
                operators.append(expr.ts_node.type)
                operands.append(expr.argument.source)
            elif isinstance(expr, ComparisonExpression):
                operators.extend([op.source for op in expr.operators])
                operands.extend([elem.source for elem in expr.elements])

    return operators, operands

def calculate_halstead_volume(operators, operands):
    n1 = len(set(operators))
    n2 = len(set(operands))

    N1 = len(operators)
    N2 = len(operands)

    N = N1 + N2
    n = n1 + n2

    if n > 0:
        volume = N * math.log2(n)
        return volume, N1, N2, n1, n2
    return 0, N1, N2, n1, n2

def count_lines(source: str):
    """Count different types of lines in source code."""
    if not source.strip():
        return 0, 0, 0, 0

    lines = [line.strip() for line in source.splitlines()]
    loc = len(lines)
    sloc = len([line for line in lines if line])

    in_multiline = False
    comments = 0
    code_lines = []

    i = 0
    while i < len(lines):
        line = lines[i]
        code_part = line
        if not in_multiline and "#" in line:
            comment_start = line.find("#")
            if not re.search(r'["\'].*#.*["\']', line[:comment_start]):
                code_part = line[:comment_start].strip()
                if line[comment_start:].strip():
                    comments += 1

        if ('"""' in line or "'''" in line) and not (
            line.count('"""') % 2 == 0 or line.count("'''") % 2 == 0
        ):
            if in_multiline:
                in_multiline = False
                comments += 1
            else:
                in_multiline = True
                comments += 1
                if line.strip().startswith('"""') or line.strip().startswith("'''"):
                    code_part = ""
        elif in_multiline:
            comments += 1
            code_part = ""
        elif line.strip().startswith("#"):
            comments += 1
            code_part = ""

        if code_part.strip():
            code_lines.append(code_part)

        i += 1

    lloc = 0
    continued_line = False
    for line in code_lines:
        if continued_line:
            if not any(line.rstrip().endswith(c) for c in ("\\", ",", "{", "[", "(")):
                continued_line = False
            continue

        lloc += len([stmt for stmt in line.split(";") if stmt.strip()])

        if any(line.rstrip().endswith(c) for c in ("\\", ",", "{", "[", "(")):
            continued_line = True

    return loc, lloc, sloc, comments

def calculate_maintainability_index(
    halstead_volume: float, cyclomatic_complexity: float, loc: int
) -> int:
    """Calculate the normalized maintainability index for a given function."""
    if loc <= 0:
        return 100

    try:
        raw_mi = (
            171
            - 5.2 * math.log(max(1, halstead_volume))
            - 0.23 * cyclomatic_complexity
            - 16.2 * math.log(max(1, loc))
        )
        normalized_mi = max(0, min(100, raw_mi * 100 / 171))
        return int(normalized_mi)
    except (ValueError, TypeError):
        return 0

def get_maintainability_rank(mi_score: float) -> str:
    """Convert maintainability index score to a letter grade."""
    if mi_score >= 85:
        return "A"
    elif mi_score >= 65:
        return "B"
    elif mi_score >= 45:
        return "C"
    elif mi_score >= 25:
        return "D"
    else:
        return "F"

def get_github_repo_description(repo_url):
    api_url = f"https://api.github.com/repos/{repo_url}"

    response = requests.get(api_url)

    if response.status_code == 200:
        repo_data = response.json()
        return repo_data.get("description", "No description available")
    else:
        return ""

def find_dead_code(codebase) -> List:
    """Find functions that are never called."""
    dead_functions = []
    for function in codebase.functions:
        if not any(function.function_calls):
            dead_functions.append(function)
    return dead_functions

def get_max_call_chain(function) -> List:
    """Get the longest call chain starting from a function."""
    G = nx.DiGraph()
    
    def build_graph(func, depth=0):
        if depth > 10:  # Prevent infinite recursion
            return
        for call in func.function_calls:
            called_func = call.function_definition
            G.add_edge(func, called_func)
            build_graph(called_func, depth + 1)
    
    build_graph(function)
    return nx.dag_longest_path(G)

def analyze_file_issues(file) -> Dict[str, List[Dict[str, str]]]:
    """Analyze a file for various types of issues."""
    issues = {
        'critical': [],
        'major': [],
        'minor': []
    }
    
    # Check for implementation errors
    for function in file.functions:
        # Check for unused parameters
        for param in function.parameters:
            if not any(param.name in str(usage) for usage in function.usages):
                issues['minor'].append({
                    'type': 'unused_parameter',
                    'message': f'Unused parameter "{param.name}" in function "{function.name}"'
                })

        # Check for null references
        if hasattr(function, 'code_block'):
            code = function.code_block.source
            if 'None' in code and not any(s in code for s in ['is None', '== None', '!= None']):
                issues['critical'].append({
                    'type': 'unsafe_null_check',
                    'message': f'Potential unsafe null reference in function "{function.name}"'
                })

        # Check for incomplete implementations
        if 'TODO' in function.source or 'FIXME' in function.source:
            issues['major'].append({
                'type': 'incomplete_implementation',
                'message': f'Incomplete implementation in function "{function.name}"'
            })

    # Check for code duplication
    seen_blocks = {}
    for function in file.functions:
        if hasattr(function, 'code_block'):
            code = function.code_block.source.strip()
            if len(code) > 50:  # Only check substantial blocks
                if code in seen_blocks:
                    issues['major'].append({
                        'type': 'code_duplication',
                        'message': f'Code duplication between functions "{function.name}" and "{seen_blocks[code]}"'
                    })
                else:
                    seen_blocks[code] = function.name

    return issues

def build_repo_structure(files, file_issues, file_symbols) -> Dict:
    """Build a hierarchical repository structure with issue counts and symbols."""
    root = {
        'name': 'root',
        'type': 'directory',
        'path': '',
        'children': {},
        'issues': {'critical': 0, 'major': 0, 'minor': 0},
        'stats': {
            'files': 0,
            'directories': 0,
            'symbols': 0,
            'issues': 0
        }
    }
    
    # First pass: Create all directories
    all_dirs = set()
    for file in files:
        dir_path = os.path.dirname(file.filepath)
        if dir_path:
            parts = dir_path.split('/')
            current_path = ''
            for part in parts:
                current_path = os.path.join(current_path, part) if current_path else part
                all_dirs.add(current_path)
    
    # Create directory nodes
    for dir_path in sorted(all_dirs):
        parts = dir_path.split('/')
        current = root
        current_path = ''
        
        for part in parts:
            current_path = os.path.join(current_path, part) if current_path else part
            if part not in current['children']:
                current['children'][part] = {
                    'name': part,
                    'type': 'directory',
                    'path': current_path,
                    'children': {},
                    'issues': {'critical': 0, 'major': 0, 'minor': 0},
                    'stats': {
                        'files': 0,
                        'directories': 0,
                        'symbols': 0,
                        'issues': 0
                    }
                }
                current['stats']['directories'] += 1
            current = current['children'][part]
    
    # Add files
    for file in sorted(files, key=lambda f: f.filepath):
        dir_path = os.path.dirname(file.filepath)
        filename = os.path.basename(file.filepath)
        
        # Navigate to the correct directory
        current = root
        if dir_path:
            for part in dir_path.split('/'):
                current = current['children'][part]
        
        # Create file node
        file_node = {
            'name': filename,
            'type': 'file',
            'file_type': get_file_type(filename),
            'path': file.filepath,
            'issues': {'critical': 0, 'major': 0, 'minor': 0},
            'stats': {
                'symbols': 0,
                'issues': 0
            }
        }
        
        # Add issue counts
        if file.filepath in file_issues:
            issues = file_issues[file.filepath]
            file_node['issues'] = {
                'critical': len(issues['critical']),
                'major': len(issues['major']),
                'minor': len(issues['minor'])
            }
            file_node['stats']['issues'] = sum(file_node['issues'].values())
            
            # Propagate issue counts up the tree
            temp_path = dir_path
            temp = current
            while temp is not None:
                for severity in ['critical', 'major', 'minor']:
                    temp['issues'][severity] += file_node['issues'][severity]
                temp['stats']['issues'] += file_node['stats']['issues']
                if temp_path:
                    parent_path = os.path.dirname(temp_path)
                    temp = root
                    if parent_path:
                        for part in parent_path.split('/'):
                            temp = temp['children'][part]
                    temp_path = parent_path
                else:
                    temp = None
        
        # Add symbols
        if file.filepath in file_symbols:
            file_node['symbols'] = file_symbols[file.filepath]
            file_node['stats']['symbols'] = len(file_symbols[file.filepath])
            
            # Propagate symbol counts up the tree
            temp_path = dir_path
            temp = current
            while temp is not None:
                temp['stats']['symbols'] += file_node['stats']['symbols']
                if temp_path:
                    parent_path = os.path.dirname(temp_path)
                    temp = root
                    if parent_path:
                        for part in parent_path.split('/'):
                            temp = temp['children'][part]
                    temp_path = parent_path
                else:
                    temp = None
        
        current['children'][filename] = file_node
        current['stats']['files'] += 1
        root['stats']['files'] += 1
    
    return root

def get_file_type(filename: str) -> str:
    """Get the type of file based on its extension."""
    ext = Path(filename).suffix.lower()
    if ext in ['.py', '.pyi', '.pyx']:
        return 'python'
    elif ext in ['.js', '.jsx', '.ts', '.tsx']:
        return 'javascript'
    elif ext in ['.java']:
        return 'java'
    elif ext in ['.c', '.cpp', '.h', '.hpp']:
        return 'cpp'
    elif ext in ['.go']:
        return 'go'
    elif ext in ['.rs']:
        return 'rust'
    elif ext in ['.rb']:
        return 'ruby'
    elif ext in ['.php']:
        return 'php'
    elif ext in ['.cs']:
        return 'csharp'
    elif ext in ['.swift']:
        return 'swift'
    elif ext in ['.kt']:
        return 'kotlin'
    elif ext in ['.scala']:
        return 'scala'
    elif ext in ['.html', '.htm']:
        return 'html'
    elif ext in ['.css', '.scss', '.sass', '.less']:
        return 'css'
    elif ext in ['.json']:
        return 'json'
    elif ext in ['.xml']:
        return 'xml'
    elif ext in ['.md', '.markdown']:
        return 'markdown'
    elif ext in ['.yml', '.yaml']:
        return 'yaml'
    elif ext in ['.sh', '.bash']:
        return 'shell'
    elif ext in ['.sql']:
        return 'sql'
    elif ext in ['.dockerfile', '.containerfile']:
        return 'docker'
    elif ext in ['.gitignore', '.dockerignore']:
        return 'config'
    elif ext in ['.txt']:
        return 'text'
    else:
        return 'unknown'

def get_detailed_symbol_context(symbol: Symbol) -> Dict[str, Any]:
    """Get detailed context for any symbol type."""
    base_info = {
        'id': str(hash(symbol.name + symbol.filepath)),
        'name': symbol.name,
        'type': symbol.__class__.__name__,
        'filepath': symbol.filepath,
        'start_line': symbol.start_point[0] if hasattr(symbol, 'start_point') else 0,
        'end_line': symbol.end_point[0] if hasattr(symbol, 'end_point') else 0,
        'source': symbol.source if hasattr(symbol, 'source') else None,
    }

    # Get usage statistics
    usages = symbol.symbol_usages
    imported_symbols = [x.imported_symbol for x in usages if isinstance(x, Import)]
    
    usage_stats = {
        'total_usages': len(usages),
        'usage_breakdown': {
            'functions': len([x for x in usages if isinstance(x, Symbol) and x.symbol_type == SymbolType.Function]),
            'classes': len([x for x in usages if isinstance(x, Symbol) and x.symbol_type == SymbolType.Class]),
            'global_vars': len([x for x in usages if isinstance(x, Symbol) and x.symbol_type == SymbolType.GlobalVar]),
            'interfaces': len([x for x in usages if isinstance(x, Symbol) and x.symbol_type == SymbolType.Interface])
        },
        'imports': {
            'total': len(imported_symbols),
            'breakdown': {
                'functions': len([x for x in imported_symbols if isinstance(x, Symbol) and x.symbol_type == SymbolType.Function]),
                'classes': len([x for x in imported_symbols if isinstance(x, Symbol) and x.symbol_type == SymbolType.Class]),
                'global_vars': len([x for x in imported_symbols if isinstance(x, Symbol) and x.symbol_type == SymbolType.GlobalVar]),
                'interfaces': len([x for x in imported_symbols if isinstance(x, Symbol) and x.symbol_type == SymbolType.Interface]),
                'external_modules': len([x for x in imported_symbols if isinstance(x, ExternalModule)]),
                'files': len([x for x in imported_symbols if isinstance(x, SourceFile)])
            }
        }
    }

    # Add type-specific information
    if isinstance(symbol, Function):
        base_info.update({
            'function_info': {
                'return_statements': len(symbol.return_statements),
                'parameters': [
                    {
                        'name': p.name,
                        'type': p.type if hasattr(p, 'type') else None,
                        'default_value': p.default_value if hasattr(p, 'default_value') else None
                    }
                    for p in symbol.parameters
                ],
                'function_calls': [
                    {
                        'name': call.name,
                        'args': [arg.source for arg in call.args] if hasattr(call, 'args') else [],
                        'line': call.start_point[0] if hasattr(call, 'start_point') else 0
                    }
                    for call in symbol.function_calls
                ],
                'call_sites': [
                    {
                        'caller': site.parent_function.name if hasattr(site, 'parent_function') else None,
                        'line': site.start_point[0] if hasattr(site, 'start_point') else 0,
                        'file': site.filepath if hasattr(site, 'filepath') else None
                    }
                    for site in symbol.call_sites
                ],
                'decorators': [d.source for d in symbol.decorators] if hasattr(symbol, 'decorators') else [],
                'dependencies': [
                    {
                        'name': dep.name,
                        'type': dep.__class__.__name__,
                        'filepath': dep.filepath if hasattr(dep, 'filepath') else None
                    }
                    for dep in symbol.dependencies
                ] if hasattr(symbol, 'dependencies') else []
            }
        })

        # Add complexity metrics
        if hasattr(symbol, 'code_block'):
            complexity = calculate_cyclomatic_complexity(symbol)
            operators, operands = get_operators_and_operands(symbol)
            volume, N1, N2, n1, n2 = calculate_halstead_volume(operators, operands)
            loc = len(symbol.code_block.source.splitlines())
            mi_score = calculate_maintainability_index(volume, complexity, loc)

            base_info['metrics'] = {
                'cyclomatic_complexity': {
                    'value': complexity,
                    'rank': cc_rank(complexity)
                },
                'halstead_metrics': {
                    'volume': volume,
                    'unique_operators': n1,
                    'unique_operands': n2,
                    'total_operators': N1,
                    'total_operands': N2
                },
                'maintainability_index': {
                    'value': mi_score,
                    'rank': get_maintainability_rank(mi_score)
                },
                'lines_of_code': {
                    'total': loc,
                    'code': len([l for l in symbol.code_block.source.splitlines() if l.strip()]),
                    'comments': len([l for l in symbol.code_block.source.splitlines() if l.strip().startswith('#')])
                }
            }

    elif isinstance(symbol, Class):
        base_info.update({
            'class_info': {
                'parent_classes': symbol.parent_class_names,
                'methods': [
                    {
                        'name': m.name,
                        'parameters': len(m.parameters) if hasattr(m, 'parameters') else 0,
                        'line': m.start_point[0] if hasattr(m, 'start_point') else 0
                    }
                    for m in symbol.methods
                ],
                'attributes': [
                    {
                        'name': a.name,
                        'type': a.type if hasattr(a, 'type') else None,
                        'line': a.start_point[0] if hasattr(a, 'start_point') else 0
                    }
                    for a in symbol.attributes
                ],
                'decorators': [d.source for d in symbol.decorators] if hasattr(symbol, 'decorators') else [],
                'dependencies': [
                    {
                        'name': dep.name,
                        'type': dep.__class__.__name__,
                        'filepath': dep.filepath if hasattr(dep, 'filepath') else None
                    }
                    for dep in symbol.dependencies
                ] if hasattr(symbol, 'dependencies') else [],
                'inheritance_depth': len(symbol.superclasses) if hasattr(symbol, 'superclasses') else 0,
                'inheritance_chain': [
                    {
                        'name': s.name,
                        'filepath': s.filepath if hasattr(s, 'filepath') else None
                    }
                    for s in symbol.superclasses
                ] if hasattr(symbol, 'superclasses') else []
            }
        })

    base_info['usage_stats'] = usage_stats
    return base_info

@fastapi_app.get("/api/codebase/stats")
async def get_codebase_stats(codebase_id: str) -> CodebaseStats:
    """Get comprehensive statistics about the codebase."""
    try:
        # Filter test functions and classes
        test_functions = [x for x in codebase.functions if x.name.startswith('test_')]
        test_classes = [x for x in codebase.classes if x.name.startswith('Test')]
        
        # Calculate tests per file
        tests_per_file = len(test_functions) / len(codebase.files) if codebase.files else 0
        
        # Find class with deepest inheritance
        deepest_class = None
        if codebase.classes:
            deepest = max(codebase.classes, key=lambda x: len(x.superclasses))
            deepest_class = {
                'name': deepest.name,
                'depth': len(deepest.superclasses),
                'chain': [s.name for s in deepest.superclasses]
            }
        
        # Find recursive functions
        recursive = [f.name for f in codebase.functions 
                    if any(call.name == f.name for call in f.function_calls)][:5]
        
        # Find most called function
        most_called = max(codebase.functions, key=lambda f: len(f.call_sites))
        most_called_info = {
            'name': most_called.name,
            'call_count': len(most_called.call_sites),
            'callers': [{'function': call.parent_function.name, 
                        'line': call.start_point[0]} 
                       for call in most_called.call_sites]
        }
        
        # Find function with most calls
        most_calls = max(codebase.functions, key=lambda f: len(f.function_calls))
        most_calls_info = {
            'name': most_calls.name,
            'calls_count': len(most_calls.function_calls),
            'called_functions': [call.name for call in most_calls.function_calls]
        }
        
        # Find unused functions
        unused = [{'name': f.name, 'filepath': f.filepath} 
                 for f in codebase.functions if len(f.call_sites) == 0]
        
        # Find dead code
        dead_code = find_dead_code(codebase)
        dead_code_info = [{'name': f.name, 'filepath': f.filepath} for f in dead_code]
        
        return CodebaseStats(
            test_functions_count=len(test_functions),
            test_classes_count=len(test_classes),
            tests_per_file=tests_per_file,
            total_classes=len(codebase.classes),
            total_functions=len(codebase.functions),
            total_imports=len(codebase.imports),
            deepest_inheritance_class=deepest_class,
            recursive_functions=recursive,
            most_called_function=most_called_info,
            function_with_most_calls=most_calls_info,
            unused_functions=unused,
            dead_code=dead_code_info
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@fastapi_app.get("/api/codebase/test-files")
async def get_test_file_stats(codebase_id: str) -> List[FileTestStats]:
    """Get statistics about test files in the codebase."""
    try:
        test_classes = [x for x in codebase.classes if x.name.startswith('Test')]
        file_test_counts = Counter([x.file for x in test_classes])
        
        stats = []
        for file, num_tests in file_test_counts.most_common()[:5]:
            stats.append(FileTestStats(
                filepath=file.filepath,
                test_class_count=num_tests,
                file_length=len(file.source),
                function_count=len(file.functions)
            ))
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@fastapi_app.get("/api/function/{function_id}/context")
async def get_function_context(function_id: str) -> FunctionContext:
    """Get detailed context for a specific function."""
    try:
        function = get_function_by_id(function_id)  # You'll need to implement this
        
        context = {
            "implementation": {
                "source": function.source,
                "filepath": function.filepath
            },
            "dependencies": [],
            "usages": []
        }
        
        # Add dependencies
        for dep in function.dependencies:
            if isinstance(dep, Import):
                dep = hop_through_imports(dep)  # You'll need to implement this
            context["dependencies"].append({
                "source": dep.source,
                "filepath": dep.filepath
            })
        
        # Add usages
        for usage in function.usages:
            context["usages"].append({
                "source": usage.usage_symbol.source,
                "filepath": usage.usage_symbol.filepath
            })
        
        return FunctionContext(**context)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@fastapi_app.get("/api/function/{function_id}/call-chain")
async def get_function_call_chain(function_id: str) -> List[str]:
    """Get the maximum call chain for a function."""
    try:
        function = get_function_by_id(function_id)
        chain = get_max_call_chain(function)
        return [f.name for f in chain]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@fastapi_app.post("/analyze_repo")
async def analyze_repo(request: RepoRequest) -> AnalysisResponse:
    """Single entry point for repository analysis."""
    repo_url = request.repo_url
    codebase = Codebase.from_repo(repo_url)

    # Original analysis
    num_files = len(codebase.files(extensions="*"))
    num_functions = len(codebase.functions)
    num_classes = len(codebase.classes)

    total_loc = total_lloc = total_sloc = total_comments = 0
    total_complexity = 0
    total_volume = 0
    total_mi = 0
    total_doi = 0

    monthly_commits = get_monthly_commits(repo_url)

    # Analyze files and collect symbols
    file_issues = {}
    file_symbols = {}
    
    for file in codebase.files:
        # Line metrics
        loc, lloc, sloc, comments = count_lines(file.source)
        total_loc += loc
        total_lloc += lloc
        total_sloc += sloc
        total_comments += comments

        # Analyze issues
        issues = analyze_file_issues(file)
        if any(len(v) > 0 for v in issues.values()):
            file_issues[file.filepath] = issues

        # Collect symbols
        symbols = []
        
        # Add functions as symbols
        for func in file.functions:
            issues = []
            
            # Check for issues
            if not any(func.name in str(usage) for usage in func.usages):
                issues.append({
                    'type': 'minor',
                    'message': f'Unused function'
                })
            
            if hasattr(func, 'code_block'):
                code = func.code_block.source
                if 'None' in code and not any(s in code for s in ['is None', '== None', '!= None']):
                    issues.append({
                        'type': 'critical',
                        'message': f'Potential unsafe null reference'
                    })
                
                if 'TODO' in code or 'FIXME' in code:
                    issues.append({
                        'type': 'major',
                        'message': f'Incomplete implementation'
                    })

            symbols.append(Symbol(
                id=str(hash(func.name + file.filepath)),
                name=func.name,
                type='function',
                filepath=file.filepath,
                start_line=func.start_point[0] if hasattr(func, 'start_point') else 0,
                end_line=func.end_point[0] if hasattr(func, 'end_point') else 0,
                issues=issues if issues else None
            ))
        
        # Add classes as symbols
        for cls in file.classes:
            symbols.append(Symbol(
                id=str(hash(cls.name + file.filepath)),
                name=cls.name,
                type='class',
                filepath=file.filepath,
                start_line=cls.start_point[0] if hasattr(cls, 'start_point') else 0,
                end_line=cls.end_point[0] if hasattr(cls, 'end_point') else 0
            ))
        
        if symbols:
            file_symbols[file.filepath] = symbols

    # Build repository structure with symbols
    repo_structure = build_repo_structure(codebase.files, file_issues, file_symbols)

    # Calculate metrics
    callables = codebase.functions + [m for c in codebase.classes for m in c.methods]
    num_callables = 0
    
    for func in callables:
        if not hasattr(func, "code_block"):
            continue

        complexity = calculate_cyclomatic_complexity(func)
        operators, operands = get_operators_and_operands(func)
        volume, N1, N2, n1, n2 = calculate_halstead_volume(operators, operands)
        loc = len(func.code_block.source.splitlines())
        mi_score = calculate_maintainability_index(volume, complexity, loc)

        total_complexity += complexity
        total_volume += volume
        total_mi += mi_score
        num_callables += 1

    for cls in codebase.classes:
        doi = calculate_doi(cls)
        total_doi += doi

    desc = get_github_repo_description(repo_url)

    return AnalysisResponse(
        repo_url=repo_url,
        description=desc,
        num_files=num_files,
        num_functions=num_functions,
        num_classes=num_classes,
        line_metrics={
            "total": {
                "loc": total_loc,
                "lloc": total_lloc,
                "sloc": total_sloc,
                "comments": total_comments,
                "comment_density": (total_comments / total_loc * 100)
                if total_loc > 0
                else 0,
            },
        },
        cyclomatic_complexity={
            "average": total_complexity / num_callables if num_callables > 0 else 0,
        },
        depth_of_inheritance={
            "average": total_doi / len(codebase.classes) if codebase.classes else 0,
        },
        halstead_metrics={
            "total_volume": int(total_volume),
            "average_volume": int(total_volume / num_callables)
            if num_callables > 0
            else 0,
        },
        maintainability_index={
            "average": int(total_mi / num_callables) if num_callables > 0 else 0,
        },
        monthly_commits=monthly_commits,
        repo_structure=repo_structure
    )

@fastapi_app.get("/function/{function_id}/call-chain")
async def get_function_call_chain(function_id: str) -> List[str]:
    """Get the maximum call chain for a function."""
    try:
        function = get_function_by_id(function_id)
        chain = get_max_call_chain(function)
        return [f.name for f in chain]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@fastapi_app.get("/function/{function_id}/context")
async def get_function_context(function_id: str) -> FunctionContext:
    """Get detailed context for a specific function."""
    try:
        function = get_function_by_id(function_id)
        
        context = {
            "implementation": {
                "source": function.source,
                "filepath": function.filepath
            },
            "dependencies": [],
            "usages": []
        }
        
        # Add dependencies
        for dep in function.dependencies:
            if isinstance(dep, Import):
                dep = hop_through_imports(dep)
            context["dependencies"].append({
                "source": dep.source,
                "filepath": dep.filepath
            })
        
        # Add usages
        for usage in function.usages:
            context["usages"].append({
                "source": usage.usage_symbol.source,
                "filepath": usage.usage_symbol.filepath
            })
        
        return FunctionContext(**context)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@fastapi_app.get("/symbol/{symbol_id}/context")
async def get_symbol_context(symbol_id: str) -> Dict[str, Any]:
    """Get detailed context for any symbol."""
    try:
        symbol = get_symbol_by_id(symbol_id)  # You'll need to implement this
        return get_detailed_symbol_context(symbol)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Helper function to get a function by ID (you'll need to implement this)
def get_function_by_id(function_id: str):
    # Implementation depends on how you store/retrieve functions
    pass

# Helper function to resolve imports (you'll need to implement this)
def hop_through_imports(import_symbol):
    # Implementation depends on how you handle imports
    pass

@app.function(image=image)
@modal.asgi_app()
def fastapi_modal_app():
    return fastapi_app

if __name__ == "__main__":
    import uvicorn
    import socket
    
    def find_available_port(start_port=8000, max_port=8100):
        """Find an available port starting from start_port"""
        for port in range(start_port, max_port):
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind(('0.0.0.0', port))
                    return port
            except OSError:
                continue
        raise RuntimeError(f"No available ports found between {start_port} and {max_port}")
    
    # Find an available port
    port = find_available_port()
    print(f"🚀 Starting FastAPI server on http://localhost:{port}")
    print(f"📚 API documentation available at http://localhost:{port}/docs")
    
    uvicorn.run(fastapi_app, host="0.0.0.0", port=port)



# ================================================================================
# COMPREHENSIVE_ANALYSIS.PY - DEEP ANALYSIS
# ================================================================================

    UNUSED_FUNCTION = "Unused function"
    UNUSED_CLASS = "Unused class"
    UNUSED_IMPORT = "Unused import"
    UNUSED_PARAMETER = "Unused parameter"
    PARAMETER_MISMATCH = "Parameter mismatch"
    MISSING_TYPE_ANNOTATION = "Missing type annotation"
    CIRCULAR_DEPENDENCY = "Circular dependency"
    IMPLEMENTATION_ERROR = "Implementation error"
    EMPTY_FUNCTION = "Empty function"
    UNREACHABLE_CODE = "Unreachable code"

class ComprehensiveIssueSeverity:
    """Severity levels for issues."""
    CRITICAL = "critical"
    ERROR = "error"
    WARNING = "warning"
    INFO = "info"

class ComprehensiveIssue:
    """Represents an issue found during codebase analysis."""
    def __init__(self, 
                 item: Any,
                 issue_type: str,
                 message: str,
                 severity: str = IssueSeverity.WARNING,
                 location: Optional[str] = None,
                 suggestion: Optional[str] = None):
        self.item = item
        self.type = issue_type
        self.message = message
        self.severity = severity
        self.location = self._get_location(item) if location is None else location
        self.suggestion = suggestion
        
    def _get_location(self, item: Any) -> str:
        """Get a string representation of the item's location."""
        if hasattr(item, 'file') and hasattr(item.file, 'path'):
            file_path = item.file.path
            if hasattr(item, 'line'):
                return f"{file_path}:{item.line}"
            return file_path
        elif hasattr(item, 'path'):
            return item.path
        else:
            return "Unknown location"
    
    def __str__(self) -> str:
        """Return a string representation of the issue."""
        base = f"[{self.severity.upper()}] {self.type}: {self.message} ({self.location})"
        if self.suggestion:
            base += f" - Suggestion: {self.suggestion}"
        return base

class ComprehensiveAnalyzer:
    """
    Comprehensive analyzer for codebases using the Codegen SDK.
    Implements deep analysis of code issues, dependencies, and metrics.
    """
    
    def __init__(self, repo_path_or_url: str):
        """
        Initialize the analyzer with a repository path or URL.
        
        Args:
            repo_path_or_url: Path to local repo or URL to GitHub repo
        """
        self.repo_path_or_url = repo_path_or_url
        self.issues: List[Issue] = []
        self.start_time = time.time()
        self.codebase = None
        
    def analyze(self) -> Dict[str, Any]:
        """
        Perform a comprehensive analysis of the codebase.
        
        Returns:
            Dictionary with analysis results
        """
        print(f"Starting comprehensive analysis of {self.repo_path_or_url}...")
        
        # Initialize codebase
        try:
            print(f"Initializing codebase from {self.repo_path_or_url}")
            if self.repo_path_or_url.startswith(("http://", "https://")):
                # Extract repo name for GitHub URLs
                parts = self.repo_path_or_url.rstrip('/').split('/')
                repo_name = f"{parts[-2]}/{parts[-1]}"
                try:
                    self.codebase = Codebase.from_repo(repo_full_name=repo_name)
                    print(f"Successfully initialized codebase from GitHub repository: {repo_name}")
                except Exception as e:
                    print(f"Error initializing codebase from GitHub: {e}")
                    self.issues.append(Issue(
                        self.repo_path_or_url,
                        "Initialization Error",
                        f"Failed to initialize codebase from GitHub: {e}",
                        IssueSeverity.ERROR,
                        suggestion="Check your network connection and GitHub access permissions."
                    ))
                    return {
                        "error": f"Failed to initialize codebase: {str(e)}",
                        "success": False
                    }
            else:
                # Local path
                try:
                    self.codebase = Codebase(self.repo_path_or_url)
                    print(f"Successfully initialized codebase from local path: {self.repo_path_or_url}")
                except Exception as e:
                    print(f"Error initializing codebase from local path: {e}")
                    self.issues.append(Issue(
                        self.repo_path_or_url,
                        "Initialization Error",
                        f"Failed to initialize codebase from local path: {e}",
                        IssueSeverity.ERROR,
                        suggestion="Ensure the path exists and contains valid source code."
                    ))
                    return {
                        "error": f"Failed to initialize codebase: {str(e)}",
                        "success": False
                    }
            
            # Check if codebase was initialized correctly
            if not hasattr(self.codebase, 'files') or not self.codebase.files:
                self.issues.append(Issue(
                    self.repo_path_or_url,
                    "Empty Codebase",
                    "Codebase was initialized but contains no files",
                    IssueSeverity.ERROR,
                    suggestion="Check if the repository contains supported language files."
                ))
                print("Warning: Codebase contains no files")
            
            # Run all analyses - with error handling for each step
            try:
                self._analyze_dead_code()
            except Exception as e:
                print(f"Error in dead code analysis: {e}")
                self.issues.append(Issue(
                    self.repo_path_or_url, 
                    "Analysis Error", 
                    f"Dead code analysis failed: {e}",
                    IssueSeverity.ERROR
                ))
                
            try:
                self._analyze_parameter_issues()
            except Exception as e:
                print(f"Error in parameter analysis: {e}")
                self.issues.append(Issue(
                    self.repo_path_or_url, 
                    "Analysis Error", 
                    f"Parameter analysis failed: {e}",
                    IssueSeverity.ERROR
                ))
                
            try:
                self._analyze_type_annotations()
            except Exception as e:
                print(f"Error in type annotation analysis: {e}")
                self.issues.append(Issue(
                    self.repo_path_or_url, 
                    "Analysis Error", 
                    f"Type annotation analysis failed: {e}",
                    IssueSeverity.ERROR
                ))
                
            try:
                self._analyze_circular_dependencies()
            except Exception as e:
                print(f"Error in circular dependency analysis: {e}")
                self.issues.append(Issue(
                    self.repo_path_or_url, 
                    "Analysis Error", 
                    f"Circular dependency analysis failed: {e}",
                    IssueSeverity.ERROR
                ))
                
            try:
                self._analyze_implementation_issues()
            except Exception as e:
                print(f"Error in implementation issue analysis: {e}")
                self.issues.append(Issue(
                    self.repo_path_or_url, 
                    "Analysis Error", 
                    f"Implementation issue analysis failed: {e}",
                    IssueSeverity.ERROR
                ))
            
            # Generate final report
            return self._generate_report()
            
        except Exception as e:
            print(f"Error analyzing codebase: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "success": False
            }
    
    def _analyze_dead_code(self):
        """Find and log unused code (functions, classes, imports)."""
        # Find unused functions
        for func in self.codebase.functions:
            if not func.usages:
                self.issues.append(Issue(
                    func, 
                    IssueType.UNUSED_FUNCTION,
                    f"Unused function: {func.name}",
                    IssueSeverity.WARNING,
                    suggestion="Consider removing this unused function or documenting why it's needed"
                ))
        
        # Find unused classes
        for cls in self.codebase.classes:
            if not cls.usages:
                self.issues.append(Issue(
                    cls,
                    IssueType.UNUSED_CLASS,
                    f"Unused class: {cls.name}",
                    IssueSeverity.WARNING,
                    suggestion="Consider removing this unused class or documenting why it's needed"
                ))
        
        # Find unused imports
        for file in self.codebase.files:
            for imp in file.imports:
                if not imp.usages:
                    self.issues.append(Issue(
                        imp,
                        IssueType.UNUSED_IMPORT,
                        f"Unused import: {imp.source if hasattr(imp, 'source') else str(imp)}",
                        IssueSeverity.INFO,
                        suggestion="Remove this unused import"
                    ))
    
    def _analyze_parameter_issues(self):
        """Find and log parameter issues (unused, mismatches)."""
        for func in self.codebase.functions:
            # Check for unused parameters
            for param in func.parameters:
                # Skip 'self' in methods
                if param.name == 'self' and func.is_method:
                    continue
                    
                # Check if parameter is used in function body
                param_dependencies = [dep.name for dep in func.dependencies if hasattr(dep, 'name')]
                if param.name not in param_dependencies:
                    self.issues.append(Issue(
                        func,
                        IssueType.UNUSED_PARAMETER,
                        f"Function '{func.name}' has unused parameter: {param.name}",
                        IssueSeverity.INFO,
                        suggestion=f"Consider removing the unused parameter '{param.name}' if it's not needed"
                    ))
            
            # Check call sites for parameter mismatches
            for call in func.call_sites:
                if hasattr(call, 'args') and hasattr(func, 'parameters'):
                    expected_params = set(p.name for p in func.parameters if not p.is_optional and p.name != 'self')
                    actual_params = set()
                    
                    # Extract parameter names from call arguments
                    if hasattr(call, 'args'):
                        for arg in call.args:
                            if hasattr(arg, 'parameter_name') and arg.parameter_name:
                                actual_params.add(arg.parameter_name)
                    
                    # Find missing parameters
                    missing = expected_params - actual_params
                    if missing:
                        # Skip if function has **kwargs
                        has_kwargs = any(p.name.startswith('**') for p in func.parameters)
                        if not has_kwargs:
                            self.issues.append(Issue(
                                call,
                                IssueType.PARAMETER_MISMATCH,
                                f"Call to '{func.name}' is missing parameters: {', '.join(missing)}",
                                IssueSeverity.ERROR,
                                suggestion="Add the missing parameters to the function call"
                            ))
    
    def _analyze_type_annotations(self):
        """Find and log missing type annotations."""
        for func in self.codebase.functions:
            # Skip if function is in a type-annotated file
            file_path = str(func.file.path) if hasattr(func, 'file') and hasattr(func.file, 'path') else ''
            if any(file_ext in file_path for file_ext in ['.pyi']):
                continue
                
            # Check return type
            if not func.return_type and not func.name.startswith('__'):
                self.issues.append(Issue(
                    func,
                    IssueType.MISSING_TYPE_ANNOTATION,
                    f"Function '{func.name}' is missing return type annotation",
                    IssueSeverity.INFO,
                    suggestion="Add a return type annotation to improve type safety"
                ))
            
            # Check parameter types
            params_without_type = [p.name for p in func.parameters 
                                 if not p.type and p.name != 'self' and not p.name.startswith('*')]
            if params_without_type:
                self.issues.append(Issue(
                    func,
                    IssueType.MISSING_TYPE_ANNOTATION,
                    f"Function '{func.name}' has parameters without type annotations: {', '.join(params_without_type)}",
                    IssueSeverity.INFO,
                    suggestion="Add type annotations to all parameters"
                ))
    
    def _analyze_circular_dependencies(self):
        """Find and log circular dependencies."""
        circular_deps = {}
        
        # Basic implementation to detect file-level circular dependencies
        for file in self.codebase.files:
            visited = set()
            path = []
            self._check_circular_deps(file, visited, path, circular_deps)
        
        # Log circular dependencies
        for file_path, cycles in circular_deps.items():
            for cycle in cycles:
                cycle_str = " -> ".join([f.path for f in cycle])
                self.issues.append(Issue(
                    file_path,
                    IssueType.CIRCULAR_DEPENDENCY,
                    f"Circular dependency detected: {cycle_str}",
                    IssueSeverity.ERROR,
                    suggestion="Refactor the code to break the circular dependency"
                ))
    
    def _check_circular_deps(self, file, visited, path, circular_deps):
        """Helper method to check for circular dependencies using DFS."""
        if file in path:
            # Found a cycle
            cycle = path[path.index(file):] + [file]
            if file.path not in circular_deps:
                circular_deps[file.path] = []
            circular_deps[file.path].append(cycle)
            return
            
        if file in visited:
            return
            
        visited.add(file)
        path.append(file)
        
        # Check imports
        for imp in file.imports:
            if hasattr(imp, 'resolved_module') and imp.resolved_module:
                self._check_circular_deps(imp.resolved_module, visited, path.copy(), circular_deps)
        
        path.pop()
    
    def _analyze_implementation_issues(self):
        """Find and log implementation issues (empty functions, etc.)."""
        for func in self.codebase.functions:
            # Skip dunder methods and abstract methods
            if func.name.startswith('__') and func.name.endswith('__'):
                continue
            
            # Check for empty function bodies
            if not func.body or not func.body.strip():
                # Skip if it's a method overridden from parent class
                is_override = False
                if hasattr(func, 'parent') and isinstance(func.parent, Class):
                    for parent_class in func.parent.parents:
                        if any(m.name == func.name for m in parent_class.methods):
                            is_override = True
                            break
                
                if not is_override:
                    self.issues.append(Issue(
                        func,
                        IssueType.EMPTY_FUNCTION,
                        f"Function '{func.name}' has an empty body",
                        IssueSeverity.WARNING,
                        suggestion="Implement the function or remove it if it's not needed"
                    ))
    
    def _generate_report(self) -> Dict[str, Any]:
        """
        Generate a comprehensive report of the analysis results.
        
        Returns:
            Dictionary with analysis results
        """
        analysis_duration = time.time() - self.start_time
        
        # Get statistics
        stats = {
            "total_files": len(list(self.codebase.files)),
            "total_functions": len(list(self.codebase.functions)),
            "total_classes": len(list(self.codebase.classes)),
            "total_imports": len(list(self.codebase.imports)),
            "total_issues": len(self.issues),
            "analysis_duration": analysis_duration
        }
        
        # Get issues by severity
        issues_by_severity = {}
        for issue in self.issues:
            if issue.severity not in issues_by_severity:
                issues_by_severity[issue.severity] = []
            issues_by_severity[issue.severity].append(issue)
            
        # Get issues by type
        issues_by_type = {}
        for issue in self.issues:
            if issue.type not in issues_by_type:
                issues_by_type[issue.type] = []
            issues_by_type[issue.type].append(issue)
        
        # Prepare report
        report = {
            "success": True,
            "repo": self.repo_path_or_url,
            "timestamp": datetime.now().isoformat(),
            "duration": analysis_duration,
            "stats": stats,
            "issues": {
                "all": [str(issue) for issue in self.issues],
                "by_severity": {sev: [str(issue) for issue in issues] 
                                for sev, issues in issues_by_severity.items()},
                "by_type": {type_: [str(issue) for issue in issues] 
                            for type_, issues in issues_by_type.items()}
            },
            # Detailed counts
            "counts": {
                "unused_functions": len(issues_by_type.get(IssueType.UNUSED_FUNCTION, [])),
                "unused_classes": len(issues_by_type.get(IssueType.UNUSED_CLASS, [])),
                "unused_imports": len(issues_by_type.get(IssueType.UNUSED_IMPORT, [])),
                "parameter_issues": len(issues_by_type.get(IssueType.UNUSED_PARAMETER, [])) + 
                                   len(issues_by_type.get(IssueType.PARAMETER_MISMATCH, [])),
                "type_annotation_issues": len(issues_by_type.get(IssueType.MISSING_TYPE_ANNOTATION, [])),
                "circular_dependencies": len(issues_by_type.get(IssueType.CIRCULAR_DEPENDENCY, [])),
                "implementation_issues": len(issues_by_type.get(IssueType.IMPLEMENTATION_ERROR, [])) +
                                        len(issues_by_type.get(IssueType.EMPTY_FUNCTION, []))
            }
        }
        
        # Print the report to console
        self._print_report(report)
        
        # Save report to file
        self._save_report(report)
        
        return report
    
    def _print_report(self, report: Dict[str, Any]):
        """Print the analysis report to the console."""
        print("\n" + "=" * 80)
        print(f"🔍 COMPREHENSIVE CODEBASE ANALYSIS: {self.repo_path_or_url}")
        print("=" * 80)
        print(f"Analysis Time: {report['timestamp']}")
        print(f"Analysis Duration: {report['duration']:.2f} seconds")
        
        # Print stats
        print("\n" + "-" * 40)
        print("CODEBASE STATISTICS:")
        print("-" * 40)
        print(f"Total Files: {report['stats']['total_files']}")
        print(f"Total Functions: {report['stats']['total_functions']}")
        print(f"Total Classes: {report['stats']['total_classes']}")
        print(f"Total Imports: {report['stats']['total_imports']}")
        print(f"Total Issues: {report['stats']['total_issues']}")
        
        # Print issue counts by type
        print("\n" + "-" * 40)
        print("ISSUES BY TYPE:")
        print("-" * 40)
        for issue_type, count in report['counts'].items():
            print(f"{issue_type.replace('_', ' ').title()}: {count}")
        
        # Print issue counts by severity
        print("\n" + "-" * 40)
        print("ISSUES BY SEVERITY:")
        print("-" * 40)
        for severity, issues in report['issues']['by_severity'].items():
            print(f"{severity.upper()}: {len(issues)}")
        
        # Print top issues
        print("\n" + "-" * 40)
        print("TOP ISSUES:")
        print("-" * 40)
        for i, issue in enumerate(report['issues']['all'][:10], 1):
            print(f"{i}. {issue}")
        
        if len(report['issues']['all']) > 10:
            print(f"... and {len(report['issues']['all']) - 10} more issues")
    
    def _save_report(self, report: Dict[str, Any]):
        """Save the analysis report to a file."""
        # Create timestamp for filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"comprehensive_analysis_{timestamp}.txt"
        detailed_filename = f"detailed_analysis_{timestamp}.txt"
        
        print(f"\nSaving full analysis to {filename}...")
        with open(filename, "w") as f:
            f.write(f"COMPREHENSIVE CODEBASE ANALYSIS: {self.repo_path_or_url}\n")
            f.write(f"Analysis Time: {report['timestamp']}\n")
            f.write(f"Analysis Duration: {report['duration']:.2f} seconds\n\n")
            
            # Write stats
            f.write("CODEBASE STATISTICS:\n")
            f.write(f"Total Files: {report['stats']['total_files']}\n")
            f.write(f"Total Functions: {report['stats']['total_functions']}\n")
            f.write(f"Total Classes: {report['stats']['total_classes']}\n")
            f.write(f"Total Imports: {report['stats']['total_imports']}\n")
            f.write(f"Total Issues: {report['stats']['total_issues']}\n\n")
            
            # Write issue counts by type
            f.write("ISSUES BY TYPE:\n")
            for issue_type, count in report['counts'].items():
                f.write(f"{issue_type.replace('_', ' ').title()}: {count}\n")
            f.write("\n")
            
            # Write issue counts by severity
            f.write("ISSUES BY SEVERITY:\n")
            for severity, issues in report['issues']['by_severity'].items():
                f.write(f"{severity.upper()}: {len(issues)}\n")
            f.write("\n")
            
            # Write all issues
            f.write("ALL ISSUES:\n")
            for i, issue in enumerate(report['issues']['all'], 1):
                f.write(f"{i}. {issue}\n")
            
            # Add note about detailed summaries
            f.write("\n")
            f.write("=" * 80 + "\n")
            f.write("NOTE: Detailed summaries of codebase elements are available in a separate file\n")
            f.write(f"See: {detailed_filename}\n")
            f.write("=" * 80 + "\n")
        
        # Save detailed summaries to a separate file
        try:
            print(f"Saving detailed analysis to {detailed_filename}...")
            self._save_detailed_summaries(detailed_filename)
            print(f"Detailed summaries saved to {detailed_filename}")
        except Exception as e:
            print(f"Error saving detailed summaries: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"Analysis results saved to {filename}")
    
    def _save_detailed_summaries(self, filename: str):
        """Save detailed summaries of the codebase, files, classes, and functions."""
        with open(filename, "w") as f:
            f.write("=" * 80 + "\n")
            f.write(" " * 20 + "COMPREHENSIVE CODEBASE ANALYSIS DETAILS\n")
            f.write(" " * 20 + f"Repository: {self.repo_path_or_url}\n")
            f.write(" " * 20 + f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
        
            # Write codebase summary
            try:
                f.write("=" * 80 + "\n")
                f.write(" " * 30 + "CODEBASE SUMMARY\n")
                f.write("=" * 80 + "\n")
                f.write(get_codebase_summary(self.codebase))
                f.write("\n\n")
            except Exception as e:
                f.write(f"Error generating codebase summary: {str(e)}\n\n")
            
            # Write file summaries
            try:
                f.write("=" * 80 + "\n")
                f.write(" " * 30 + "FILE SUMMARIES\n")
                f.write("=" * 80 + "\n")
                files = list(self.codebase.files)
                for i, file in enumerate(files):
                    f.write(f"File {i+1}/{len(files)}: {getattr(file, 'path', 'Unknown path')}\n")
                    f.write("-" * 60 + "\n")
                    try:
                        f.write(get_file_summary(file))
                    except Exception as e:
                        f.write(f"Error generating file summary: {str(e)}\n")
                    f.write("\n\n")
            except Exception as e:
                f.write(f"Error processing file summaries: {str(e)}\n\n")
            
            # Write class summaries
            try:
                f.write("=" * 80 + "\n")
                f.write(" " * 30 + "CLASS SUMMARIES\n")
                f.write("=" * 80 + "\n")
                classes = list(self.codebase.classes)
                if classes:
                    for i, cls in enumerate(classes):
                        f.write(f"Class {i+1}/{len(classes)}: {getattr(cls, 'name', 'Unknown class')}\n")
                        f.write("-" * 60 + "\n")
                        try:
                            f.write(get_class_summary(cls))
                        except Exception as e:
                            f.write(f"Error generating class summary: {str(e)}\n")
                        f.write("\n\n")
                else:
                    f.write("No classes found in the codebase.\n\n")
            except Exception as e:
                f.write(f"Error processing class summaries: {str(e)}\n\n")
            
            # Write function summaries (limit to top 30 for large codebases)
            try:
                f.write("=" * 80 + "\n")
                f.write(" " * 30 + "FUNCTION SUMMARIES\n")
                f.write("=" * 80 + "\n")
                functions = list(self.codebase.functions)
                if functions:
                    max_funcs = min(30, len(functions))
                    f.write(f"Showing {max_funcs} of {len(functions)} functions\n\n")
                    for i, func in enumerate(functions[:max_funcs]):
                        f.write(f"Function {i+1}/{max_funcs}: {getattr(func, 'name', 'Unknown function')}\n")
                        f.write("-" * 60 + "\n")
                        try:
                            f.write(get_function_summary(func))
                        except Exception as e:
                            f.write(f"Error generating function summary: {str(e)}\n")
                        f.write("\n\n")
                    
                    if len(functions) > max_funcs:
                        f.write(f"... and {len(functions) - max_funcs} more functions not shown\n\n")
                else:
                    f.write("No functions found in the codebase.\n\n")
            except Exception as e:
                f.write(f"Error processing function summaries: {str(e)}\n\n")
                
            # Add symbol usage summary for important symbols
            try:
                f.write("=" * 80 + "\n")
                f.write(" " * 30 + "KEY SYMBOL USAGE SUMMARIES\n")
                f.write("=" * 80 + "\n")
                
                # Get top 10 most used symbols
                symbols = list(self.codebase.symbols)
                if symbols:
                    # Sort symbols by usage count (if available)
                    try:
                        symbols_with_usage = [(s, len(getattr(s, 'usages', []))) for s in symbols]
                        sorted_symbols = [s for s, _ in sorted(symbols_with_usage, key=lambda x: x[1], reverse=True)]
                        top_symbols = sorted_symbols[:10]
                    except:
                        # Fall back to first 10 symbols if sorting fails
                        top_symbols = symbols[:10]
                        
                    for i, symbol in enumerate(top_symbols):
                        f.write(f"Symbol {i+1}/{len(top_symbols)}: {getattr(symbol, 'name', 'Unknown symbol')}\n")
                        f.write("-" * 60 + "\n")
                        try:
                            f.write(get_symbol_summary(symbol))
                        except Exception as e:
                            f.write(f"Error generating symbol summary: {str(e)}\n")
                        f.write("\n\n")
                else:
                    f.write("No symbols found for detailed analysis.\n\n")
            except Exception as e:
                f.write(f"Error processing symbol summaries: {str(e)}\n\n")
                
            f.write("=" * 80 + "\n")
            f.write(" " * 25 + "END OF DETAILED CODEBASE ANALYSIS\n")
            f.write("=" * 80 + "\n")

def main():
    """Run the comprehensive analyzer from the command line."""
    parser = argparse.ArgumentParser(
        description="Analyze a codebase comprehensively using the Codegen SDK"
    )
    parser.add_argument(
        "--repo", 
        default="./",
        help="Repository URL or local path to analyze"
    )
    args = parser.parse_args()
    
    analyzer = ComprehensiveAnalyzer(args.repo)
    analyzer.analyze()
    
    return 0

if __name__ == "__main__":
    sys.exit(main())


# ================================================================================
# UNIFIED MAIN EXECUTION BLOCK
# ================================================================================

if __name__ == "__main__":
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(
        description="Complete Consolidated Codebase Analytics API",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Available modes:
  api        - Start FastAPI web server (default)
  analyze    - Run comprehensive analysis using CodebaseAnalyzer
  cli        - Command line interface using ComprehensiveAnalyzer  
  html       - Generate HTML report
        """
    )
    
    parser.add_argument("--mode", choices=["api", "analyze", "cli", "html"], default="api")
    parser.add_argument("--repo", help="Repository path or URL")
    parser.add_argument("--port", type=int, default=8000, help="API server port")
    parser.add_argument("--output", help="Output file path")
    parser.add_argument("--format", choices=["json", "html", "console"], default="console")
    
    args = parser.parse_args()
    
    if args.mode == "api":
        try:
            import uvicorn
            print(f"🚀 Starting Complete Analytics API on http://localhost:{args.port}")
            print(f"📚 Documentation: http://localhost:{args.port}/docs")
            
            # Try to find the FastAPI app
            if 'fastapi_app' in globals():
                uvicorn.run(fastapi_app, host="0.0.0.0", port=args.port)
            elif 'app' in globals():
                uvicorn.run(app, host="0.0.0.0", port=args.port)
            else:
                print("❌ FastAPI app not found. Check consolidation.")
                sys.exit(1)
        except ImportError:
            print("❌ FastAPI/uvicorn not available. Install with: pip install fastapi uvicorn")
            sys.exit(1)
    
    elif args.mode == "analyze":
        if not args.repo:
            print("❌ --repo required for analysis mode")
            sys.exit(1)
        
        try:
            print(f"🔍 Analyzing {args.repo} using CodebaseAnalyzer...")
            analyzer = CodebaseAnalyzer(repo_path=args.repo)
            result = analyzer.analyze()
            
            if args.output:
                analyzer.save_results(args.output)
                print(f"✅ Results saved to {args.output}")
            else:
                analyzer._print_console_report()
        except Exception as e:
            print(f"❌ Analysis failed: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    elif args.mode == "cli":
        if not args.repo:
            print("❌ --repo required for CLI mode")
            sys.exit(1)
        
        try:
            print(f"🔍 Analyzing {args.repo} using ComprehensiveAnalyzer...")
            comp_analyzer = ComprehensiveAnalyzer(args.repo)
            result = comp_analyzer.analyze()
            comp_analyzer._print_report(result)
        except Exception as e:
            print(f"❌ CLI analysis failed: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    elif args.mode == "html":
        if not args.repo:
            print("❌ --repo required for HTML mode")
            sys.exit(1)
        
        try:
            print(f"🔍 Generating HTML report for {args.repo}...")
            analyzer = CodebaseAnalyzer(repo_path=args.repo)
            analyzer.analyze()
            output_file = args.output or "analysis_report.html"
            analyzer._generate_html_report(output_file)
            print(f"✅ HTML report generated: {output_file}")
        except Exception as e:
            print(f"❌ HTML generation failed: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
